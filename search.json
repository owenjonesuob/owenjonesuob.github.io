[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Products of Procrastination",
    "section": "",
    "text": "Hi, I‚Äôm Owen üëã I‚Äôm a data scientist/engineer living in Cardiff, United Kingdom. This is a collection of projects I have spent time on while I was supposed to be doing other things.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse deploy keys to access private repos within GitHub Actions\n\n\n\n\n\nMy preferred workflow for accessing non-public repos from within GitHub Actions\n\n\n\n\n\nApr 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nModular R code for analytical projects with {box}\n\n\n\n\n\nHow we create modular code and use it across multiple projects\n\n\n\n\n\nApr 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNew blog, who dis?\n\n\n\n\n\nI honestly did intend to just write a blog post‚Ä¶ I guess this is the post?\n\n\n\n\n\nApr 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Names for Datasets in R Packages\n\n\n\n\n\nCan we use multiple names to refer to the same dataset, without including multiple copies of it in the package?\n\n\n\n\n\nJan 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nDebugging: Installing NVIDIA Drivers on Fedora\n\n\n\n\n\nTL;DR: I had mega issues with getting NVIDIA graphics to work on Fedora. This is a play-by-play of my debugging process, and ultimately the solution turned out to be rather simple.\n\n\n\n\n\nJun 13, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nAFK, back in 2 months\n\n\n\n\n\nSome things I learned which definitely have completely nothing at all to do with my job.\n\n\n\n\n\nSep 29, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nUsing RStudio on a Google Cloud Dataproc cluster\n\n\n\n\n\nGetting set up to use Spark with R on a Google Cloud computing cluster.\n\n\n\n\n\nMay 16, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nLove Machine: Automating the romantic songwriting process\n\n\n\n\n\nBecause being creative is hard and machine learning is the answer to everything.\n\n\n\n\n\nFeb 14, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nScale up your interest: key steps for getting into data science\n\n\n\n\n\nYou have just encountered the term ‚Äúdata science‚Äù for the first time, and it sounds like it might be interesting but you don‚Äôt have a clue what it is. Something to do with computers? Should you bring a lab coat, or a VR headset? Or both? What is a data and how does one science it?\n\n\n\n\n\nJan 4, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nH2O.ai: Going for a paddle\n\n\n\n\n\nA brief dive into the H2O machine learning framework\n\n\n\n\n\nAug 23, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nAdding a Git Bash alias\n\n\n\n\n\nAnother useful time-saving trick\n\n\n\n\n\nAug 22, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nMarkov and Churchill: an exploration of predictive text\n\n\n\n\n\nA foray into memoryless stochastic processes\n\n\n\n\n\nJul 23, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nA Token Update\n\n\n\n\n\nIt‚Äôs been a while‚Ä¶\n\n\n\n\n\nJul 8, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of 30 Days, 30 Visualizations, 1 Dataset\n\n\n\n\n\nA condensed version of TDTVOD\n\n\n\n\n\nJan 27, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nSnippets in RStudio\n\n\n\n\n\nMini time-saving abbreviations!\n\n\n\n\n\nJan 21, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nBANEScarparking - an R package\n\n\n\n\n\nJanuary exams are imminent, so I should be revising. Therefore I have written an R package‚Ä¶\n\n\n\n\n\nJan 13, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n30 Days, 30 Visualizations, 1 Dataset: Days 26-30\n\n\n\n\n\nThe epic finale of a month-long journey\n\n\n\n\n\nDec 7, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n30 Days, 30 Visualizations, 1 Dataset: Days 21-25\n\n\n\n\n\nThe penultimate chapter as TDTVOD plunges headlong into December\n\n\n\n\n\nDec 2, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n30 Days, 30 Visualizations, 1 Dataset: Days 16-20\n\n\n\n\n\nTDTVOD Goes Fourth\n\n\n\n\n\nNov 27, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n30 Days, 30 Visualizations, 1 Dataset: Days 11-15\n\n\n\n\n\nThe third riveting instalment of TDTVOD\n\n\n\n\n\nNov 22, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n30 Days, 30 Visualizations, 1 Dataset: Days 06-10\n\n\n\n\n\nThe second instalment of an ongoing dataviz project\n\n\n\n\n\nNov 17, 2016\n\n\n\n\n\n\n\n\n\n\n\n\n30 Days, 30 Visualizations, 1 Dataset\n\n\n\n\n\nA challenge to exhaustively explore and analyse a single dataset\n\n\n\n\n\nNov 11, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of backpropagation, plus a network diagram\n\n\n\n\n\nDeriving the backpropagation formulae, and a network diagram for one of my previous projects\n\n\n\n\n\nOct 21, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSaving/loading RStudio settings with Command Prompt\n\n\n\n\n\nThis one doesn‚Äôt really deserve the title of ‚Äòproject‚Äô, but I thought I‚Äôd share it anyway.\n\n\n\n\n\nOct 12, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nPlacement Year Salaries - Getting data from HTML\n\n\n\n\n\nNotes from my first web data project\n\n\n\n\n\nOct 9, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nIE Automation with VBA in Excel\n\n\n\n\n\nA rough-and-ready automated name-searching program in VBA\n\n\n\n\n\nOct 2, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nDigit Recognition - Building a neural network from scratch in R\n\n\n\n\n\nNotes from my very first project: building a simple neural network for handwritten digit recognition\n\n\n\n\n\nSep 30, 2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2016-12-07-30-days-30-visualizations-1-dataset-part-6/index.html",
    "href": "posts/2016-12-07-30-days-30-visualizations-1-dataset-part-6/index.html",
    "title": "30 Days, 30 Visualizations, 1 Dataset: Days 26-30",
    "section": "",
    "text": "Day 26 (07/12/16): Mean occupancy by weekday revisited\n\n\nWe‚Äôre on the home straight!\n\n\nPartly due to lack of time, partly due to lack of new ideas, and mostly because I want to, I am going to spend the last five days of this project revisiting some of the earlier plots I made, tidying them up and using cleaned data.\n\n\nLet‚Äôs make this ‚Äúcleaned data‚Äù - having spent the last couple of weeks looking at what makes the data messy, I‚Äôm now in a much better position to do this than I would have been at the start!\n\nrdf1 &lt;- select(df0, Name, LastUpdate, DateUploaded,               Capacity, Occupancy, Percentage, Status) %&gt;%     mutate(LastUpdate = as.POSIXct(LastUpdate,                                    format = \"%d/%m/%Y %I:%M:%S %p\"),            DateUploaded = as.POSIXct(DateUploaded,                                      format = \"%d/%m/%Y %I:%M:%S %p\")) %&gt;%     # Cleaning:     #   Remove single (?) row with NA     filter(!is.na(LastUpdate)) %&gt;%     #   Remove \"strange\" occupancies (\"strange\" is arbitrary...)     filter(Occupancy &gt; -100) %&gt;%     filter(Percentage &lt; 110) %&gt;%     #   Remove duplicates     group_by(Name, LastUpdate) %&gt;%     slice(1L) %&gt;%     ungroup()\n\nI‚Äôll start by replotting Day 04. This time around I will use dplyr rather than base functions to prepare the data. I‚Äôll also make sure to include an x-axis on each plot, and I won‚Äôt have any issues with my faceting this time either!\n\n```rdf2 &lt;- select(df1, Name, Percentage, LastUpdate) %&gt;% filter(Name != ‚Äútest car park‚Äù) %&gt;% mutate(Time = (hour(LastUpdate) + round(minute(LastUpdate), -1)/60), Day = wday(LastUpdate, label = TRUE)) %&gt;% group_by(Name, Day, Time) %&gt;% summarize(X = mean(Percentage))\np &lt;- ggplot(df2, aes(x = Time, y = X)) + geom_line(aes(colour = Name)) + facet_wrap(~ Day, nrow = 2, scales = ‚Äúfree_x‚Äù) + ggtitle(‚ÄúMean occupancy by weekday‚Äù) + labs(y = ‚ÄúPercentage occupancy‚Äù, x = ‚ÄúTime (hour)‚Äù) + theme(plot.title = element_text(size = rel(1.8))) + guides(colour = guide_legend(override.aes = list(size = 3))) p```\n\n\n\n\nDay 27 (08/12/16): Occupancy per weekday by car park, revisited\n\n\nRevisiting Day 05. The alpha levels are scaled much better now there are no outlying data points.\n\nrp &lt;- ggplot(data = df2, aes(Time, y = Day)) +     facet_wrap(~ Name, nrow = 2, scales = \"free_x\") +     geom_point(aes(colour = Name, alpha = X), shape = 20, size = 4) +     guides(colour = guide_legend(override.aes = list(size = 3))) +     scale_alpha(range = c(0, 1)) +     scale_y_discrete(limits = rev(levels(df2$Day))) +     labs(x = \"Time (hour)\", y = \"\") +     ggtitle(\"Occupancy per weekday, by car park\") +     theme(plot.title = element_text(size = 22, face = \"bold\"),           legend.position = \"None\") p\n\n\n\nDay 28 (09/12/16): Status by weekday, revisited\n\n\nAgain, just tidying up a little and using the clean data.\n\n```rlibrary(lubridate) library(reshape2) library(scales)\ndf2 &lt;- select(df1, Name, LastUpdate, Status) %&gt;% filter(Name != ‚Äútest car park‚Äù) %&gt;% mutate(Day = wday(LastUpdate, label = TRUE)) %&gt;% group_by(Name, Day) %&gt;% summarize(Filling = sum(Status == ‚ÄúFilling‚Äù) / n(), Static = sum(Status == ‚ÄúStatic‚Äù) / n(), Emptying = sum(Status == ‚ÄúEmptying‚Äù) / n()) %&gt;% melt()\np &lt;- ggplot(df2, aes(x = Day, y = value, fill = variable)) + geom_bar(stat = ‚Äúidentity‚Äù) + coord_flip() + facet_wrap(~ Name, nrow = 2) + scale_fill_manual(values = c(‚Äú#FF4444‚Äù, ‚Äú#AAAAAA‚Äù, ‚Äú#6666FF‚Äù)) + scale_x_discrete(limits = rev(levels(df2$Day))) + scale_y_continuous(labels = percent) + labs(y = ‚ÄúPercent of records‚Äù, x = ‚ÄúDay‚Äù, fill = ‚Äú‚Äú) + ggtitle(‚ÄùStatus by weekday‚Äù) + theme(plot.title = element_text(size = 22, face = ‚Äúbold‚Äù)) p``` \n\n\nDay 29 (10/12/16): Mean percentage occupancy by week, revisited\n\n\nRevisiting Day 09. The lines are, generally speaking, a little smoother now.\n\n```rdf2 &lt;- select(df1, Name, LastUpdate, Percentage) %&gt;% filter(Name != ‚Äútest car park‚Äù) %&gt;% mutate(Week = week(LastUpdate)) %&gt;% group_by(Name, Week) %&gt;% summarize(meanP = mean(Percentage))\nmaxP &lt;- top_n(df2, n = 1)\np &lt;- ggplot(df2, aes(x = Week, y = meanP)) + geom_line() + facet_wrap(~ Name, nrow = 2) + geom_label(data = maxP, aes(x = Week, y = meanP + 10, label = Week)) + ggtitle(‚ÄúMean percentage occupancy per week‚Äù) + ylab(‚ÄúPercentage‚Äù) + theme(plot.title = element_text(size = 22, face = ‚Äúbold‚Äù)) p``` \n\n\nDay 30 (11/12/16): The End\n\n\nWell, here I am. One month later.\n\n\nThis has probably been the hardest project I‚Äôve attempted to date.\n\n\nI say that not because the learning has been particularly challenging, although obviously I was learning ‚Äòon-the-go‚Äô.\n\n\nIt‚Äôs tempting to say that the most difficult aspect was coming up with new ideas and new ways to explore a pretty limited set of data. In a way it is fortunate that there were so many messy and anomalous records to explore, or I would have been running out of ideas within two weeks.\n\n\nBut actually, the hardest thing about this project was finding the time and motivation each day to spend a couple of hours, an hour, even 20 minutes working on it - when really, true (as per usual!) to this website‚Äôs general theme, I should have been doing other things.\n\n\nHaving said that, I‚Äôve done it! I stuck with it. I have (approximately) 30 plots to prove it, as well as a much better command of ggplot and dplyr for manipulating and visualizing data in the future. And hopefully some of the knowledge I‚Äôve gained about the dataset will come in useful for the Bath ML Meetup project.\n\n\nI thought I‚Äôd round off by sharing some of my ideas and doodles from when I was just starting out.\n\n\n\n\n\n\n\n\n\n\n\nI‚Äôm now facing a busy month or two of revision and exams, so I can‚Äôt say for sure when my next project will materialize (to be truthful they tend to be fairly spontaneous anyway‚Ä¶) - so until then, fare thee all well, and best wishes for the remainder of 2016 and for the New Year."
  },
  {
    "objectID": "posts/2018-01-04-scale-up-your-interest/index.html",
    "href": "posts/2018-01-04-scale-up-your-interest/index.html",
    "title": "Scale up your interest: key steps for getting into data science",
    "section": "",
    "text": "Note: I originally wrote this post for the Mango Solutions blog, and they have kindly allowed me to repost it here alongside the rest of my work. You can find the original post here!\n\n\nPrelude\nMaybe you‚Äôre looking for a change of scene. Maybe you‚Äôre looking for your first job. Maybe you‚Äôre stuck in conversation with a relative who you haven‚Äôt spoken to since last Christmas and who has astonishingly strong opinions on whether cells ought to be merged or not in Excel spreadsheets.\nThe fact of the matter is that you have just encountered the term ‚Äúdata science‚Äù for the first time, and it sounds like it might be interesting but you don‚Äôt have a clue what it is. Something to do with computers? Should you bring a lab coat, or a VR headset? Or both? What is a data and how does one science it?\nFear not. I am here to offer subjective, questionable and most importantly FREE advice from the perspective of someone who was in that very position not such a long time ago. Read on at your peril.\n\n\n\nI. Adagio: Hear about data science\nThis is the hard bit. It‚Äôs surprisingly difficult to stumble upon data science unless someone tells you about it.\nBut the good news is that you‚Äôre reading this, so you‚Äôve already done it. Possibly a while ago, or possibly just now; either way, put a big tick next to Step 1. Congratulations!\n(By the way, you‚Äôll remember the person who told you about data science. When you grow in confidence yourself, be someone else‚Äôs ‚Äúperson who told me about data science‚Äù. It‚Äôs a great thing to share. But all in good time‚Ä¶)\n\n\n\nII. Andante: Find out more\nBut what actually is data science?\nTo be honest, it‚Äôs a fairly loosely-defined term. There are plenty of articles out there that try to give an overview, but most descend into extended discussions about the existence of unicorns or resort to arranging countless combinations of potentially relevant acronyms in hideous indecipherable Venn diagrams.\nYou‚Äôre much better off finding examples of people ‚Äúdoing‚Äù data science. Find some blogs (here are a few awesome ones to get you started) and read about what people are up to in the real world.\nDon‚Äôt be afraid to narrow down and focus on a specific topic that interests you - there‚Äôs so much variety out there that you‚Äôre bound to find something that inspires you to keep reading and learning. But equally, explore as many new areas as you can, because the more context you can get about the sector the better your understanding will be and you‚Äôll start to see how different subjects and different roles relate to each other.\nBelieve it or not, one of the best tools for keeping up to date with the latest developments in the field is Twitter. If you follow all your blog-writing heroes, not only will you be informed whenever they publish a new article but you‚Äôll also get an invaluable glimpse into their day-to-day jobs and working habits, as well as all the cool industry-related stuff they share. Even if you never tweet anything yourself you‚Äôll be exposed to much more than you‚Äôd be able to find on your own. If you want to get involved there‚Äôs no need to be original - you could just use it to share content you‚Äôve found interesting yourself.\nIf you‚Äôre super keen, you might even want to get yourself some data science books tackling a particular topic. Keep an eye out for free online/ebook versions too!\n\n\n\nIII. Allegretto: Get hands-on\nObserving is great, but it will only get you so far.\nImagine that you‚Äôve just heard about an amazing new thing called ‚Äúpiano‚Äù. It sounds great. No, it sounds INCREDIBLE. It‚Äôs the sort of thing you really want to be good at.\nSo you get online and read more about it. Descriptions, analyses, painstaking breakdowns of manual anatomy and contrapuntal textures. You watch videos of people playing pianos, talking about pianos, setting pianos on fire and hurling them across dark fields. You download reams of free sheet music and maybe even buy a book of pieces you really want to learn.\nBut at some point‚Ä¶ you need to play a piano.\nThe good news is that with data science, you don‚Äôt need to buy a piano, or find somewhere to keep it, or worry about bothering your family/friends/neighbours/pets with your late-night composing sessions.\nOnline interactive coding tutorials are a great place to start if you want to learn a new programming language. Sites like DataCamp and Codecademy offer a number of free courses to get yourself started with data science languages like R and Python. If you are feeling brave enough, take the plunge and run things on your own machine! (I‚Äôd strongly recommend using R with RStudio and using Anaconda for Python.) Language-specific ‚Äúnative-format‚Äù resources such as SWIRL for R or this set of Jupyter notebooks for Python are a great way to learn more advanced skills. Take advantage of the exercises in any books you have - don‚Äôt just skip them all!\nData science is more than just coding though - it‚Äôs all about taking a problem, understanding it, solving it and then communicating those ideas to other people. So Part 1 of my Number One Two-Part Top Tip for you today is:\n\n1. Pick a project and write about it\n\nHow does one ‚Äúpick a project‚Äù? Well, find something that interests you. For me it was neural networks (and later, car parks‚Ä¶) but it could be literally anything, so long as you‚Äôre going to be able to find some data to work with. Maybe have a look at some of the competitions hosted on Kaggle or see if there‚Äôs a group in your area which publishes open data.\nThen once you‚Äôve picked something, go for it! Try out that cool package you saw someone else using. Figure out why there are so many missing values in that dataset. Take risks, explore, try new things and push yourself out of your comfort zone. And don‚Äôt be afraid to take inspiration from something that someone else has already done: regardless of whether you follow the same process or reach the same outcome, your take on it is going to be different to theirs.\nBy writing about that project - which is often easier than deciding on one in the first place - you‚Äôre developing your skills as a communicator by presenting your work in a coherent manner, rather than as a patchwork of dodgy scripts interspersed with the occasional hasty comment. And even if you don‚Äôt want to make your writing public, you‚Äôll be amazed how often you go back and read something you wrote before because it‚Äôs come up again in something else you‚Äôre working on and you‚Äôve forgotten how to do it.\nI‚Äôd really encourage you to get your work out there though. Which brings us smoothly to‚Ä¶\n\n\n\nIV. Allegro maestoso: Get yourself out there\nIf you never play the piano for anyone else, no-one‚Äôs ever going to find out how good you are! So Part 2 of my Number One Two-Part Top Tip is:\n\n2. Start a blog\n\nIt‚Äôs pretty easy to get going with Wordpress or similar, and it takes your writing to the next level because now you‚Äôre writing for an audience. It may not be a very big audience, but if someone, somewhere finds your writing interesting or useful then surely it‚Äôs worth it. And if you know you‚Äôre potentially writing for someone other than yourself then you need to explain everything properly, which means you need to understand everything properly. I often learn more when I‚Äôm writing up a project than when I‚Äôm playing around with the code in the first place.\nAlso, a blog is a really good thing to have on your CV and to talk about at interviews, because it gives you some physical (well, virtual) evidence which you can point at as you say ‚Äúlook at this thing wot I‚Äôve done‚Äù.\n(Don‚Äôt actually say those exact words. Remember that you‚Äôre a Good Communicator.)\nIf you‚Äôre feeling brave you can even put that Twitter account to good use and start shouting about all the amazing things you‚Äôre doing. You‚Äôll build up a loyal following amazingly quickly. Yes, half of them will probably be bots, but half of them will be real people who enjoy reading your work and who can give you valuable feedback.\nSpeaking of real people‚Ä¶\n\n3. Get involved in the community\n\nYes, that was indeed Part 3 of my Number One Two-Part Top Tip, but it‚Äôs so important that it needs to be in there.\nThe online data science community is one of the best out there. The R community in particular is super friendly and supportive (check out forums like RStudio Community, community groups like R4DS, and the #rstats tag on Twitter). Get involved in conversations, learn from people already working in the sector, share your own knowledge and make friends.\nWant to go one better than online?\nGet a Meetup account, sign up to some local groups and go out to some events. It might be difficult to force yourself to go for the first time, but pluck up the courage and do it. Believe me when I say there‚Äôs no substitute for meeting up and chatting to people. Many good friends are people I met for the first time at meetups. And of course, it‚Äôs the perfect opportunity to network - I met 5 or 6 of my current colleagues through BathML before I even knew about Mango!\n(If you‚Äôre in or near Bristol or London, Bristol Data Scientists and LondonR are both hosted by Mango and new members are always welcome!)\n\n\n\nPostlude\nOf course, everything I‚Äôve just said is coming from my point of view and is entirely based on my own experiences.\nFor example, I‚Äôve talked about coding quite a lot because I personally code quite a lot; and I code quite a lot because I enjoy it. That might not be the case for you. That‚Äôs fine. In fact it‚Äôs more than ‚Äúfine‚Äù; the huge diversity in people‚Äôs backgrounds and interests is what makes data science such a fantastic field to be working in right now.\nMaybe you‚Äôre interested in data visualisation. Maybe you‚Äôre into webscraping. Or stats. Or fintech, or NLP, or AI, or BI, or CI. Maybe you are the relative at Christmas dinner who won‚Äôt stop banging on about why you should NEVER, under ANY circumstances, merge cells in an Excel spreadsheet (UNLESS it is PURELY for purposes of presentation).\nOh, why not:\n\n4. Find the parts of data science that you enjoy and arrange them so that they work for you."
  },
  {
    "objectID": "posts/2016-11-27-30-days-30-visualizations-1-dataset-part-4/index.html",
    "href": "posts/2016-11-27-30-days-30-visualizations-1-dataset-part-4/index.html",
    "title": "30 Days, 30 Visualizations, 1 Dataset: Days 16-20",
    "section": "",
    "text": "Day 16 (27/11/16): Shifting massively negative values\n\n\nAs a culmination of the effort of the last few days, let‚Äôs see if the shifted values line up nicely with the unaffected values from that day.\n\n\nI‚Äôll keep my guess of 15220 for the shift k from yesterday for now.\n\n```rtimerange &lt;- interval(ymd_hms(‚Äú2015-01-05 10:00:00‚Äù), ymd_hms(‚Äú2015-01-05 18:00:00‚Äù))\nk &lt;- 15220\ndf2 &lt;- select(df, Name, LastUpdate, Occupancy) %&gt;% filter(Name == ‚ÄúSouthGate General CP‚Äù) %&gt;% filter(LastUpdate %within% timerange) %&gt;% arrange(LastUpdate) %&gt;% mutate(newOcc1 = ifelse(Occupancy &lt; -10000, Occupancy + k, Occupancy))&lt;p&gt;I can actually check whether my guess for k is reasonable fairly easily - I‚Äôll find the largest changes in occupancy, and then average them.&lt;/p&gt;rrng &lt;- range(diff(df2$Occupancy)) mean(abs(rng))rconsole## [1] 15219.5&lt;p&gt;It turns out my guess was actually very reasonable indeed!&lt;/p&gt; &lt;p&gt;OK, let‚Äôs try plotting the corrected occupancies on top of the correct records.&lt;/p&gt;rp &lt;- ggplot(df2, aes(x = LastUpdate, y = newOcc)) + geom_path(aes(colour = (Occupancy &lt; -10000))) + ggtitle(‚ÄúAdjustment of massively negative occupancies‚Äù)\np&lt;img src=\"day16.jpeg\" /&gt; &lt;p&gt;I had hoped that the simple shift would mean the records would line up - but we can see that in fact this is not the case.&lt;/p&gt; &lt;p&gt;Perhaps then, despite the saga of the last few days, it is going to be too much trouble to try to correct these records, and it will be much simpler to just ignore them.&lt;/p&gt; &lt;hr&gt; &lt;h3&gt;Day 17 (28/11/16): Duplicate records&lt;/h3&gt; &lt;p&gt;While working with the massively negative records over the past few days, I noticed something slightly odd.&lt;/p&gt; &lt;p&gt;On occasion, there are two records which are identical - same car park, same occupancy, same time of update. These duplicate records are certainly not helpful, and might affect calculated averages - for example, mean occupancy of a car park over a given week, if this were calculated simply by averaging the occupancy values from all records in that week and if an outlying value were duplicated.&lt;/p&gt; &lt;p&gt;Let‚Äôs see where we have duplicate records‚Ä¶&lt;/p&gt;rdf2 &lt;- select(df, Name, LastUpdate) %&gt;% filter(Name != ‚Äútest car park‚Äù) %&gt;% group_by(Name, LastUpdate) %&gt;% summarize(count = n())\np &lt;- ggplot(df2, aes(x = LastUpdate, y = count, colour = Name)) + geom_point() + ggtitle(‚ÄúDuplicate records‚Äù) + ylab(‚ÄúNumber of records‚Äù) + theme(plot.title = element_text(size = rel(1.5)))\np&lt;img src=\"day17.jpeg\" /&gt; &lt;p&gt;It turns out that not only are there many, many instances where a record is recorded twice. There are, in fact, a lot of records which are present hundreds of times - one record from Odd Down P+R is present 1145 times!&lt;/p&gt; &lt;hr&gt; &lt;h3&gt;Day 18 (29/11/16): Number of n-plicate records&lt;/h3&gt; &lt;p&gt;Having discovered the slightly worrying number of duplicate records yesterday, I thought I would see how many groups of identical records there are, and how many records are in these groups.&lt;/p&gt;rdf2 &lt;- select(df, Name, LastUpdate) %&gt;% filter(Name != ‚Äútest car park‚Äù) %&gt;% group_by(Name, LastUpdate) %&gt;% summarize(count = n()) %&gt;% group_by(count) %&gt;% summarize(metacount = n(), proportion = metacount / nrow(.))\nhead(df2)rconsole## # A tibble: 6 √ó 3 ## count metacount proportion ##\n## 1 1 1041683 0.8290611487 ## 2 2 211517 0.1683434663 ## 3 3 1072 0.0008531900 ## 4 4 730 0.0005809969 ## 5 5 217 0.0001727073 ## 6 6 185 0.0001472390rtail(df2)rconsole## # A tibble: 6 √ó 3 ## count metacount proportion ##\n## 1 1069 1 7.958862e-07 ## 2 1087 1 7.958862e-07 ## 3 1089 1 7.958862e-07 ## 4 1130 1 7.958862e-07 ## 5 1143 1 7.958862e-07 ## 6 1145 1 7.958862e-07&lt;p&gt;So the majority of records are only recorded once, but a significant proportion (about 16.8%) of records are recorded twice, and another small subset (around 0.3%) are recorded more than twice - with some recorded over 1000 times, as we saw yesterday!&lt;/p&gt; &lt;p&gt;Let‚Äôs try to visualize exactly what we‚Äôre dealing with here.&lt;/p&gt;rp &lt;- ggplot(df2[-c(1), ], aes(x = count, y = (metacount + 0.5))) + geom_bar(colour = ‚Äúblack‚Äù, stat = ‚Äúidentity‚Äù) + scale_y_log10() + ggtitle(‚ÄúNumber of n-plicate records‚Äù) + xlab(‚ÄúSize of group of identical records‚Äù) + ylab(‚ÄúNumber of groups‚Äù) + theme(plot.title = element_text(size = rel(1.5)))\np&lt;img src=\"day18.jpeg\" /&gt; &lt;p&gt;I‚Äôve had to use a log scale since there are many more size-2-to-12-ish records than most others - but this causes the bars to all but vanish when there is only one group of a particular size. I‚Äôve tried to get around this problem by adding a black border to the bars, so if you look very closely you can just about see them on the x-axis.&lt;/p&gt; &lt;p&gt;Anyway, we can see there are a very large number of groups with a few identical records, as well as a few groups with a very large number of identical records.&lt;/p&gt; &lt;hr&gt; &lt;h3&gt;Day 19 (30/11/16): Times of duplicate uploads&lt;/h3&gt; &lt;p&gt;First I‚Äôm just going to make sure that if two records have the same Name and LastUpdate entries, that they are also identical in the other fields - most importantly, Occupancy.&lt;/p&gt;rdf3 &lt;- filter(df, Name != ‚Äútest car park‚Äù) %&gt;% group_by(Name, LastUpdate) %&gt;% filter(n() &gt; 1)\ndf4 &lt;- filter(df, Name != ‚Äútest car park‚Äù) %&gt;% group_by(Name, LastUpdate, Occupancy) %&gt;% filter(n() &gt; 1)\nsetdiff(df3, df4)rconsole## # A tibble: 0 √ó 0&lt;p&gt;So there are 0 rows identical in Name and LastUpdate but different in Occupancy. This is a good thing!&lt;/p&gt; &lt;p&gt;We have a dataframe (well, 2 identical dataframes!) containing all the duplicate records. Let‚Äôs see if there‚Äôs any sort of pattern in the times when duplicates are recorded.&lt;/p&gt;rp &lt;- ggplot(df3, aes(x = LastUpdate, y = ‚Äú‚Äú)) + geom_point(alpha = 0.1) + facet_wrap(~ Name, nrow = 2) + ggtitle(‚ÄùTimes of duplicate uploads‚Äù) + ylab(‚Äú‚Äú) + theme(plot.title = element_text(size = rel(1.5)), panel.background = element_blank())\np``` \n\nWe can see that duplicates are recorded almost constantly at the SouthGate car parks, very often at the Odd Down and Lansdown P+Rs and then less frequently but seemingly at random in the other car parks.\n\n\n\nDay 20 (01/12/16): Delay between update and upload\n\n\nFor the majority of this project I have been working with df, a trimmed-down version of the original dataframe which I created in the first week or so.\n\n\nOne of the columns I cut when I created df was DateUploaded: which is, of course, the time when a record was uploaded to the database.\n\n\nPerhaps there are some interesting features to find in this column though‚Ä¶\n\n\nFor example, what is the delay in between a record being taken, and it being uploaded?\n\n```r# Read in the original dataframe again df0 &lt;- read.csv(‚ÄúC:/Users/Owen/Documents/Coding/Parking/data/BANES_Historic_Car_Park_Occupancy.csv‚Äù)\nlibrary(dplyr) library(lubridate)\ndf1 &lt;- df0 %&gt;% select(Name, LastUpdate, DateUploaded) %&gt;% mutate(LastUpdate = as.POSIXct(LastUpdate, tz = ‚ÄúUTC‚Äù, format = ‚Äú%d/%m/%Y %I:%M:%S %p‚Äù), DateUploaded = as.POSIXct(DateUploaded, tz = ‚ÄúUTC‚Äù, format = ‚Äú%d/%m/%Y %I:%M:%S %p‚Äù)) %&gt;% mutate(Delay = as.numeric(DateUploaded - LastUpdate))\nlibrary(ggplot2)\np &lt;- ggplot(df1, aes(x = Delay)) + geom_histogram(colour = ‚Äúblack‚Äù, bins = 40) + ggtitle(‚ÄúDelay between update and upload‚Äù) + xlab(‚ÄúSeconds‚Äù) + ylab(‚ÄúNumber of records‚Äù)\np&lt;img src=\"day20.jpeg\" /&gt; &lt;p&gt;So the vast majority of the records are uploaded pretty quickly. But there is a suspicious black line along the bottom of the plot‚Ä¶&lt;/p&gt; &lt;p&gt;Let‚Äôs replot using a log scale.&lt;/p&gt;rp + scale_y_log10()``` \n\nSo we actually have quite a lot of records where the delay between update and upload is fairly large - over 300000 seconds (about 3.5 days) in some cases. There are also a significant number of records which were somehow uploaded BEFORE they were recorded."
  },
  {
    "objectID": "posts/2018-02-14-love-machine/index.html",
    "href": "posts/2018-02-14-love-machine/index.html",
    "title": "Love Machine: Automating the romantic songwriting process",
    "section": "",
    "text": "Note: I originally wrote this post for the Mango Solutions blog, and they have kindly allowed me to repost it here alongside the rest of my work. You can find the original post here!\n\n\nSongwriting is a very mysterious process. It feels like creating something from nothing. It‚Äôs something I don‚Äôt feel like I really control.\n‚Äì Tracy Chapman\n\n\nIt is February. The shortest, coldest, wettest, miserablest month of the British year.\nOnly two things happen in Britain during February. For a single evening, the people refrain from dipping all their food in batter and deep-frying it, and instead save some time by pouring the batter straight into a frying pan and eating it by itself; and for an entire day, the exchange of modest indications of affection between consenting adults is permitted, although the government advises against significant deviation from the actions specified in the state-issued Approved Romantic Gestures Handbook.\nIn Section 8.4 (Guidelines for Pre-Marital Communication) the following suggestion is made:\n\n‚ÄúWritten expressions of emotion should be avoided where possible. Should it become absolutely necessary to express emotion in a written format, it should be limited to a ‚Äòpopular‚Äô form of romantic lyricism. Examples of such ‚Äòpopular‚Äô forms include ‚Äòlove poem‚Äô and ‚Äòlove song‚Äô.\n\nThankfully, for those who have not achieved at least a master‚Äôs degree in a related field, writing a poem or song is a virtually impossible task. And following the sustained and highly successful effort to persuade the British youth that a career in the arts is a fast-track to unemployment, the number of applications to study non-STEM subjects at British universities has been falling consistently since the turn of the decade. This ensures that only the very best and most talented songwriters, producing the most creatively ingenuous work, are able to achieve widespread recognition, and therefore the British public are only exposed to high-quality creative influences.\nBut to us scientists, the lack of method is disturbing. This ‚Äúcreativity‚Äù must have a rational explanation. There must be some pattern.\nThis is unquestionably a problem which can be solved by machine learning, so let‚Äôs take the most obvious approach we can: we‚Äôll train a recurrent neural network to generate song lyrics character by character.\n\n\nYou write down a paragraph or two describing several different subjects creating a kind of story ingredients-list, I suppose, and then cut the sentences into four or five-word sections; mix ‚Äôem up and reconnect them. You can get some pretty interesting idea combinations like this. You can use them as is or, if you have a craven need to not lose control, bounce off these ideas and write whole new sections.\n‚Äì David Bowie\n\n\nTo build our neural network I‚Äôm going to be using the Keras machine learning interface (which we‚Äôre very excited about here at Mango right now - keep an eye out for workshops in the near future!). I‚Äôve largely followed the steps in this example from the Keras for R website, and I‚Äôm going to stick to a high-level description of what‚Äôs going on, but if you‚Äôre the sort of person who would rather dive head-first into the code, don‚Äôt feel like you have to hang around here - go ahead and have a play! And if you want to read more about RNNs, this excellent post by Andrej Kaparthy is at least as entertaining and significantly more informative than the one you‚Äôre currently reading.\nWe start by scraping as many love song lyrics as possible from the web - these will form our training material. Here‚Äôs the sort of thing we‚Äôre talking about:\n\nWell‚Ä¶ that‚Äôs how they look to us. Actually, after a bit of preprocessing, the computer sees something more like this:\n\nAll line breaks are represented by the pair of characters ‚Äú\\n‚Äù, and so all the lyrics from all the songs are squashed down into one big long string.\nThen we use this string to train the network. We show the network a section of the string, and tell it what comes next.\n\nSo the network gradually learns which characters tend to follow a given fixed-length ‚Äúsentence‚Äù. The more of these what-comes-next examples it sees, the better it gets at correctly guessing what should follow any sentence we feed in.\nAt this point, our network is like a loyal student of a great artist, dutifully copying every brushstroke in minuscule detail and receiving a slap on the wrist and a barked correction every time it slips up. Via this process it appears to have done two things.\nFirstly, it seems to have developed an understanding of the ‚Äúrules‚Äù of writing a song. These rules are complex and multi-levelled; the network first had to learn the rules of English spelling and grammar, before it could start to make decisions about when to move to a new line or which rhyming pattern to use.\n(Of course, it hasn‚Äôt actually ‚Äúdeveloped an understanding‚Äù of these rules. It has no idea what a ‚Äúword‚Äù is, or a ‚Äúnew line‚Äù. It just knows that every few characters it should guess \" \", and then sometimes it should put in a \"\\\", and whenever it puts in a \"\\\" then it‚Äôs got to follow that up with a \"n\" and then immediately a capital letter. Easy peasy.)\nSecondly, and in exactly the same way, the network will have picked up some of the style of the work it is copying. If we were training it on the songs one specific artist, it would have learned to imitate the style of that particular artist - but we‚Äôve gone one better than that and trained it on all the love songs we could find. So effectively, it‚Äôs learned how everyone else writes love songs.\nBut no-one gets famous by writing songs which have already been written. What we need now is some creativity, some passion, a little bit of je ne sais quoi.\nLet‚Äôs stop telling our network what comes next. Let‚Äôs give it the freedom to write whatever it likes.\n\n\nI don‚Äôt think you can ever do your best. Doing your best is a process of trying to do your best.\n‚Äì Townes van Zandt\n\n\nIt‚Äôs interesting to look at the songwriting attempts of the network in the very early stages of training. At first, it is guessing more or less at random what character should come next, so we end up with semi-structured gobbledegook:\nfameliawmalYaws. Boflyi, methabeethirts yt\nplay3mppioty2=ytrnfuunuiYs blllstis\nByyovcecrowth andtpazo's youltpuduc,s Ijd\"a]bemob8b&gt;fiume,;Co\nBliovlkfrenuyione (ju'te,'ve ru t Kis\ngo arLUUs,k'CaufkfR )s'xCvectdvoldes\n\n4So\nAvanrvous Ist'dyMe Dolriri\nBut notice that even in that example, which was taken from a very early training stage, the network has already nailed the ‚Äú\\n‚Äù newline combo and has even started to pick up on other consistent structural patterns like closing a ‚Äú(‚Äù with a ‚Äú)‚Äù. Actually, the jumbled nonsense becomes coherent English (or English-esque) ramblings quite quickly.\nThere is one interesting parameter to adjust when we ask the model to produce some output: the ‚Äúdiversity‚Äù parameter, which determines how adventurous the network should be in its choice of character. The higher we set this parameter, the more the network will favour slightly-less-probable characters over the most obvious choice at each point.\nIf we set the diversity parameter too low, we often degenerate into uncontrolled bursts of la-ing:\nla la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la\nla la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la\nla la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la la\n(... lots more \"la\"s)\nBut set it too high and the network decides dictionary English is too limiting.\nOh, this younan every, drock on\nScridh's tty'\nIs go only ealled\nYou could have like the one don'm I dope\nLove me\nAnd woment while you all that\nWas it statiinc. I living you must?\nWe dirls anythor\nIt‚Äôs difficult to find the write balance between syllabic repetition and progressive vocabulary, and there‚Äôs a surprisingly fine line between the two - this will probably prove to be a fruitful area for further academic research.\nI think that identifying the optimal diversity parameter is probably the key to good songwriting.\n\n\nSongwriting is like editing. You write down all this stuff - all this bad, stupid stuff - and then you have to get rid of everything except the very best.\n‚Äì Juliana Hatfield\n\n\nHere are some particularly ‚Äúbeautiful‚Äù passages taken from the great amount of (largely poor) material the model produced. I haven‚Äôt done any editing other than to isolate a few consecutive lines at a time and in the last few examples, to start the network off with certain sentences. Wishing you a Happy Valentine‚Äôs Day!\n\n\nI want to do the trute\nI want to do and I don't want it to love you\nI want to work around\nAnd the world we can stay the sun\nAnd the sky is a wind a time\nI won't never need a love\n\n\nAnd I Mister the sunshine that I was to be with you\nI need you\nI want to wanna learn a river\nAnd I want you to stand in the sun\nI need you and I want you and I can stay away\n\n\nI wonder the stars the stars the smile\nAnd I wonder the ones of my life\nI was the litter more\nAnd I was the sun\nShe too moment we can tell my life\nI can't be when I wanted to leave it all\nI wonder the light and the stars the sun\nAnd I can still let me love me\nI can still let me love me\nI want to fall a lover your love\nI want to fall in love\nI want to see your eyes\n\n\nI must find a believer to say\nBut where ain't all about you\nI must have to make to meet to leare\nI must have to think of you\nI would never knows, don't make to love you\nThat she cry\nAnd I could always find your sunchin' starter\nWhat's enough the hold my way you can stall\nA darly love my heart I don't know\nAnd I must have to mean to me\nI'm so time now\n\n\n\nI know your eyes in the morning sun\nI feel the name of love\nLove is a picked the sun\nAll my life I can make me wanna be with you\nI just give up in your head\nAnd I can stay that you want a life\nI've stay the more than I do\n\n\nHow long will I love you\nAs long as there is that songs\nAll the things that you want to find you\nI could say me true\nI want to fall in love with you\nI want my life\nAnd you're so sweet\nWhen I see you wanted to that for you\nI can see you and thing, baby\nI wanna be alone\n\n\nOh yeah I tell you somethin'\nI think you'll understand\nWhen I say that somethin'\nI thought the dartion hyand\nI want me way to hear\nAll the things what you do\n\n\nWise men say\nOnly fools rush in\nBut I can hear your love\nAnd I don't wanna be alone\n\n\nIf I should stay\nI would only be in your head\nI wanna know that I hope I see the sun\nI want a best there for me too\nI just see that I can have beautiful\nSo hold me to you"
  },
  {
    "objectID": "posts/2016-11-17-30-days-30-visualizations-1-dataset-part-2/index.html",
    "href": "posts/2016-11-17-30-days-30-visualizations-1-dataset-part-2/index.html",
    "title": "30 Days, 30 Visualizations, 1 Dataset: Days 06-10",
    "section": "",
    "text": "Day 06 (17/11/16): Podium CP timeshift check\n\n\nAs promised yesterday, I‚Äôm going to investigate the potential timeshift error in the data relating to one particular car park, caused by the changeover from GMT to BST.\n\n# I'll have a look at records from spring of 2016 - clocks changed on 27/03/2016\n# at 01:00\ntimes &lt;- df$LastUpdate[grep(\"^27/03/2016\", df$LastUpdate)]\nhours &lt;- as.POSIXlt(times, format = \"%d/%m/%Y %I:%M:%S %p\", tz = \"UTC\")$hour\n\n# Check number of records each hour (shouldn't be any at 1AM i.e. 2nd entry)\nsapply(c(0:23), function(i) sum(hours == i))\nrconsole##  [1] 96 12 86 94 96 96 96 96 96 96 97 95 96 96 96 96 96 96 96 96 96 98 95 ## [24] 95\n# Approximately 1/8 as many records at 01:00 as at other times - is just one of\n# the 8 car parks responsible?\nnames &lt;- df$Name[grep(\"^27/03/2016\", df$LastUpdate)]\nnames[hours == 1]\nrconsole##  [1] Podium CP Podium CP Podium CP Podium CP Podium CP Podium CP Podium CP ##  [8] Podium CP Podium CP Podium CP Podium CP Podium CP ## 9 Levels: Avon Street CP Charlotte Street CP ... test car park\n```r# It seems Podium CP‚Äôs clock is not changing!\n\nLet‚Äôs check previous year too - clocks changed on 29/03/2015\nt &lt;- df\\(LastUpdate[grep(\"^29/03/2015\", df\\)LastUpdate)] h &lt;- as.POSIXlt(t, format = ‚Äú%d/%m/%Y %I:%M:%S %p‚Äù, tz = ‚ÄúUTC‚Äù)\\(hour\nn &lt;- df\\)Name[grep(‚Äú^29/03/2015‚Äù, df$LastUpdate)]\nsapply(c(0:23), function(i) sum(h == i))\n\n```rconsole##  [1] 84 12 72 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84 84\n## [24] 84```\n\n```rn[h == 1]```\n```rconsole##  [1] Podium CP Podium CP Podium CP Podium CP Podium CP Podium CP Podium CP\n##  [8] Podium CP Podium CP Podium CP Podium CP Podium CP\n## 9 Levels: Avon Street CP Charlotte Street CP ... test car park```\n&lt;p&gt;Hmmm‚Ä¶ let‚Äôs quickly try to see the effect of this error from one week to the next by comparing a day in a GMT week with the same weekday in a BST week. I‚Äôm going to use a couple of packages which are new to me - &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;lubridate&lt;/code&gt; - to manipulate the data.&lt;/p&gt;\n\n```r\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Get records for Podium CP from Tuesday, 22/03/2016 (arbitrary weekday!)\nweekgmt &lt;-\n    df %&gt;%\n    filter(grepl(\"^22/03/2016\", LastUpdate)) %&gt;%\n    filter(grepl(\"Podium CP\", Name))\n\nroundedgmt &lt;- round_date(as.POSIXct(weekgmt$LastUpdate,\n                                 format = \"%d/%m/%Y %I:%M:%S %p\", tz = \"UTC\"),\n                      unit = \"10 min\")\n\nweekgmt$Time &lt;- hour(roundedgmt) + minute(roundedgmt)/60\n    \n# Get records for Podium CP from exactly 1 week later, after clocks had changed\nweekbst &lt;- df %&gt;%\n    filter(grepl(\"^29/03/2016\", LastUpdate)) %&gt;%\n    filter(grepl(\"Podium CP\", Name))\n\nroundedbst &lt;- round_date(as.POSIXct(weekbst$LastUpdate,\n                                    format = \"%d/%m/%Y %I:%M:%S %p\",\n                                    tz = \"UTC\"),\n                         unit = \"10 min\")\n\nweekbst$Time &lt;- hour(roundedbst) + minute(roundedbst)/60\n\n\n# Nothing too fancy with the plotting today!\nplot(weekgmt$Time, weekgmt$Percentage, pch = 20, ylim = c(0, 100),\n     xlab = \"Time\", ylab = \"Percentage\", main = \"Podium CP timeshift check\")\n\npoints(weekbst$Time, weekbst$Percentage, pch = 20, col = \"blue\")\n\nlegend(\"topright\", c(\"22/03/2016 (GMT)\", \"29/03/2016 (BST)\"),\n       fill = c(\"black\", \"blue\"))\n\n\nOh‚Ä¶ those curves look quite similar. There certainly isn‚Äôt an obvious one-hour shift from one week to the next.\n\n\nLet‚Äôs have another look at that vector from earlier of the number of records from each hour:\n\nsapply(c(0:23), function(i) sum(hours == i))\nrconsole##  [1] 96 12 86 94 96 96 96 96 96 96 97 95 96 96 96 96 96 96 96 96 96 98 95 ## [24] 95\n\nI was focusing so much on the suspicious 2nd element that I didn‚Äôt spot the also-somewhat-suspicious 3rd element - there are noticeably fewer records from 02:00 to 02:59 than from the other hours. This would be explained if Podium CP changes its clocks at 02:00, rather than 01:00‚Ä¶ let‚Äôs see.\n\nrsum(names[hours == 2] == \"Podium CP\") r## [1] 0\n\nSo none of the records taken between 02:00 and 03:00 were from Podium CP - confirming my theory.\n\n\nThere is, then, as I suspected, some timeshift error in the data from Podium CP.\n\n\nHowever, the error is present for a grand total of 2 hours each year (one hour on a spring day, one on an autumn day), and records from these hours are between 01:00 and 03:00, when there is virtually no change in the occupancy of the car park. Therefore I think it is reasonable not to worry about it too much!\n\n\n\nDay 07 (18/11/16): Status by weekday\n\n\nI seem to have been converting the LastUpdate column to various date formats fairly regularly. Right now, I can‚Äôt think of any good reason why I need to leave it in character format, so I‚Äôll save myself a step in future and re-write the column in my CSV.\n\nlibrary(dplyr)\n\ndf &lt;- read.csv(\"df.csv\")\n\ndf &lt;- mutate(df, LastUpdate = as.POSIXct(LastUpdate,\n                                         format = \"%d/%m/%Y %I:%M:%S %p\"))\n\nwrite.csv(df, \"df.csv\")\n\nOn to today‚Äôs work.\n\n\nThere‚Äôs one column I haven‚Äôt really looked at so far: the Status column, which describes the change in occupancy of the car park compared to the previous record. Let‚Äôs have a look:\n\nrsummary(df$Status) rconsole##          Emptying  Filling   Static  ##     1333   474595   404339   647960\n\nThere are a number of records without entries in the Status column - but remembering a discovery from a previous day, I have an idea of who the culprit might be‚Ä¶\n\nrsummary(df[df$Name == \"test car park\", ]$Status) rconsole##          Emptying  Filling   Static  ##     1333        0        0        0\n\nThat accounts for the 1333 records with no Status entry, then.\n\n\nOnwards! We‚Äôll remove the ‚Äútest car park‚Äù records, and then for each car park and each weekday we‚Äôll calculate the percentage of records that have each possible Status.\n\nrlibrary(lubridate)\ndf2 &lt;-\n    select(df, Name, LastUpdate, Status) %&gt;%\n    filter(Name != \"test car park\") %&gt;%\n    mutate(Day = wday(LastUpdate, label = TRUE)) %&gt;%\n    group_by(Name, Day) %&gt;%\n    summarize(Filling = sum(Status == \"Filling\") / n(),\n              Static = sum(Status == \"Static\") / n(),\n              Emptying = sum(Status == \"Emptying\") / n())\n\nhead(df2)\nrconsole## Source: local data frame [6 x 5] ## Groups: Name [1] ##  ##             Name   Day   Filling    Static  Emptying ##           &lt;fctr&gt; &lt;ord&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; ## 1 Avon Street CP   Sun 0.2601505 0.3397476 0.4001019 ## 2 Avon Street CP   Mon 0.3163539 0.3390450 0.3446012 ## 3 Avon Street CP  Tues 0.3585985 0.2763592 0.3650423 ## 4 Avon Street CP   Wed 0.3831001 0.2751629 0.3417370 ## 5 Avon Street CP Thurs 0.3306908 0.2689445 0.4003647 ## 6 Avon Street CP   Fri 0.3864523 0.2756433 0.3379044\n\nI‚Äôm aiming for a stacked-bar-type chart today, and it seems the most effective way to do this is to have the data in so-called ‚Äòlong‚Äô format - so I‚Äôll use another package I haven‚Äôt used much before, reshape2, to reformat the data:\n\nlibrary(reshape2)\n\ndf3 &lt;- melt(df2)\nrconsole## Using Name, Day as id variables rhead(df3)&gt; rconsole##             Name   Day variable     value ## 1 Avon Street CP   Sun  Filling 0.2601505 ## 2 Avon Street CP   Mon  Filling 0.3163539 ## 3 Avon Street CP  Tues  Filling 0.3585985 ## 4 Avon Street CP   Wed  Filling 0.3831001 ## 5 Avon Street CP Thurs  Filling 0.3306908 ## 6 Avon Street CP   Fri  Filling 0.3864523\n\nRight, let‚Äôs plot!\n\n```rlibrary(ggplot2)\np &lt;- ggplot(df3, aes(x = Day, y = value, fill = variable)) + geom_bar(stat = ‚Äúidentity‚Äù) + coord_flip() + facet_wrap(~ Name, nrow = 2) + scale_fill_manual(values = c(‚Äú#FF4444‚Äù, ‚Äú#AAAAAA‚Äù, ‚Äú#6666FF‚Äù))\np&lt;img src=\"day07odd.jpeg\" /&gt; &lt;p&gt;Immediately we notice that our week has been extended with an eighth day - NAday.&lt;/p&gt; &lt;p&gt;This extra day seems to have been provided by Podium CP, which is by this point getting a reputation as a bit of a rebel‚Ä¶ and as well as giving us NAday, it seems that Podium CP never fills or empties either.&lt;/p&gt;rsummary(df[df\\(Name == \"Podium CP\",]\\)Status)rconsole## Emptying Filling Static ## 0 0 0 191022&lt;p&gt;Maybe every car that enters perfectly synchronizes with an exiting car. Maybe no-one ever enters or exits. We may never know.&lt;/p&gt; &lt;p&gt;Let‚Äôs remove the NAday records, just to tidy up the plot a little, and then re-plot.&lt;/p&gt;rdf &lt;- df[!is.na(df$LastUpdate), ] # Repeat all code for df2, df3\n\n\nI‚Äôll make the plot a bit prettier too\nlibrary(scales)\np &lt;- ggplot(df3, aes(x = Day, y = value, fill = variable)) + geom_bar(stat = ‚Äúidentity‚Äù) + coord_flip() + facet_wrap(~ Name, nrow = 2) + scale_fill_manual(values = c(‚Äú#FF4444‚Äù, ‚Äú#AAAAAA‚Äù, ‚Äú#6666FF‚Äù)) + scale_x_discrete(limits = rev(levels(df2$Day))) + scale_y_continuous(labels = percent) + ggtitle(‚ÄúStatus by weekday‚Äù) + labs(x = ‚ÄúDay‚Äù, y = ‚ÄúPercent of records‚Äù, fill = ‚Äú‚Äú) + theme(plot.title = element_text(size = 22, face =‚Äùbold‚Äù))\np``` \n\nThis plot gives us some idea of the ‚Äòturnaround‚Äô of the different car parks. Looking, for example, at the two SouthGate car parks, we can see that SG General is rarely static, suggesting a near-constant flow of cars entering and exiting. In contrast, SG Rail is static more than half the time.\n\n\nThis corresponds to the role of the two car parks: SG General is in the town centre near the main shopping area, and so is used by a lot of people for relatively short periods of time. On the other hand SG Rail is primarily used by rail commuters, so during the day there isn‚Äôt much change in its occupancy - it fills and empties rapidly each morning and evening. This behaviour can actually be seen on the plot from Day 04, where SG Rail‚Äôs occupancy is much ‚Äòsquarer‚Äô than the other car parks, particularly during the working week.\n\n\n\nDay 08 (19/11/16): More dodgy data‚Ä¶\n\n\nI just noticed a couple of things about my dataframe, df.\n\n\nFirstly, because I‚Äôve re-written it to CSV a couple of times and forgotten to turn off row.names, it‚Äôs picked up a couple of extra unnecessary columns.\n\n\nSecondly, LastUpdate still isn‚Äôt in POSIXct form.\n\n\nI‚Äôm going to go back to the original huge dataframe and sort out these issues for hopefully the last time.\n\n```rlibrary(dplyr) library(lubridate)\ndf &lt;- read.csv(‚ÄúC:/Users/Owen/Documents/Coding/Parking/data/BANES_Historic_Car_Park_Occupancy.csv‚Äù) %&gt;% select(Name, LastUpdate, Capacity, Occupancy, Percentage, Status) %&gt;% mutate(LastUpdate = as.POSIXct(LastUpdate, format = ‚Äú%d/%m/%Y %I:%M:%S %p‚Äù, tz = ‚ÄúUTC‚Äù))\nstr(df)rconsole## ‚Äòdata.frame‚Äô: 1528227 obs. of 6 variables: ## $ Name : Factor w/ 9 levels ‚ÄúAvon Street CP‚Äù,..: 7 1 3 8 6 4 2 5 8 5 ‚Ä¶ ## $ LastUpdate: POSIXct, format: ‚Äú2016-02-13 02:08:46‚Äù ‚Äú2016-02-13 02:09:36‚Äù ‚Ä¶ ## $ Capacity : int 720 630 827 140 521 698 1056 1252 140 1252 ‚Ä¶ ## $ Occupancy : int 28 32 8 15 75 21 124 0 15 0 ‚Ä¶ ## $ Percentage: int 3 5 1 10 14 3 12 0 10 0 ‚Ä¶ ## $ Status : Factor w/ 4 levels ‚Äú‚Äú,‚ÄùEmptying‚Äù,..: 4 3 4 4 4 4 3 4 4 4 ‚Ä¶&lt;p&gt;That‚Äôs more like it. Let‚Äôs write that to a file.&lt;/p&gt;rwrite.csv(df, ‚Äúdf.csv‚Äù, row.names = FALSE)&lt;p&gt;Now let‚Äôs load it in again, just to make sure:&lt;/p&gt;rdf &lt;- read.csv(‚ÄúC:/Users/Owen/Documents/Coding/Parking/df.csv‚Äù)\nstr(df)rconsole## ‚Äòdata.frame‚Äô: 1528227 obs. of 6 variables: ## $ Name : Factor w/ 9 levels ‚ÄúAvon Street CP‚Äù,..: 7 1 3 8 6 4 2 5 8 5 ‚Ä¶ ## $ LastUpdate: Factor w/ 1004552 levels ‚Äú2014-10-17 15:43:21‚Äù,..: 659008 659009 659011 659008 659007 659010 659009 659012 659119 659123 ‚Ä¶ ## $ Capacity : int 720 630 827 140 521 698 1056 1252 140 1252 ‚Ä¶ ## $ Occupancy : int 28 32 8 15 75 21 124 0 15 0 ‚Ä¶ ## $ Percentage: int 3 5 1 10 14 3 12 0 10 0 ‚Ä¶ ## $ Status : Factor w/ 4 levels ‚Äú‚Äú,‚ÄùEmptying‚Äù,..: 4 3 4 4 4 4 3 4 4 4 ‚Ä¶&lt;p&gt;Oh. It seems that we lose the POSIXct date format when we write to and/or read from a CSV file. I suppose I will just have to convert it whenever I need to.&lt;/p&gt; &lt;p&gt;OK, on to today. The Christmas lights in Bath were switched on a couple of days ago in readiness for the Christmas market, which starts next week. Knowing that Bath gets particularly busy during this period, I thought I‚Äôd have a look at the mean occupancy of each car park per week, and see if there is some sort of increase in late November and early December - particularly in the park and ride (P+R) car parks, which the council advises visitors to use during the market period.&lt;/p&gt; &lt;p&gt;Let‚Äôs set up a dataframe and use &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;lubridate&lt;/code&gt; functions to calculate the mean occupancy per week, by car park.&lt;/p&gt;rdf2 &lt;- df %&gt;% select(Name, LastUpdate, Percentage) %&gt;% na.omit() %&gt;% filter(Name != ‚Äútest car park‚Äù) %&gt;% mutate(Week = week(ymd_hms(LastUpdate))) %&gt;% group_by(Name, Week) %&gt;% summarize(meanP = mean(Percentage))\n\n\nLet‚Äôs have a look at the max values of these percentages\nmaxP &lt;- summarize(df2, max(meanP))\nmaxPrconsole## # A tibble: 8 √ó 2 ## Name max(meanP) ##   ## 1 Avon Street CP 82.59902 ## 2 Charlotte Street CP 46.39036 ## 3 Lansdown P+R 34.27553 ## 4 Newbridge P+R 112.53799 ## 5 Odd Down P+R 33.22015 ## 6 Podium CP 44.61979 ## 7 SouthGate General CP 53.13495 ## 8 SouthGate Rail CP 63.89707&lt;p&gt;Hang on. At some point, Newbridge CP was apparently, on average, well above 100% full over the course of a week.&lt;/p&gt; &lt;p&gt;Let‚Äôs see how this has happened.&lt;/p&gt;rnewbridge &lt;- df %&gt;% filter(Name == ‚ÄúNewbridge P+R‚Äù) %&gt;% mutate(Week = round_date(ymd_hms(LastUpdate), ‚Äúweek‚Äù)) %&gt;% group_by(Name, Week) %&gt;% summarize(meanP = mean(Percentage))\nplot(newbridge\\(Week, newbridge\\)meanP, main = ‚ÄúNewbridge P+R: Mean percentage per week‚Äù, pch = 20, xlab = ‚ÄúWeek‚Äù, ylab = ‚ÄúPercentage‚Äù)```\n\n\nWell, it‚Äôs immediately obvious where the problem is! That point at above 300% can only be explained by a broken exit sensor.\n\n\nTomorrow I‚Äôll sort out this problem and carry on with my initial aim.\n\n\n\nDay 09 (20/11/16): Mean occupancy per week\n\n\nContinuing from yesterday‚Äôs work, I‚Äôll add a couple of steps to the pipeline to cut out any weeks with a mean occupancy greater than 100%, and then to average by week.\n\nrdf2 &lt;- df %&gt;%     select(Name, LastUpdate, Percentage) %&gt;%     na.omit() %&gt;%     filter(Name != \"test car park\") %&gt;%     mutate(Week = round_date(ymd_hms(LastUpdate), \"week\")) %&gt;%     group_by(Name, Week) %&gt;%     summarize(meanP = mean(Percentage)) %&gt;%     filter(meanP &lt;= 100) %&gt;%     mutate(Week = week(Week)) %&gt;%     group_by(Name, Week) %&gt;%     summarize(meanP = mean(meanP))\n\nNow let‚Äôs have another look at the maximum values per car park:\n\nrmaxP &lt;- top_n(df2, n = 1) rconsole## Selecting by meanP rmaxP&lt;/span&gt; rconsole## Source: local data frame [8 x 3] ## Groups: Name [8] ##  ##                   Name  Week    meanP ##                 &lt;fctr&gt; &lt;dbl&gt;    &lt;dbl&gt; ## 1       Avon Street CP    22 62.19749 ## 2  Charlotte Street CP    38 46.25655 ## 3         Lansdown P+R    49 35.03728 ## 4        Newbridge P+R    46 54.86416 ## 5         Odd Down P+R    49 31.49185 ## 6            Podium CP    50 44.81424 ## 7 SouthGate General CP    51 53.58644 ## 8    SouthGate Rail CP    46 65.25588\n\nThat looks better!\n\n\nLet‚Äôs plot the data then. I‚Äôll add a label with the week number for the week with the maximum mean percentage occupancy.\n\n```rlibrary(ggplot2)\np &lt;- ggplot(df2, aes(x = Week, y = meanP)) + geom_line() + facet_wrap(~ Name, nrow = 2) + geom_label(data = maxP, aes(x = Week, y = meanP + 10, label = Week)) + ggtitle(‚ÄúMean percentage occupancy per week‚Äù) + ylab(‚ÄúPercentage‚Äù)\np``` \n\nOther than Avon Street CP, which seems to fluctuate wildly over the course of the year, and Charlotte Street CP, which interestingly seems to drop slightly around November, the maximum values are - as I had expected - generally towards the end of the year.\n\n\nSouthGate General CP actually has a noticeable peak in Week 51 - potentially due to last-minute Christmas shopping/Boxing Day sales.\n\n\nAnd two of the P+Rs have their max average occupancy in Week 49 - approximately the first week of December, right in the middle of the market period (my prediction was spot on!).\n\n\nAlso, for all car parks there is a noticeable tail-off in the last couple of weeks of the year, presumably due to most people preferring to be at home over the Christmas period than out and about in the town.\n\n\n\nDay 10 (21/11/16): Strange occupancies\n\n\nToday I received not one, but two pieces of coursework for two separate modules of my uni course - and as such I am going to be under serious time pressure for the next couple of weeks. So I will do my best to keep up with these posts but bear with me if they become a little sloppier or less detailed.\n\n\nI am going to start investigating some of the quirks of the dataset - to be honest, I probably should have done this earlier, before some of my other analyses, but I‚Äôll see how serious any issues are and then I can always revisit and correct previous visualizations.\n\n\nLet‚Äôs see some of the stranger records in the dataset - the records where a car park has zero occupancy (possible), &gt;100% occupancy (possible if the car park is full and more cars are circulating waiting for spaces, but probably uncommon), and negative occupancy (not possible and definitely due to dodgy sensors!).\n\nrlibrary(dplyr) library(lubridate) ```rdf3 &lt;- df %&gt;% filter(Occupancy &lt;= 0 | Occupancy &gt;= Capacity) %&gt;% group_by(Name)\nlibrary(ggplot2)\nggplot(df3, aes(x = ymd_hms(LastUpdate), y = Percentage)) + geom_point() + facet_wrap(~ Name, scales = ‚Äúfree_y‚Äù, nrow = 2) + ggtitle(‚ÄúStrange Occupancies‚Äù) + labs(x = ‚ÄúTime of record‚Äù, y = ‚ÄúPercentage occupancy‚Äù)``` \n\nOf particular note are the times when Newbridge P+R contained 4 times as many cars as its maximum capacity, and when SouthGate General CP contained about -20 times its maximum capacity (i.e.¬†about -14400 cars)."
  },
  {
    "objectID": "posts/2017-08-22-adding-a-git-bash-alias/index.html",
    "href": "posts/2017-08-22-adding-a-git-bash-alias/index.html",
    "title": "Adding a Git Bash alias",
    "section": "",
    "text": "I thought I‚Äôd share another little time-saving trick I discovered earlier.\nWhen I‚Äôm writing code I tend to use Git Bash to run commands, mostly because it (obviously!) works nicely with git and partly because‚Ä¶ well, I think it looks nice. It‚Äôs so colourful! This is a valid reason for liking something, right?\nAnyway, when you run a command in a terminal such as Git Bash, the first word is always an instruction to the terminal. The terminal decides:\n\n‚ÄúAh, I can do this!‚Äù (For shell commands such as cd, mkdir and echo); or\n‚ÄúNominate [program].‚Äù (For anything else)\n\n\nNow, my go-to code/plaintext editor is Notepad++. (It‚Äôs great - I‚Äôd highly encourage you to have a look!)\nTyping notepad [filename] opens the file in Notepad. But typing notepad++ [filename] doesn‚Äôt work. Very upsetting.\n\nThere is, however, a simple way to make this work - or, indeed, to add any little shortcut you like. These shortcuts are called aliases.\nYou define aliases in the .bashrc file, which contains settings that are loaded for Git Bash whenever you open it. This file lives in your home directory (in my case, my user folder ojones/) which can be quickly represented by a ~ in any commands.\nIf you‚Äôve never edited your .bashrc file before, you might not have one. So first we can quickly create one in our home directory:\ntouch ~/.bashrc\nThen open it with the editor of your choice (for example, vim):\nvim ~/.bashrc\nNow we‚Äôre going to add an alias for Notepad++ by adding the following line to the .bashrc file:\nalias npp=\"/c/Program\\ Files\\ \\(x86\\)/Notepad++/notepad++.exe\"\nThe string is just the unix-style filepath to where Notepad++ is installed on my computer(notice that we‚Äôve had to escape spaces and parentheses with backslashes). So we‚Äôve just told Git Bash that if it sees the command npp (for ‚Äúnotepad-plus-plus‚Äù) it should go to that filepath, which results in it launching Notepad++ for me. Hooray!\nBut you don‚Äôt have to stop there. You can set aliases for anything you like.\nalias npp=\"/c/Program\\ Files\\ \\(x86\\)/Notepad++/notepad++.exe\";\n\nalias gc=\"git commit -m \\\"Lazy commit\\\"\";\n\nalias doggo=\"printf $'\n            ____,\\'\\`-, \\n\n      _,--\\'   ,/::.: \\n\n   ,-\\'       ,/::,\\' \\`---.___        ___,_ \\n\n   |       ,:\\'::/        :\\'\"\\`:\"\\`--./ ,-^.:--. \\n\n   |:     ,:\\':,\\'         \\'         \\`.   :\\`   \\`-. \\n\n    \\:.,:::/:/ -:.                   \\`  | \\`     \\`-. \\n\n     \\:::,\\'//__.:  ,:  ,  ,  :.\\`-.   :. |  :       :. \\n\n      \\,\\',\\':/O)^. :\\'  :  :   \\'__\\` \\`  :::\\`.       .:\\' ) \\n\n      |,\\'  |\\__,: :      :  \\'/O)\\`.   :::\\`:       \\' ,\\' \\n\n           |\\`--\\'\\'            \\__,\\' , ::::(       ,\\' \\n\n           \\`    ,            \\`--\\' ,: :::,\\'\\   ,-\\' \\n\n            | ,:         ,    ,::\\'  ,:::   |,\\' \\n\n            |,:        .(          ,:::|   \\` \\n\n            ::\\'_   _   ::         ,::/:| \\n\n           ,\\',\\' \\`-\\' \\   \\`.      ,:::/,:| \\n\n          | : _  _   |   \\'     ,::,\\' ::: \\n\n          | \\ O\\`\\'O  ,\\',   ,    :,\\'   ::: \\n\n           \\ \\`-\\'\\`--\\',:\\' ,\\' , ,,\\'      :: \\n\n            \\`\\`:.:.__   \\',-\\',\\'        ::\\' \\n\n               \\`--.__, ,::.         ::\\' \\n\n                   |:  ::::.       ::\\' \\n\n                   |:  ::::::    ,::'\"\n(Having said that, if you want to create aliases specifically for git commands then you might want to use git‚Äôs own alias system rather than defining general aliases in .bashrc. There‚Äôs a good explanation of how to do that here.)"
  },
  {
    "objectID": "posts/2017-01-27-summary-of-30-days-30-visualizations-1-dataset/index.html",
    "href": "posts/2017-01-27-summary-of-30-days-30-visualizations-1-dataset/index.html",
    "title": "Summary of 30 Days, 30 Visualizations, 1 Dataset",
    "section": "",
    "text": "Last November, the Bath Machine Learning Meetup kicked off its first project: to use the BANES Historic Car Park Occupancy dataset to predict the occupancy of car parks at a given time in the future.\nSince mid-October 2014 the occupancy of each of eight car parks in Bath has been recorded every five minutes. Therefore the aforementioned dataset is pretty large, and constantly growing: it currently contains over 1.6 million records.\nBefore we began doing any serious work on the machine learning aspect of the project, I thought it would be a good idea to investigate the data we will be using; and inspired by similar analyses I had come across online I decided that I would try to produce one visualization of the data each day for one month.\nIf you are interested in the full, nitty-gritty, R-splattered details of how I went about this then feel free to have a look through my original posts. What follows here is a somewhat condensed version highlighting the interesting bits!"
  },
  {
    "objectID": "posts/2017-01-27-summary-of-30-days-30-visualizations-1-dataset/index.html#introduction",
    "href": "posts/2017-01-27-summary-of-30-days-30-visualizations-1-dataset/index.html#introduction",
    "title": "Summary of 30 Days, 30 Visualizations, 1 Dataset",
    "section": "",
    "text": "Last November, the Bath Machine Learning Meetup kicked off its first project: to use the BANES Historic Car Park Occupancy dataset to predict the occupancy of car parks at a given time in the future.\nSince mid-October 2014 the occupancy of each of eight car parks in Bath has been recorded every five minutes. Therefore the aforementioned dataset is pretty large, and constantly growing: it currently contains over 1.6 million records.\nBefore we began doing any serious work on the machine learning aspect of the project, I thought it would be a good idea to investigate the data we will be using; and inspired by similar analyses I had come across online I decided that I would try to produce one visualization of the data each day for one month.\nIf you are interested in the full, nitty-gritty, R-splattered details of how I went about this then feel free to have a look through my original posts. What follows here is a somewhat condensed version highlighting the interesting bits!"
  },
  {
    "objectID": "posts/2017-01-27-summary-of-30-days-30-visualizations-1-dataset/index.html#trends-in-car-park-occupancy",
    "href": "posts/2017-01-27-summary-of-30-days-30-visualizations-1-dataset/index.html#trends-in-car-park-occupancy",
    "title": "Summary of 30 Days, 30 Visualizations, 1 Dataset",
    "section": "Trends in car park occupancy",
    "text": "Trends in car park occupancy\nI started off by having a look at some of the general trends in the data, such as how full each car park is, on average, on each day of the week.\n\n\nAs you might expect, in general the car parks tend to fill during the morning and empty in the afternoon, with most reaching their peak occupancy at about 13:00. However, there are some subtle points of interest here.\nFor example, during the working week the ‚Äúpeak‚Äù for SouthGate Rail car park is noticeably broader and squarer than those of the other car parks. This is due to the fact that SouthGate Rail is used only by railway commuters: therefore it tends to fill up quickly early in the morning, and then empty quickly later in the evening.\nHowever, if we compare this to the weekend we see that SouthGate Rail is not so busy and instead the Podium and SouthGate General car parks tend to be busiest - this perhaps isn‚Äôt surprising, seeing as these are the closest car parks to the town centre and therefore are generally used by people visiting the local shops. Interestingly Friday is closer to this pattern than it is to the other weekdays, suggesting perhaps that Friday is a popular day to work from home.\nThe dataset also includes a ‚ÄúStatus‚Äù column, indicating the change in occupancy from one record to the next (although this doesn‚Äôt seem to be recorded correctly for Podium car park).\n\nThis helps us to see how ‚Äúdynamic‚Äù the occupancy of each car park is: for example, SouthGate Rail and the P+R car parks spend a significant amount of time as ‚ÄúStatic‚Äù (meaning that there was no net change in occupancy from the previous record), whereas the shopping car parks such as SouthGate General spend more time filling or emptying. While ‚ÄúStatic‚Äù doesn‚Äôt necessarily mean no cars entered or exited, this still gives us a good idea of which car parks have a higher turnaround of cars.\nSo far we‚Äôve been looking at a breakdown by weekday, but we can see other interesting trends if we look at different time periods. The plot below shows the average occupancy of each car park in each week of the year (the label shows the week with the highest average occupancy).\nWhile we see some wild fluctuations, there is a point worth noting here: the P+R car parks in particular tend to be used most around weeks 46-48, which translates to the latter half of November through to early December. I have absolutely no idea why this is the case."
  },
  {
    "objectID": "posts/2017-01-27-summary-of-30-days-30-visualizations-1-dataset/index.html#dubious-records",
    "href": "posts/2017-01-27-summary-of-30-days-30-visualizations-1-dataset/index.html#dubious-records",
    "title": "Summary of 30 Days, 30 Visualizations, 1 Dataset",
    "section": "Dubious records",
    "text": "Dubious records\nOn the surface, these trends shine through; the result of taking averages of various combinations of over 1.6 million records. But dig a little deeper and it turns out that like any genuine real-life dataset, this one is full of quirks.\nLet‚Äôs strip down that last chart and just look at one car park: Newbridge P+R.\n\nIt‚Äôs clear that we have some outlying points here. It turns out that if we start looking for them, then there are quite a lot of ‚Äústrange‚Äù occupancies.\n\nWhereas a car park at 0% or 100% capacity might not be too unusual, a car park at more than 100% capacity shouldn‚Äôt be too common. Having said that, on occasion this can be explained - as the dataset‚Äôs documentation notes - by cars being parked in unofficial spaces, or by cars prowling full car parks in hope of nabbing a spot as someone else leaves. (Notice most of the full/overfilled records are from the shopping car parks.)\nBut we also have a large number of records where there were somehow fewer than zero cars in the car park. Obviously when this happens there must have been some sort of error - and in most cases, the method of data collection is the culprit.\nAgain referring to the documentation we learn that occupancy is measured by keeping a running total of two counts - entries and exits. For each car park, there is a sensor on each gate. When a car goes in, we add 1 to the occupancy, and when a car leaves, we subtract 1. Simple enough, and generally effective.\nEvery so often though one of these sensors fails, and we end up with a monotonic increase or decrease in occupancy until someone notices and runs down to the offending car park to change the sensor. The occupancy is then manually recalibrated (this happens on occasion anyway, usually overnight). Broken sensors explain the vast majority of the remaining ‚Äústrange‚Äù records - in the case of the negative occupancy records, 99.8% of them are relatively close to zero.\n\nBut then again, some aren‚Äôt relatively close to zero. The 26 records where an occupancy of about -15000 is recorded all come from SouthGate General car park, from a single afternoon - there is just an immediate drop from one record to the next, and then an immediate rise a couple of hours later. But even if we shift the records back up to approximately where they should be (by adding about 15220 to the recorded occupancy) we can see that they don‚Äôt line up with the records on either side.\n\nOK, in case it wasn‚Äôt clear, I wasn‚Äôt entirely serious earlier when I said I didn‚Äôt know why the P+Rs were busiest in early December (I think anyone who‚Äôs lived in Bath knows why!). However, in the case of these rogue records I genuinely have no idea why they are so wrong; my best guess is some sort of error in the process of uploading the records from the sensors to the online database.\nI can also now confess that the plots which appear in the first section of this piece are based on a somewhat tidied version of the data where I have removed any dubious records. These records do have an effect on calculated averages, as can be seen by comparing the plots below - the second plot is identical to the plot from earlier, the first is the same plot but with dubious records included."
  },
  {
    "objectID": "posts/2017-01-27-summary-of-30-days-30-visualizations-1-dataset/index.html#repeated-records",
    "href": "posts/2017-01-27-summary-of-30-days-30-visualizations-1-dataset/index.html#repeated-records",
    "title": "Summary of 30 Days, 30 Visualizations, 1 Dataset",
    "section": "Repeated records",
    "text": "Repeated records\nThere are two date-time values associated with each record: LastUpdate, which is the time that the record was taken, and DateUploaded, which is the time that the record was uploaded to the Bath Hacked datastore.\nIn theory, a new record is taken at each car park every 5 minutes, and then the eight new records are uploaded to the datastore in a batch shortly afterwards. In reality, a new record is usually taken every 5 minutes, and the most recent record from each car park is usually uploaded in a batch shortly afterwards.\nOccasionally a new record is not taken before the next upload is due. When this occurs the previous record is just uploaded again - with the same LastUpdate time, but a new DateUploaded time. This ensures that a fresh batch of 8 records (one from each car park) is added to the datastore every 5 minutes.\nTherefore if we judge the ‚Äúuniqueness‚Äù of a record by LastUpdate we end up with quite a few duplicate records. In fact about 17% of all records in the dataset are repeated uploads. The vast majority of records which are repeated are only repeated once; but in the most extreme cases the same record is uploaded over 1000 times before a new record is taken.\n\nThe delay between LastUpdate and DateUploaded is generally very short, as expected; but sometimes a very long time can pass before a new record is taken. Don‚Äôt be too shocked by the graph below - it uses a log scale on the y-axis, so the overwhelming majority of records are uploaded shortly after being taken. However, if no new record is taken in a 5-minute interval then a repeat record is uploaded instead, so the difference between LastUpdate and DateUploaded increases: the largest difference is over 300000 seconds (which equates to about 3.5 days).\n\nWe can see that there are also a surprisingly large number of records where the record was apparently uploaded to the datastore before it was actually recorded. All such records are from the two SouthGate car parks, but again I don‚Äôt have a good explanation for why this occurs.\nNow, just a few short paragraphs ago I claimed that every batch uploaded to the datastore contained 8 records. Alas, once again I was slightly deceitful: on occasion there are batches containing as few as 4 records.\n\n(Note: the y-axis label on the right-hand plot should more accurately read: ‚ÄúOf all the records which were uploaded to the datastore in a batch of this size, the proportion which came from each of the car parks‚Äù. But this is rather a mouthful.)"
  },
  {
    "objectID": "posts/2017-01-27-summary-of-30-days-30-visualizations-1-dataset/index.html#closing-thoughts",
    "href": "posts/2017-01-27-summary-of-30-days-30-visualizations-1-dataset/index.html#closing-thoughts",
    "title": "Summary of 30 Days, 30 Visualizations, 1 Dataset",
    "section": "Closing thoughts",
    "text": "Closing thoughts\nI was pleasantly surprised when I was asked to produce this write-up for Bath Hacked. While carrying out this project I never really thought I was producing something that other people would eventually be interested in reading!\nI‚Äôm not ashamed to admit that when I set out upon this prolonged analytical journey back in early November I didn‚Äôt really know what I was doing. To begin with I intended it to be purely a learning exercise; a means of improving my data manipulation and visualization skills using R. And it did indeed serve this purpose very well.\nHowever, once I began investigating the dataset in more and more detail, I found myself becoming more and more interested in what I was finding, and more and more focused in what I was looking for. When I finished the project thirty days after I began, I was relieved to be free of the burden of it, but simultaneously a little sad that a previously (admittedly briefly) constant part of my life would now be something to look back on rather than forward to.\nI suppose if you spend enough time with anything and get to know it well enough you risk becoming somewhat emotionally attached to it.\n\nThis post was featured on the Bath: Hacked blog (and if you want to read it again there, by all means feel free to do so! You can find it here)."
  },
  {
    "objectID": "posts/2016-10-02-ie-automation-with-vba-in-microsoft-excel/index.html",
    "href": "posts/2016-10-02-ie-automation-with-vba-in-microsoft-excel/index.html",
    "title": "IE Automation with VBA in Excel",
    "section": "",
    "text": "A bit of context: as well as being a super-hard-working maths student (‚Ä¶), I am a peer mentor for eight first-year students who have just started maths courses at the university. I am, in fact, a ‚Äòlead‚Äô peer mentor - generally speaking this doesn‚Äôt entail much more work than the peer mentoring itself, but occasionally there is a little bit of work to be done around the management of the peer mentoring scheme within the department.\n\n\nA little while ago, I was sent an email by the maths department‚Äôs Director of Studies asking if I would mind helping to mix up and then divide the new intake of maths students between the 42 peer mentors. Attached to this email was a list of the names of the new students in an Excel spreadsheet.\n\n\nSkipping forward to the point where I needed to let the other peer mentors know who they needed to contact, I decided that seeing as I had nothing else to be doing, I could save my fellow mentors a bit of time by finding the email addresses of the new students myself on the university computing system.\n\n\nFour or five names in, however, I was already reassessing my benevolent decision - faced with another 320-odd names, and not willing to admit defeat, I decided to see if I could somehow automate the process.\n\n\nSeeing as I was working in Excel, I came up with the idea of writing a VBA program that would:\n\n\n\nOpen my webmail client\n\n\nSearch for a person\n\n\nCopy their email address into the Excel spreadsheet, next to their name\n\n\n\nA lot of experimentation and a lot of digging around in HTML source code later, a very rough-and-ready, very situation-specific, but nonetheless functional program did eventually emerge.\n\n\nHere‚Äôs the code:\n\nSub findemail()\n \n' Launch an Internet Explorer window\nSet IE = CreateObject(\"InternetExplorer.Application\")\n \nIE.Visible = True\n \n' Navigate to webmail client\nIE.Navigate \"http://mail.bath.ac.uk/\"\n \n' Update Excel status bar\nApplication.StatusBar = \"Bath mail client loading...\"\n \n' Wait for page to load\nDo While IE.Busy\nApplication.Wait DateAdd(\"s\", 1, Now)\nLoop\n \n' Add short delay to ensure all elements fully loaded\nApplication.Wait DateAdd(\"s\", 2, Now)\n \n' Update Excel status bar\nApplication.StatusBar = \"Finding mentee...\"\n \n' Find \"People\" button on menu bar at top of webpage\nSet topButtons = IE.document.getElementById(\"_ariaId_20\")\n \ntopButtons.Click\n \n' Wait for page to load\nDo While IE.Busy\nApplication.Wait DateAdd(\"s\", 1, Now)\nLoop\n \n' Add short delay to ensure all elements fully loaded\nApplication.Wait DateAdd(\"s\", 2, Now)\n \n' Find \"Search people\" box\nSet searchbox = IE.document.getElementsByClassName(\"_n_o1 o365button \" & _\n\"ms-font-m ms-border-color-themeLighter\")\n' Workaround to resolve issue occuring when trying to click single object\nFor Each obj In searchbox\nobj.Click\nNext\n \n' Add short delay for loading\nApplication.Wait DateAdd(\"s\", 1, Now)\n \n' Input name from spreadsheet in \"Forename Surname\" format\nSendKeys ActiveSheet.Range(\"G\" & ActiveCell.Row).Value & \" \" & _\nActiveSheet.Range(\"F\" & ActiveCell.Row).Value\n \n' Wait to ensure name is pasted\nApplication.Wait DateAdd(\"s\", 1, Now)\n \n' Find search button\nSet searchButton = _\nIE.document.getElementsByClassName(\"_n_z searchImgWidth o365button ms-font-m\")\n \n' Workaround for single-object click issue mentioned earlier\nFor Each obj In searchButton\nIf obj.Type = \"button\" Then\nobj.Click\nEnd If\nNext\n \n' Update Excel status bar\nApplication.StatusBar = \"Getting mentee email...\"\n \n' Give results time to load\nApplication.Wait DateAdd(\"s\", 2, Now)\n \n' Create list of all results from search\nSet people = IE.document.getElementsByClassName(\"_pe_b _pe_s\")\n \n' There are 2 \"invisible\" non-result elements of the same class as the results,\n' so a length-3 list means 1 person found\n' If no people or multiple people found, give an error message (manual\n' intervention needed later)\n' Else find the person's email address on the page, and copy it into the\n' spreadsheet\nIf people.Length &lt;&gt; 3 Then\n \nActiveCell.Value = \"ERROR!\"\n \nElse\n \nemailAddress = IE.document.getElementsByClassName(\"_rpc_41 ms-font-s \" & _\n\"allowTextSelection _rpc_m1 ms-font-color-themePrimary \" & _\n\"_rpc_p1\").Item(0).innerText\n \nActiveCell.Value = emailAddress\n \nEnd If\n \n' Close IE window\nIE.Quit\n \n' Clear Excel status bar\nApplication.StatusBar = False\n \nEnd Sub\n \n'=============================================================================\n \nSub Go()\n \nFor Each cell In ActiveSheet.Range(\"J2:J326\")\ncell.Select\nfindemail\n' Wait to ensure process has finished before moving on to next cell\nApplication.Wait DateAdd(\"s\", 2, Now)\nNext\n \nEnd Sub\n \n'=============================================================================\n \nSub GoFrom()\n \nFor Each cell In ActiveSheet.Range(ActiveCell, \"J326\")\ncell.Select\nfindemail\n' Wait to ensure process has finished before moving on to next cell\nApplication.Wait DateAdd(\"s\", 2, Now)\nNext\n \nEnd Sub\n\nAnd here‚Äôs the program in action:\n\n\n\n\n(Sorry, it looks like your browser doesn‚Äôt like my video!) \n\nWriting the program, and running the whole list of names through it, probably took much longer than it would have done to find the email addresses manually; and certainly it would have been much easier to take the divide-and-conquer approach and ask the other mentors to find their own mentees‚Äô email addresses themselves. However, I did feel immensely proud of myself for rising to the challenge I had set myself, for creating a working program in a language I wasn‚Äôt too familiar with and for helping out my fellow mentors.\n\n\nThe following day, I was sent an updated list of names by the department - with an extra column containing each student‚Äôs email address."
  },
  {
    "objectID": "posts/2016-11-12-30-days-30-visualizations-1-dataset-part-1/index.html",
    "href": "posts/2016-11-12-30-days-30-visualizations-1-dataset-part-1/index.html",
    "title": "30 Days, 30 Visualizations, 1 Dataset",
    "section": "",
    "text": "I am currently involved with a very exciting machine learning project being run within the Bath Machine Learning Meetup group (I‚Äôm sure I‚Äôll write a future post with more details on this project, but it is currently at a very early stage!). In collaboration with Bath: Hacked we are using this set of open data, which contains information about the occupancy of car parks in the Bath & North East Somerset (B&NES) area from the last two years or so.\nWhile looking for something to listen to whilst making my daily bus commute to and from the university, I recently discovered the marvellous Partially Derivative podcast. In one of the early episodes (S1E2, to be precise) one topic of conversation particularly caught my attention: Paul Downey‚Äôs blog post ‚ÄúOne CSV, thirty stories‚Äù. The concept is simple: take a single dataset, and produce a different visualization of the data each day for 30 days.\nSeeing as we will be working with the car parking dataset a great deal in coming months, and inspired by Downey‚Äôs post, I thought I would try something similar. So, without further ado‚Ä¶\n\n\n\nI thought it would be a good idea to have a quick look at the dataset before starting out. I will, of course, be using R (to start with at least!).\nI downloaded the dataset as a CSV file from the Bath: Hacked datastore (the link is above) - the file on the website is updated continuously with live data from sensors in the car parks. The version I downloaded (at 22:11 on 11/11/16) weighs in at a hefty 316MB, and contains a very large number of records:\nlength(count.fields(\"data/BANES_Historic_Car_Park_Occupancy.csv\"))\n## [1] 1528228\nTo avoid converting my Pentium-core laptop into a puddle of molten plastic, for now I‚Äôll only try to deal with about 10% of these records. Looking at the dataset‚Äôs documentation we see that ‚Äúscripts are set up to query the B&NES car park database every 5 minutes, the data is then pushed to the Bath: Hacked data store to‚Ä¶ append to a historical set‚Äù. Therefore it shouldn‚Äôt be a problem to read in only the first 150000 rows of the CSV file, since these rows should contain an equal spread of data from each of the distinct car parks.\nLet‚Äôs have a quick first look at our data! Note: a few records are empty in certain fields (future investigation may be necessary‚Ä¶) - I‚Äôll fill these fields with NAs for now to make it more obvious when data is missing.\nD &lt;- read.csv(\"data/BANES_Historic_Car_Park_Occupancy.csv\", nrow = 150000,\n              na.strings = c(\"NA\", \"\"))\nstr(D)\n## 'data.frame':    150000 obs. of  12 variables:\n##  $ ID          : Factor w/ 150000 levels \"000016ca6149d3f3bed350cf3b37f854\",..: 51364 64067 64678 65738 69335 113228 134317 134563 4865 17729 ...\n##  $ LastUpdate  : Factor w/ 88741 levels \"01/01/2015 01:00:01 AM\",..: 28126 28128 28131 28126 28124 28130 28128 28134 28346 28354 ...\n##  $ Name        : Factor w/ 8 levels \"Avon Street CP\",..: 7 1 3 8 6 4 2 5 8 5 ...\n##  $ Description : Factor w/ 8 levels \"Avon Street CP / A367 Green Park Road / Corn Street / CP / Bath\",..: 7 1 3 8 6 4 2 5 8 5 ...\n##  $ Capacity    : int  720 630 827 140 521 698 1056 1252 140 1252 ...\n##  $ Status      : Factor w/ 3 levels \"Emptying\",\"Filling\",..: 3 2 3 3 3 3 2 3 3 3 ...\n##  $ Occupancy   : int  28 32 8 15 75 21 124 0 15 0 ...\n##  $ Percentage  : int  3 5 1 10 14 3 12 0 10 0 ...\n##  $ Easting     : int  375115 374884 373183 375083 375109 371853 374445 373363 375083 373363 ...\n##  $ Northing    : int  164421 164469 168104 164424 165083 165766 165097 161610 164424 161610 ...\n##  $ DateUploaded: Factor w/ 23983 levels \"01/01/2015 01:00:01 AM\",..: 7636 7636 7636 7636 7636 7636 7636 7636 7675 7675 ...\n##  $ Location    : Factor w/ 8 levels \"(51.352935229, -2.38389427175)\",..: 2 4 8 3 5 7 6 1 3 1 ...\nhead(D)\n##                                 ID             LastUpdate\n## 1 577ffcf1cf17544c6030eef4ba85663e 13/02/2016 02:08:46 AM\n## 2 6d383b77bce3b7c87153ccf141158140 13/02/2016 02:09:36 AM\n## 3 6e415c9bf6c8231a189d11896ed42b04 13/02/2016 02:09:48 AM\n## 4 70151449ba6bcd7d28ee000b3042c7f9 13/02/2016 02:08:46 AM\n## 5 762bc1f6aafc736dabc1e4a6d006598e 13/02/2016 02:06:14 AM\n## 6 c0ffd83d29df5bb8bb5ee83c8122de28 13/02/2016 02:09:47 AM\n##                   Name\n## 1 SouthGate General CP\n## 2       Avon Street CP\n## 3         Lansdown P+R\n## 4    SouthGate Rail CP\n## 5            Podium CP\n## 6        Newbridge P+R\n##                                                                   Description\n## 1 SouthGate General CP / A367 St James Parade / Dorchester Street / CP / Bath\n## 2             Avon Street CP / A367 Green Park Road / Corn Street / CP / Bath\n## 3                               Lansdown P+R / Lansdown Road / P+R / Lansdown\n## 4    SouthGate Rail CP / A367 St James Parade / Dorchester Street / CP / Bath\n## 5                                       Podium CP / Walcot Street / CP / Bath\n## 6                              Newbridge P+R / A4 Newbridge Road / P+R / Bath\n##   Capacity  Status Occupancy Percentage Easting Northing\n## 1      720  Static        28          3  375115   164421\n## 2      630 Filling        32          5  374884   164469\n## 3      827  Static         8          1  373183   168104\n## 4      140  Static        15         10  375083   164424\n## 5      521  Static        75         14  375109   165083\n## 6      698  Static        21          3  371853   165766\n##             DateUploaded                        Location\n## 1 13/02/2016 02:10:01 AM (51.3782901111, -2.35893376096)\n## 2 13/02/2016 02:10:01 AM (51.3787114813, -2.36225609279)\n## 3 13/02/2016 02:10:01 AM (51.4113171399, -2.38697144144)\n## 4 13/02/2016 02:10:01 AM (51.3783156754, -2.35939373665)\n## 5 13/02/2016 02:10:01 AM (51.3842422001, -2.35906657317)\n## 6 13/02/2016 02:10:01 AM (51.3902305335, -2.40590782211)\nLooking interesting‚Ä¶ but I won‚Äôt look too closely for now. The fun starts tomorrow!\n\n\n\n\nA logical first step would be to work out where all the information we have is coming from!\nA week or two ago I attended a learning night run by Bath: Hacked, which was an introduction to using the mapping tool Carto (formerly CartoDB) for visualizing geographic data. And notice that we have some location information‚Ä¶\n# D is our 150000-row dataframe of records\n\n# Reduce D to a new dataframe, which consists only of the first row to contain\n# each car park name\ncarpark_df &lt;- D[!duplicated(D$Name), ]\n\n# The Location column contains character vectors of latitude and longitude in\n# the form \"(lat, lng)\": let's separate these values and make them numeric\nlocations &lt;- strsplit(sapply(carpark_df$Location, gsub,\n                             pattern = \"\\\\(|\\\\)\", replacement = \"\"),\n                      \", \")\n\nlat &lt;- as.numeric(sapply(locations, \"[[\", 1))\nlng &lt;- as.numeric(sapply(locations, \"[[\", 2))\n\n# Now construct a dataframe for Carto with the info we're interested in\ncarto_df &lt;- cbind(carpark_df[, c(\"Name\", \"Description\", \"Capacity\")],\n                  \"Longitude\" = lng, \"Latitude\" = lat)\n\n# Let's have a look:\ncarto_df\n##                   Name\n## 1 SouthGate General CP\n## 2       Avon Street CP\n## 3         Lansdown P+R\n## 4    SouthGate Rail CP\n## 5            Podium CP\n## 6        Newbridge P+R\n## 7  Charlotte Street CP\n## 8         Odd Down P+R\n##                                                                    Description\n## 1  SouthGate General CP / A367 St James Parade / Dorchester Street / CP / Bath\n## 2              Avon Street CP / A367 Green Park Road / Corn Street / CP / Bath\n## 3                                Lansdown P+R / Lansdown Road / P+R / Lansdown\n## 4     SouthGate Rail CP / A367 St James Parade / Dorchester Street / CP / Bath\n## 5                                        Podium CP / Walcot Street / CP / Bath\n## 6                               Newbridge P+R / A4 Newbridge Road / P+R / Bath\n## 7 Charlotte Street CP / A4 Charlotte Street / Marlbourgh Buildings / CP / Bath\n## 8                                             Odd Down P+R / A367 / P+R / Bath\n##   Capacity Longitude Latitude\n## 1      720 -2.358934 51.37829\n## 2      630 -2.362256 51.37871\n## 3      827 -2.386971 51.41132\n## 4      140 -2.359394 51.37832\n## 5      521 -2.359067 51.38424\n## 6      698 -2.405908 51.39023\n## 7     1056 -2.368609 51.38434\n## 8     1252 -2.383894 51.35294\n# Write this to a CSV ready for upload\nwrite.csv(carto_df, file = \"carto_df.csv\")\nSo let‚Äôs see where these car parks are!\n\n\n\n\n\n\nMaking use of the small dataframe I created yesterday (which has one entry per car park), and for now sticking to R‚Äôs base graphics:\ndf &lt;- read.csv(\"carto_df.csv\")\n\n# Re-order by Capacity\ndf &lt;- df[order(df$Capacity), ] \n                        \nbp &lt;- barplot(df$Capacity, main = \"Car park capacities\",\n              xlab = \"Number of spaces\", xlim = c(0, 1400), horiz = TRUE,\n              col = cm.colors(8), mgp = c(2.5, 1, 0))\n\ntext(bp, labels = df$Name, pos = 4)\n\n\n\n\n\nIt crossed my mind earlier today that I had made a couple of assumptions about the data I‚Äôm working with - primarily, I assumed that for a given car park, each record would have the same values for certain columns. Before doing anything else, I thought I should check that this was actually the case.\nThe columns in question are: Location, Easting, Northing (which are all related, of course) and Capacities.\n# Read in all the data - caution to the wind, if my laptop melts then it melts\ndf &lt;- read.csv(\"data/BANES_Historic_Car_Park_Occupancy.csv\")\n\n# Create a list of the car parks\nnames &lt;- unique(df$Name)\nnames\n## [1] SouthGate General CP Avon Street CP       Lansdown P+R        \n## [4] SouthGate Rail CP    Podium CP            Newbridge P+R       \n## [7] Charlotte Street CP  Odd Down P+R         test car park       \n## 9 Levels: Avon Street CP Charlotte Street CP ... test car park\n# Notice the appearance of the mysterious \"test car park\"!\n# (Missed that yesterday too!)\n# Check for unique values of each dubious column, per car park\nlocations &lt;- lapply(names, function(name) unique(df$Location[df$Name == name]))\n\neastings &lt;- lapply(names, function(name) unique(df$Easting[df$Name == name]))\n\nnorthings &lt;- lapply(names, function(name) unique(df$Northing[df$Name == name]))\n\ncapacities &lt;- lapply(names, function(name) unique(df$Capacity[df$Name == name]))\n\nlocations\n## [[1]]\n## [1] (51.3782901111, -2.35893376096)\n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[2]]\n## [1] (51.3787114813, -2.36225609279)\n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[3]]\n## [1] (51.4113171399, -2.38697144144)\n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[4]]\n## [1] (51.3783156754, -2.35939373665)\n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[5]]\n## [1] (51.3842422001, -2.35906657317)                                \n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[6]]\n## [1] (51.3902305335, -2.40590782211)                                \n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[7]]\n## [1] (51.3843384358, -2.36860892385)\n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[8]]\n## [1] (51.352935229, -2.38389427175)\n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[9]]\n## [1] \n## 9 Levels:  ... (51.4113171399, -2.38697144144)\neastings\n## [[1]]\n## [1] 375115\n## \n## [[2]]\n## [1] 374884\n## \n## [[3]]\n## [1] 373183\n## \n## [[4]]\n## [1] 375083\n## \n## [[5]]\n## [1] 375109\n## \n## [[6]]\n## [1] 371853\n## \n## [[7]]\n## [1] 374445\n## \n## [[8]]\n## [1] 373363\n## \n## [[9]]\n## [1] NA```\n\n\n```r\nnorthings\n## [[1]]\n## [1] 164421\n## \n## [[2]]\n## [1] 164469\n## \n## [[3]]\n## [1] 168104\n## \n## [[4]]\n## [1] 164424\n## \n## [[5]]\n## [1] 165083\n## \n## [[6]]\n## [1] 165766\n## \n## [[7]]\n## [1] 165097\n## \n## [[8]]\n## [1] 161610\n## \n## [[9]]\n## [1] NA```\n\n\n\n\n```r\ncapacities\n## [[1]]\n##  [1] 720 680 900 700 626 800 860 638 603 602 630 596 623 519 528 563 840\n## \n## [[2]]\n## [1] 630 590 230\n## \n## [[3]]\n## [1]  827 1827  860\n## \n## [[4]]\n## [1] 140 200 180 132\n## \n## [[5]]\n## [1] 521\n## \n## [[6]]\n## [1] 698 470 450 600\n## \n## [[7]]\n## [1] 1056  526\n## \n## [[8]]\n## [1] 1252 1320\n## \n## [[9]]\n## [1] 500\nThe first three columns only have one value per car park - great, our car parks aren‚Äôt moving!\nHowever‚Ä¶ it does seem that some capacities are not constant. Let‚Äôs try to see how they change over time.\n# Create new dataframe with columns of interest only, and taking out \"test car\n# park\" data since this isn't of any interest to us\ndf2 &lt;- df[!(df$Name == \"test car park\"), c(\"Name\", \"LastUpdate\", \"Capacity\")]\n\n# Reformat \"LastUpdate\" column as POSIXct datetime, rather than character\ndf2$LastUpdate &lt;- as.POSIXct(df2$LastUpdate, format = \"%d/%m/%Y %I:%M:%S %p\")\n\n# We'll use ggplot2 for our plotting today\nlibrary(ggplot2)\n\np &lt;- ggplot(data = df2, aes(x = LastUpdate, y = Capacity, group = Name)) +\n    geom_line(aes(colour = Name), size = 1.5) +\n    scale_x_datetime(date_labels = \"%b %Y\") + xlab(\"\") + ylab(\"Capacity\") +\n    guides(colour = guide_legend(override.aes = list(size = 3))) +\n    ggtitle(\"Capacities over time\") +\n    theme(plot.title = element_text(size = 22, face = \"bold\"))\n\n# Plot it!\np\n\nThe plot reveals some interesting information about the ‚Äúchanges‚Äù in capacities. For Newbridge P+R, the capacity increases over time, and remains level after each increase. It seems likely, then, that this is a genuine increase in capacity over time due to extensions to the car park.\nFor the other car parks, any changes are suspiciously short-lived and the capacities return to their previous values soon after any change. For this reason I think it is likely that these are mostly misrecorded values - although of course it is possible that there were short-term, temporary extensions/closures of the car parks.\nThe records for Avon Street CP also end abruptly and early in mid-summer of this year.\nBy pure chance, my visualization from yesterday (of maximum capacities) was using records from February 2016 - a time of relative stability, and when Newbridge P+R had reached its current maximum capacity - and so it is still, I would say, a valid representation of the data. But having had a much better look at the data today it is obvious I got a bit lucky!\n\n\n\n\nIt‚Äôs a long title today. This is reflective of the long time I have spent trying to re-order the panels in today‚Äôs visualization, without success‚Ä¶ more details shortly.\nFirst, let‚Äôs get the data we‚Äôre interested in today.\n# Separate interesting columns from dataframe of all records\ndf3 &lt;- df[!(df$Name == \"test car park\"), c(\"Name\", \"Percentage\", \"LastUpdate\")]\n\n# Convert LastUpdate to POSIXct and store in a new vector\nupdates &lt;- as.POSIXct(df3$LastUpdate, format = \"%d/%m/%Y %I:%M:%S %p\",\n                      tz = \"UTC\")\n\n# Change every datatime to \"time to nearest 10 minutes\" (600 seconds)\ntimes &lt;- as.POSIXct(round(as.double(updates) / 600) * 600,\n                    origin = as.POSIXlt(\"1970-01-01\"), tz = \"UTC\")\n\ndecimal_times &lt;- as.POSIXlt(times)$hour + as.POSIXlt(times)$min/60\n\n# Change every datetime to weekday abbreviation\ndays &lt;- format(updates, \"%a\")\n\n# Add these new columns to our dataframe\ndf4 &lt;- cbind(df3, \"Day\" = days, \"Time\" = decimal_times)\n\n# Take average of Percentage over each timeblock, per day, per car park\ndf5 &lt;- aggregate(df4$Percentage,\n                 list(\"Time\" = df4$Time,\n                      \"Day\" = df4$Day,\n                      \"Name\" = df4$Name),\n                 mean)\nLet‚Äôs have a quick look at what we‚Äôve achieved:\nhead(df5)\n##        Time Day           Name        x\n## 1 0.0000000 Fri Avon Street CP 7.366279\n## 2 0.1666667 Fri Avon Street CP 6.670732\n## 3 0.3333333 Fri Avon Street CP 6.456140\n## 4 0.5000000 Fri Avon Street CP 6.465839\n## 5 0.6666667 Fri Avon Street CP 5.774566\n## 6 0.8333333 Fri Avon Street CP 6.361963\nnrow(df5)\n## [1] 8064\nReassuringly, df5 contains 8064 entries:\n6{10-min intervals per hour}*24{hours}*7{days}*8{car parks} = 8064\nNow let‚Äôs create the plot!\nlibrary(ggplot2)\n\np &lt;- ggplot(data = df5, aes(x = as.double(Time), y = df5$x, group = Name)) +\n    facet_wrap(~ Day) + labs(y = \"Percentage occupancy\", x = \"Time (hour)\") +\n    geom_line(aes(colour = Name)) +\n    guides(colour = guide_legend(override.aes = list(size = 3)))\np\n\nLooking good - except for two things:\n\nx-axis labels are not nicely formatted\nPanels are not sensibly ordered\n\nI have tried at some length to solve each of these issues - particularly the second, which I thought I could solve by reordering the underlying factor:\ndf5$Day1 &lt;- factor(df5$Day,  levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\",\n                                        \"Sat\", \"Sun\"))\nBut plotting again faceting by Day1 instead of Day only reorders the labels, and does not reorder the plots themselves - i.e.¬†Wednesday‚Äôs data is now labelled as Sunday. This is obviously worse than what I currently have (it‚Äôs plain wrong, rather than just ‚Äúnot pretty‚Äù) so I haven‚Äôt included it here.\nI am going to try to stick to the plan of one visualization per day - but along with my already busy schedule and regular maths workload, I am obviously putting myself under a great deal of time pressure! At best I have a couple of hours to dedicate to this project on any given day, and this isn‚Äôt very long considering I am learning pretty much from scratch as I go along!\nI‚Äôm hoping, then, that this won‚Äôt become a common refrain, but‚Ä¶ I‚Äôve run out of time today! I‚Äôll revisit this visualization at some point soon if I figure out what I was doing wrong.\n\n\n\n\nBefore doing anything else, I‚Äôm going to remove the redundant columns from the dataframe of all records, and save the new smaller dataframe to a CSV file - which should then be quicker to load and easier work with.\ndf &lt;- df[, c(\"Name\", \"LastUpdate\", \"Capacity\", \"Occupancy\", \"Percentage\", \"Status\")]\n\nwrite.csv(df, file = \"df.csv\")\ndf &lt;- read.csv(\"C:/Users/Owen/Documents/Coding/Parking/df.csv\")\nFollowing another moment of mild panic this morning, I‚Äôm also going to check that the times being recorded are adjusted for GMT/BST each spring and autumn. If they are then I don‚Äôt need to worry - if not, I‚Äôll have to do some time-correction work‚Ä¶\n# I'll have a look at records from spring of 2016 - clocks changed on 27/03/2016\ntimes &lt;- df$LastUpdate[grep(\"^27/03/2016\", df$LastUpdate)]\nhours &lt;- as.POSIXlt(times, format = \"%d/%m/%Y %I:%M:%S %p\", tz = \"UTC\")$hour\n\n# Check number of records for a few hours, including 1AM (shouldn't be any!)\nc(sum(hours == 1), sum(hours == 2), sum(hours == 16), sum(hours == 21))\n## [1] 12 86 96 98\nSuspiciously, there ARE records between 01:00 and 02:00 - and roughly 1/8 as many as for each other hour. The plot thickens when we remember that we have 8 car parks. Is one car park guilty?\nnames &lt;- df$Name[grep(\"^27/03/2016\", df$LastUpdate)]\nnames[hours == 1]\n##  [1] Podium CP Podium CP Podium CP Podium CP Podium CP Podium CP Podium CP\n##  [8] Podium CP Podium CP Podium CP Podium CP Podium CP\n## 9 Levels: Avon Street CP Charlotte Street CP ... test car park\nGuilty as charged‚Ä¶ I shall, for now, ignore any potential timeshift, and I will investigate this further tomorrow.\nRight, on to today‚Äôs visualization!\nI‚Äôm going to have a go at a somewhat similar plot to yesterday - but facetting by car park, rather than by day.\nlibrary(ggplot2)\n\np &lt;- ggplot(data = df5, aes(x = as.double(Time), y = Day1)) +\n    facet_wrap(~ Name) + labs(x = \"Time (hour)\", y = \"\") +\n    geom_point(aes(colour = Name, alpha = x), shape = 20, size = 4) +\n    guides(colour = guide_legend(override.aes = list(size = 3))) +\n    scale_alpha(range = c(0, 1)) +\n    scale_y_discrete(limits = rev(levels(df5$Day1))) +\n    ggtitle(\"Occupancy per weekday, by car park\") +\n    theme(plot.title = element_text(size = 22, face = \"bold\"),\n          legend.position = \"None\")\nplibrary(ggplot2)\n\np &lt;- ggplot(data = df5, aes(x = as.double(Time), y = Day1)) +\n    facet_wrap(~ Name) + labs(x = \"Time (hour)\", y = \"\") +\n    geom_point(aes(colour = Name, alpha = x), shape = 20, size = 4) +\n    guides(colour = guide_legend(override.aes = list(size = 3))) +\n    scale_alpha(range = c(0, 1)) +\n    scale_y_discrete(limits = rev(levels(df5$Day1))) +\n    ggtitle(\"Occupancy per weekday, by car park\") +\n    theme(plot.title = element_text(size = 22, face = \"bold\"),\n          legend.position = \"None\")\np\n\nAnd regarding yesterday‚Äôs plot‚Ä¶ good news! I am indebted to Paul Rougieux for pointing out that I was using y = df5$x in my ggplot code yesterday, where I ought to have been using y = x - changing this solves the problem! So here is yesterday‚Äôs visualization again, but ordered more sensibly."
  },
  {
    "objectID": "posts/2016-11-12-30-days-30-visualizations-1-dataset-part-1/index.html#project-overview",
    "href": "posts/2016-11-12-30-days-30-visualizations-1-dataset-part-1/index.html#project-overview",
    "title": "30 Days, 30 Visualizations, 1 Dataset",
    "section": "",
    "text": "I am currently involved with a very exciting machine learning project being run within the Bath Machine Learning Meetup group (I‚Äôm sure I‚Äôll write a future post with more details on this project, but it is currently at a very early stage!). In collaboration with Bath: Hacked we are using this set of open data, which contains information about the occupancy of car parks in the Bath & North East Somerset (B&NES) area from the last two years or so.\nWhile looking for something to listen to whilst making my daily bus commute to and from the university, I recently discovered the marvellous Partially Derivative podcast. In one of the early episodes (S1E2, to be precise) one topic of conversation particularly caught my attention: Paul Downey‚Äôs blog post ‚ÄúOne CSV, thirty stories‚Äù. The concept is simple: take a single dataset, and produce a different visualization of the data each day for 30 days.\nSeeing as we will be working with the car parking dataset a great deal in coming months, and inspired by Downey‚Äôs post, I thought I would try something similar. So, without further ado‚Ä¶\n\n\n\nI thought it would be a good idea to have a quick look at the dataset before starting out. I will, of course, be using R (to start with at least!).\nI downloaded the dataset as a CSV file from the Bath: Hacked datastore (the link is above) - the file on the website is updated continuously with live data from sensors in the car parks. The version I downloaded (at 22:11 on 11/11/16) weighs in at a hefty 316MB, and contains a very large number of records:\nlength(count.fields(\"data/BANES_Historic_Car_Park_Occupancy.csv\"))\n## [1] 1528228\nTo avoid converting my Pentium-core laptop into a puddle of molten plastic, for now I‚Äôll only try to deal with about 10% of these records. Looking at the dataset‚Äôs documentation we see that ‚Äúscripts are set up to query the B&NES car park database every 5 minutes, the data is then pushed to the Bath: Hacked data store to‚Ä¶ append to a historical set‚Äù. Therefore it shouldn‚Äôt be a problem to read in only the first 150000 rows of the CSV file, since these rows should contain an equal spread of data from each of the distinct car parks.\nLet‚Äôs have a quick first look at our data! Note: a few records are empty in certain fields (future investigation may be necessary‚Ä¶) - I‚Äôll fill these fields with NAs for now to make it more obvious when data is missing.\nD &lt;- read.csv(\"data/BANES_Historic_Car_Park_Occupancy.csv\", nrow = 150000,\n              na.strings = c(\"NA\", \"\"))\nstr(D)\n## 'data.frame':    150000 obs. of  12 variables:\n##  $ ID          : Factor w/ 150000 levels \"000016ca6149d3f3bed350cf3b37f854\",..: 51364 64067 64678 65738 69335 113228 134317 134563 4865 17729 ...\n##  $ LastUpdate  : Factor w/ 88741 levels \"01/01/2015 01:00:01 AM\",..: 28126 28128 28131 28126 28124 28130 28128 28134 28346 28354 ...\n##  $ Name        : Factor w/ 8 levels \"Avon Street CP\",..: 7 1 3 8 6 4 2 5 8 5 ...\n##  $ Description : Factor w/ 8 levels \"Avon Street CP / A367 Green Park Road / Corn Street / CP / Bath\",..: 7 1 3 8 6 4 2 5 8 5 ...\n##  $ Capacity    : int  720 630 827 140 521 698 1056 1252 140 1252 ...\n##  $ Status      : Factor w/ 3 levels \"Emptying\",\"Filling\",..: 3 2 3 3 3 3 2 3 3 3 ...\n##  $ Occupancy   : int  28 32 8 15 75 21 124 0 15 0 ...\n##  $ Percentage  : int  3 5 1 10 14 3 12 0 10 0 ...\n##  $ Easting     : int  375115 374884 373183 375083 375109 371853 374445 373363 375083 373363 ...\n##  $ Northing    : int  164421 164469 168104 164424 165083 165766 165097 161610 164424 161610 ...\n##  $ DateUploaded: Factor w/ 23983 levels \"01/01/2015 01:00:01 AM\",..: 7636 7636 7636 7636 7636 7636 7636 7636 7675 7675 ...\n##  $ Location    : Factor w/ 8 levels \"(51.352935229, -2.38389427175)\",..: 2 4 8 3 5 7 6 1 3 1 ...\nhead(D)\n##                                 ID             LastUpdate\n## 1 577ffcf1cf17544c6030eef4ba85663e 13/02/2016 02:08:46 AM\n## 2 6d383b77bce3b7c87153ccf141158140 13/02/2016 02:09:36 AM\n## 3 6e415c9bf6c8231a189d11896ed42b04 13/02/2016 02:09:48 AM\n## 4 70151449ba6bcd7d28ee000b3042c7f9 13/02/2016 02:08:46 AM\n## 5 762bc1f6aafc736dabc1e4a6d006598e 13/02/2016 02:06:14 AM\n## 6 c0ffd83d29df5bb8bb5ee83c8122de28 13/02/2016 02:09:47 AM\n##                   Name\n## 1 SouthGate General CP\n## 2       Avon Street CP\n## 3         Lansdown P+R\n## 4    SouthGate Rail CP\n## 5            Podium CP\n## 6        Newbridge P+R\n##                                                                   Description\n## 1 SouthGate General CP / A367 St James Parade / Dorchester Street / CP / Bath\n## 2             Avon Street CP / A367 Green Park Road / Corn Street / CP / Bath\n## 3                               Lansdown P+R / Lansdown Road / P+R / Lansdown\n## 4    SouthGate Rail CP / A367 St James Parade / Dorchester Street / CP / Bath\n## 5                                       Podium CP / Walcot Street / CP / Bath\n## 6                              Newbridge P+R / A4 Newbridge Road / P+R / Bath\n##   Capacity  Status Occupancy Percentage Easting Northing\n## 1      720  Static        28          3  375115   164421\n## 2      630 Filling        32          5  374884   164469\n## 3      827  Static         8          1  373183   168104\n## 4      140  Static        15         10  375083   164424\n## 5      521  Static        75         14  375109   165083\n## 6      698  Static        21          3  371853   165766\n##             DateUploaded                        Location\n## 1 13/02/2016 02:10:01 AM (51.3782901111, -2.35893376096)\n## 2 13/02/2016 02:10:01 AM (51.3787114813, -2.36225609279)\n## 3 13/02/2016 02:10:01 AM (51.4113171399, -2.38697144144)\n## 4 13/02/2016 02:10:01 AM (51.3783156754, -2.35939373665)\n## 5 13/02/2016 02:10:01 AM (51.3842422001, -2.35906657317)\n## 6 13/02/2016 02:10:01 AM (51.3902305335, -2.40590782211)\nLooking interesting‚Ä¶ but I won‚Äôt look too closely for now. The fun starts tomorrow!\n\n\n\n\nA logical first step would be to work out where all the information we have is coming from!\nA week or two ago I attended a learning night run by Bath: Hacked, which was an introduction to using the mapping tool Carto (formerly CartoDB) for visualizing geographic data. And notice that we have some location information‚Ä¶\n# D is our 150000-row dataframe of records\n\n# Reduce D to a new dataframe, which consists only of the first row to contain\n# each car park name\ncarpark_df &lt;- D[!duplicated(D$Name), ]\n\n# The Location column contains character vectors of latitude and longitude in\n# the form \"(lat, lng)\": let's separate these values and make them numeric\nlocations &lt;- strsplit(sapply(carpark_df$Location, gsub,\n                             pattern = \"\\\\(|\\\\)\", replacement = \"\"),\n                      \", \")\n\nlat &lt;- as.numeric(sapply(locations, \"[[\", 1))\nlng &lt;- as.numeric(sapply(locations, \"[[\", 2))\n\n# Now construct a dataframe for Carto with the info we're interested in\ncarto_df &lt;- cbind(carpark_df[, c(\"Name\", \"Description\", \"Capacity\")],\n                  \"Longitude\" = lng, \"Latitude\" = lat)\n\n# Let's have a look:\ncarto_df\n##                   Name\n## 1 SouthGate General CP\n## 2       Avon Street CP\n## 3         Lansdown P+R\n## 4    SouthGate Rail CP\n## 5            Podium CP\n## 6        Newbridge P+R\n## 7  Charlotte Street CP\n## 8         Odd Down P+R\n##                                                                    Description\n## 1  SouthGate General CP / A367 St James Parade / Dorchester Street / CP / Bath\n## 2              Avon Street CP / A367 Green Park Road / Corn Street / CP / Bath\n## 3                                Lansdown P+R / Lansdown Road / P+R / Lansdown\n## 4     SouthGate Rail CP / A367 St James Parade / Dorchester Street / CP / Bath\n## 5                                        Podium CP / Walcot Street / CP / Bath\n## 6                               Newbridge P+R / A4 Newbridge Road / P+R / Bath\n## 7 Charlotte Street CP / A4 Charlotte Street / Marlbourgh Buildings / CP / Bath\n## 8                                             Odd Down P+R / A367 / P+R / Bath\n##   Capacity Longitude Latitude\n## 1      720 -2.358934 51.37829\n## 2      630 -2.362256 51.37871\n## 3      827 -2.386971 51.41132\n## 4      140 -2.359394 51.37832\n## 5      521 -2.359067 51.38424\n## 6      698 -2.405908 51.39023\n## 7     1056 -2.368609 51.38434\n## 8     1252 -2.383894 51.35294\n# Write this to a CSV ready for upload\nwrite.csv(carto_df, file = \"carto_df.csv\")\nSo let‚Äôs see where these car parks are!\n\n\n\n\n\n\nMaking use of the small dataframe I created yesterday (which has one entry per car park), and for now sticking to R‚Äôs base graphics:\ndf &lt;- read.csv(\"carto_df.csv\")\n\n# Re-order by Capacity\ndf &lt;- df[order(df$Capacity), ] \n                        \nbp &lt;- barplot(df$Capacity, main = \"Car park capacities\",\n              xlab = \"Number of spaces\", xlim = c(0, 1400), horiz = TRUE,\n              col = cm.colors(8), mgp = c(2.5, 1, 0))\n\ntext(bp, labels = df$Name, pos = 4)\n\n\n\n\n\nIt crossed my mind earlier today that I had made a couple of assumptions about the data I‚Äôm working with - primarily, I assumed that for a given car park, each record would have the same values for certain columns. Before doing anything else, I thought I should check that this was actually the case.\nThe columns in question are: Location, Easting, Northing (which are all related, of course) and Capacities.\n# Read in all the data - caution to the wind, if my laptop melts then it melts\ndf &lt;- read.csv(\"data/BANES_Historic_Car_Park_Occupancy.csv\")\n\n# Create a list of the car parks\nnames &lt;- unique(df$Name)\nnames\n## [1] SouthGate General CP Avon Street CP       Lansdown P+R        \n## [4] SouthGate Rail CP    Podium CP            Newbridge P+R       \n## [7] Charlotte Street CP  Odd Down P+R         test car park       \n## 9 Levels: Avon Street CP Charlotte Street CP ... test car park\n# Notice the appearance of the mysterious \"test car park\"!\n# (Missed that yesterday too!)\n# Check for unique values of each dubious column, per car park\nlocations &lt;- lapply(names, function(name) unique(df$Location[df$Name == name]))\n\neastings &lt;- lapply(names, function(name) unique(df$Easting[df$Name == name]))\n\nnorthings &lt;- lapply(names, function(name) unique(df$Northing[df$Name == name]))\n\ncapacities &lt;- lapply(names, function(name) unique(df$Capacity[df$Name == name]))\n\nlocations\n## [[1]]\n## [1] (51.3782901111, -2.35893376096)\n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[2]]\n## [1] (51.3787114813, -2.36225609279)\n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[3]]\n## [1] (51.4113171399, -2.38697144144)\n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[4]]\n## [1] (51.3783156754, -2.35939373665)\n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[5]]\n## [1] (51.3842422001, -2.35906657317)                                \n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[6]]\n## [1] (51.3902305335, -2.40590782211)                                \n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[7]]\n## [1] (51.3843384358, -2.36860892385)\n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[8]]\n## [1] (51.352935229, -2.38389427175)\n## 9 Levels:  ... (51.4113171399, -2.38697144144)\n## \n## [[9]]\n## [1] \n## 9 Levels:  ... (51.4113171399, -2.38697144144)\neastings\n## [[1]]\n## [1] 375115\n## \n## [[2]]\n## [1] 374884\n## \n## [[3]]\n## [1] 373183\n## \n## [[4]]\n## [1] 375083\n## \n## [[5]]\n## [1] 375109\n## \n## [[6]]\n## [1] 371853\n## \n## [[7]]\n## [1] 374445\n## \n## [[8]]\n## [1] 373363\n## \n## [[9]]\n## [1] NA```\n\n\n```r\nnorthings\n## [[1]]\n## [1] 164421\n## \n## [[2]]\n## [1] 164469\n## \n## [[3]]\n## [1] 168104\n## \n## [[4]]\n## [1] 164424\n## \n## [[5]]\n## [1] 165083\n## \n## [[6]]\n## [1] 165766\n## \n## [[7]]\n## [1] 165097\n## \n## [[8]]\n## [1] 161610\n## \n## [[9]]\n## [1] NA```\n\n\n\n\n```r\ncapacities\n## [[1]]\n##  [1] 720 680 900 700 626 800 860 638 603 602 630 596 623 519 528 563 840\n## \n## [[2]]\n## [1] 630 590 230\n## \n## [[3]]\n## [1]  827 1827  860\n## \n## [[4]]\n## [1] 140 200 180 132\n## \n## [[5]]\n## [1] 521\n## \n## [[6]]\n## [1] 698 470 450 600\n## \n## [[7]]\n## [1] 1056  526\n## \n## [[8]]\n## [1] 1252 1320\n## \n## [[9]]\n## [1] 500\nThe first three columns only have one value per car park - great, our car parks aren‚Äôt moving!\nHowever‚Ä¶ it does seem that some capacities are not constant. Let‚Äôs try to see how they change over time.\n# Create new dataframe with columns of interest only, and taking out \"test car\n# park\" data since this isn't of any interest to us\ndf2 &lt;- df[!(df$Name == \"test car park\"), c(\"Name\", \"LastUpdate\", \"Capacity\")]\n\n# Reformat \"LastUpdate\" column as POSIXct datetime, rather than character\ndf2$LastUpdate &lt;- as.POSIXct(df2$LastUpdate, format = \"%d/%m/%Y %I:%M:%S %p\")\n\n# We'll use ggplot2 for our plotting today\nlibrary(ggplot2)\n\np &lt;- ggplot(data = df2, aes(x = LastUpdate, y = Capacity, group = Name)) +\n    geom_line(aes(colour = Name), size = 1.5) +\n    scale_x_datetime(date_labels = \"%b %Y\") + xlab(\"\") + ylab(\"Capacity\") +\n    guides(colour = guide_legend(override.aes = list(size = 3))) +\n    ggtitle(\"Capacities over time\") +\n    theme(plot.title = element_text(size = 22, face = \"bold\"))\n\n# Plot it!\np\n\nThe plot reveals some interesting information about the ‚Äúchanges‚Äù in capacities. For Newbridge P+R, the capacity increases over time, and remains level after each increase. It seems likely, then, that this is a genuine increase in capacity over time due to extensions to the car park.\nFor the other car parks, any changes are suspiciously short-lived and the capacities return to their previous values soon after any change. For this reason I think it is likely that these are mostly misrecorded values - although of course it is possible that there were short-term, temporary extensions/closures of the car parks.\nThe records for Avon Street CP also end abruptly and early in mid-summer of this year.\nBy pure chance, my visualization from yesterday (of maximum capacities) was using records from February 2016 - a time of relative stability, and when Newbridge P+R had reached its current maximum capacity - and so it is still, I would say, a valid representation of the data. But having had a much better look at the data today it is obvious I got a bit lucky!\n\n\n\n\nIt‚Äôs a long title today. This is reflective of the long time I have spent trying to re-order the panels in today‚Äôs visualization, without success‚Ä¶ more details shortly.\nFirst, let‚Äôs get the data we‚Äôre interested in today.\n# Separate interesting columns from dataframe of all records\ndf3 &lt;- df[!(df$Name == \"test car park\"), c(\"Name\", \"Percentage\", \"LastUpdate\")]\n\n# Convert LastUpdate to POSIXct and store in a new vector\nupdates &lt;- as.POSIXct(df3$LastUpdate, format = \"%d/%m/%Y %I:%M:%S %p\",\n                      tz = \"UTC\")\n\n# Change every datatime to \"time to nearest 10 minutes\" (600 seconds)\ntimes &lt;- as.POSIXct(round(as.double(updates) / 600) * 600,\n                    origin = as.POSIXlt(\"1970-01-01\"), tz = \"UTC\")\n\ndecimal_times &lt;- as.POSIXlt(times)$hour + as.POSIXlt(times)$min/60\n\n# Change every datetime to weekday abbreviation\ndays &lt;- format(updates, \"%a\")\n\n# Add these new columns to our dataframe\ndf4 &lt;- cbind(df3, \"Day\" = days, \"Time\" = decimal_times)\n\n# Take average of Percentage over each timeblock, per day, per car park\ndf5 &lt;- aggregate(df4$Percentage,\n                 list(\"Time\" = df4$Time,\n                      \"Day\" = df4$Day,\n                      \"Name\" = df4$Name),\n                 mean)\nLet‚Äôs have a quick look at what we‚Äôve achieved:\nhead(df5)\n##        Time Day           Name        x\n## 1 0.0000000 Fri Avon Street CP 7.366279\n## 2 0.1666667 Fri Avon Street CP 6.670732\n## 3 0.3333333 Fri Avon Street CP 6.456140\n## 4 0.5000000 Fri Avon Street CP 6.465839\n## 5 0.6666667 Fri Avon Street CP 5.774566\n## 6 0.8333333 Fri Avon Street CP 6.361963\nnrow(df5)\n## [1] 8064\nReassuringly, df5 contains 8064 entries:\n6{10-min intervals per hour}*24{hours}*7{days}*8{car parks} = 8064\nNow let‚Äôs create the plot!\nlibrary(ggplot2)\n\np &lt;- ggplot(data = df5, aes(x = as.double(Time), y = df5$x, group = Name)) +\n    facet_wrap(~ Day) + labs(y = \"Percentage occupancy\", x = \"Time (hour)\") +\n    geom_line(aes(colour = Name)) +\n    guides(colour = guide_legend(override.aes = list(size = 3)))\np\n\nLooking good - except for two things:\n\nx-axis labels are not nicely formatted\nPanels are not sensibly ordered\n\nI have tried at some length to solve each of these issues - particularly the second, which I thought I could solve by reordering the underlying factor:\ndf5$Day1 &lt;- factor(df5$Day,  levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\",\n                                        \"Sat\", \"Sun\"))\nBut plotting again faceting by Day1 instead of Day only reorders the labels, and does not reorder the plots themselves - i.e.¬†Wednesday‚Äôs data is now labelled as Sunday. This is obviously worse than what I currently have (it‚Äôs plain wrong, rather than just ‚Äúnot pretty‚Äù) so I haven‚Äôt included it here.\nI am going to try to stick to the plan of one visualization per day - but along with my already busy schedule and regular maths workload, I am obviously putting myself under a great deal of time pressure! At best I have a couple of hours to dedicate to this project on any given day, and this isn‚Äôt very long considering I am learning pretty much from scratch as I go along!\nI‚Äôm hoping, then, that this won‚Äôt become a common refrain, but‚Ä¶ I‚Äôve run out of time today! I‚Äôll revisit this visualization at some point soon if I figure out what I was doing wrong.\n\n\n\n\nBefore doing anything else, I‚Äôm going to remove the redundant columns from the dataframe of all records, and save the new smaller dataframe to a CSV file - which should then be quicker to load and easier work with.\ndf &lt;- df[, c(\"Name\", \"LastUpdate\", \"Capacity\", \"Occupancy\", \"Percentage\", \"Status\")]\n\nwrite.csv(df, file = \"df.csv\")\ndf &lt;- read.csv(\"C:/Users/Owen/Documents/Coding/Parking/df.csv\")\nFollowing another moment of mild panic this morning, I‚Äôm also going to check that the times being recorded are adjusted for GMT/BST each spring and autumn. If they are then I don‚Äôt need to worry - if not, I‚Äôll have to do some time-correction work‚Ä¶\n# I'll have a look at records from spring of 2016 - clocks changed on 27/03/2016\ntimes &lt;- df$LastUpdate[grep(\"^27/03/2016\", df$LastUpdate)]\nhours &lt;- as.POSIXlt(times, format = \"%d/%m/%Y %I:%M:%S %p\", tz = \"UTC\")$hour\n\n# Check number of records for a few hours, including 1AM (shouldn't be any!)\nc(sum(hours == 1), sum(hours == 2), sum(hours == 16), sum(hours == 21))\n## [1] 12 86 96 98\nSuspiciously, there ARE records between 01:00 and 02:00 - and roughly 1/8 as many as for each other hour. The plot thickens when we remember that we have 8 car parks. Is one car park guilty?\nnames &lt;- df$Name[grep(\"^27/03/2016\", df$LastUpdate)]\nnames[hours == 1]\n##  [1] Podium CP Podium CP Podium CP Podium CP Podium CP Podium CP Podium CP\n##  [8] Podium CP Podium CP Podium CP Podium CP Podium CP\n## 9 Levels: Avon Street CP Charlotte Street CP ... test car park\nGuilty as charged‚Ä¶ I shall, for now, ignore any potential timeshift, and I will investigate this further tomorrow.\nRight, on to today‚Äôs visualization!\nI‚Äôm going to have a go at a somewhat similar plot to yesterday - but facetting by car park, rather than by day.\nlibrary(ggplot2)\n\np &lt;- ggplot(data = df5, aes(x = as.double(Time), y = Day1)) +\n    facet_wrap(~ Name) + labs(x = \"Time (hour)\", y = \"\") +\n    geom_point(aes(colour = Name, alpha = x), shape = 20, size = 4) +\n    guides(colour = guide_legend(override.aes = list(size = 3))) +\n    scale_alpha(range = c(0, 1)) +\n    scale_y_discrete(limits = rev(levels(df5$Day1))) +\n    ggtitle(\"Occupancy per weekday, by car park\") +\n    theme(plot.title = element_text(size = 22, face = \"bold\"),\n          legend.position = \"None\")\nplibrary(ggplot2)\n\np &lt;- ggplot(data = df5, aes(x = as.double(Time), y = Day1)) +\n    facet_wrap(~ Name) + labs(x = \"Time (hour)\", y = \"\") +\n    geom_point(aes(colour = Name, alpha = x), shape = 20, size = 4) +\n    guides(colour = guide_legend(override.aes = list(size = 3))) +\n    scale_alpha(range = c(0, 1)) +\n    scale_y_discrete(limits = rev(levels(df5$Day1))) +\n    ggtitle(\"Occupancy per weekday, by car park\") +\n    theme(plot.title = element_text(size = 22, face = \"bold\"),\n          legend.position = \"None\")\np\n\nAnd regarding yesterday‚Äôs plot‚Ä¶ good news! I am indebted to Paul Rougieux for pointing out that I was using y = df5$x in my ggplot code yesterday, where I ought to have been using y = x - changing this solves the problem! So here is yesterday‚Äôs visualization again, but ordered more sensibly."
  },
  {
    "objectID": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html",
    "href": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html",
    "title": "Modular R code for analytical projects with {box}",
    "section": "",
    "text": "I work with a team of data scientists and statistical modellers, and we do pretty much everything in R.\nWe work on all sorts of projects; I wanted to share a really useful pattern that we‚Äôve developed to help us write better code across all of those projects, and a little bit of the reasoning behind it!"
  },
  {
    "objectID": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html#introduction",
    "href": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html#introduction",
    "title": "Modular R code for analytical projects with {box}",
    "section": "",
    "text": "I work with a team of data scientists and statistical modellers, and we do pretty much everything in R.\nWe work on all sorts of projects; I wanted to share a really useful pattern that we‚Äôve developed to help us write better code across all of those projects, and a little bit of the reasoning behind it!"
  },
  {
    "objectID": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html#modular-r-code",
    "href": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html#modular-r-code",
    "title": "Modular R code for analytical projects with {box}",
    "section": "Modular R code",
    "text": "Modular R code\nPlenty of code we write ends up being useful in multiple places. So ideally, we want to make sure our code is:\n\neasy to reproduce - in particular, it should be clear when code depends on other code\neasy to maintain - if we need to change how something works, we want to make changes in as few places as possible\neasy to find - if we know a certain piece of functionality exists already, we shouldn‚Äôt have to search too hard to find it\n\nTogether, these concepts form the basis of modular code: that is, keeping related functionality grouped together, and having a sensible way of declaring dependencies in your code.\nOne solution to this problem would be to turn any ‚Äúmodularisable‚Äù code within a project into an R package. And that is absolutely a good approach, and absolutely doable, but it does have a couple of drawbacks:\n\nIt requires a bit of additional knowledge, namely how to construct R packages - which actually isn‚Äôt too tricky, this is barely a drawback\nThe package needs to be reinstalled after any updates - again, not arduous, but slight additional friction\nWe would need to choose a package name - this is hard\n\nI‚Äôm not joking. Choosing a package name can be really difficult - which actually is often a symptom of the potential package not having a well-defined scope. I think this is especially true of analytical projects, where there are often multiple ‚Äúscopes‚Äù involved (e.g.¬†importing data, processing, producing outputs‚Ä¶) - and, from bitter experience, where these scopes can change or expand quite dramatically as the project progresses.\nSo ideally we want all the benefits of modular code, without the restrictions implied by having to fit our code into a formal package.\nLuckily, there‚Äôs a package for that‚Ä¶"
  },
  {
    "objectID": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html#what-is-box",
    "href": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html#what-is-box",
    "title": "Modular R code for analytical projects with {box}",
    "section": "What is {box}?",
    "text": "What is {box}?\nIt‚Äôs an R package providing a framework which makes it easier to write modular code. The box::use() function is used to declare when one piece of code is dependent on another - kind of like source(), but without dumping everything into the global environment.\nIf you haven‚Äôt come across {box} before, I‚Äôd really strongly encourage you to pause here and go take a quick look at the package website to see how it works. It‚Äôs awesome.\nOn its own, it is already immensely useful within the context of a single project. But let me share a couple of additional ideas that revolutionised how we write modular code across multiple projects."
  },
  {
    "objectID": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html#defining-module-types",
    "href": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html#defining-module-types",
    "title": "Modular R code for analytical projects with {box}",
    "section": "Defining module types",
    "text": "Defining module types\nAs we began using {box} more and more to structure our code, we realised that we tended to end up three different ‚Äútypes‚Äù of module, distinguished by where they were being used‚Ä¶\n\nGeneral modules - [box/*]\nThese contain functionality which is useful across multiple projects. For example, we often need to work with our AWS Redshift data warehouse - so we have a general module called [box/redshift], which contains utility functions that make it easier to connect to & work with the data warehouse. We develop these modules in their own repo, and include them within other projects via a git submodule - I promise that‚Äôs not as scary as it sounds, bear with me!\n\n\nProject modules - [prj/*]\nThese are used within a single project, and are unique to that project‚Äôs context. For example, a [prj/data] module might contain functions to fetch project-specific data from various places, and then a [prj/plots] module might contain functions to create certain project-specific graphs using that data. If anything ends up being more broadly useful, it might be ‚Äúpromoted‚Äù into a general module.\n\n\nLocal modules - [./*]\nThese are specific to the context of a particular set of files. They can be useful for keeping interdependent code neatly organised within a directory: code can be split into multiple files, with a ‚Äúdeclaration of dependence‚Äù specified at the top of each file (via box::use()) which ensures it‚Äôs easy to keep track of what depends on what. These can be handy while actively developing new code, but it‚Äôs often worth upgrading local modules to project modules if possible."
  },
  {
    "objectID": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html#automatically-setting-the-box-search-path-for-a-project",
    "href": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html#automatically-setting-the-box-search-path-for-a-project",
    "title": "Modular R code for analytical projects with {box}",
    "section": "Automatically setting the {box} search path for a project",
    "text": "Automatically setting the {box} search path for a project\n{box} finds modules by following a ‚Äúsearch path‚Äù, which we can set in one of the following ways:\n\noptions(\"box.path\") - a vector of paths where modules can be found; like .libPaths() for R packages\nR_BOX_PATH environment variable - a single string; like $PATH for system commands\n\n(There‚Äôs more info about this in the package docs.)\nLet‚Äôs assume that we‚Äôre working on a project which lives in its own folder. In our case, each of our team‚Äôs projects lives in its own git repo, and we work with those via RStudio projects.\nThen we can use the project‚Äôs .Rprofile file to set the {box} search path when we start an R session in that directory (e.g.¬†by opening the associated RStudio project)."
  },
  {
    "objectID": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html#bringing-it-all-together",
    "href": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html#bringing-it-all-together",
    "title": "Modular R code for analytical projects with {box}",
    "section": "Bringing it all together",
    "text": "Bringing it all together\nSo with all of that in mind, here‚Äôs the general approach we follow for setting up a new project which will use {box} modules:\n\nIn the project repo where you would like to use modules, create a subdirectory to store them, e.g.¬†src/R/:\n $ # From the project's root directory\n $ mkdir -p src/R/\nAdd your ‚Äúgeneral modules‚Äù repo as a git submodule, in a box/ directory within that new subdirectory:\n $ git submodule add [general modules repo's URL] src/R/box\nCreate a prj/ directory alongside the box/ directory, to store project modules:\n $ mkdir src/R/prj/\nUpdate the project‚Äôs .Rprofile file to set the box.path R option, and (optionally) to ensure that the general-modules submodule is updated to the appropriate point in its history when your project is opened:\nlocal({\n  # Local box path\n  box_path &lt;- file.path(getwd(), \"src\", \"R\")\n\n  # Update existing box path (e.g. a path set by Rprofile.site)\n  options(\"box.path\" = c(box_path, getOption(\"box.path\")))\n\n  # Make sure box submodule is pulled in at correct ref\n  system2(\"git\", c(\"submodule\", \"update\", \"--init\", \"--recursive\", \"--\", file.path(box_path, \"box\")))\n})\nIf at any point you want to update the submodule to include the latest changes from the general repo, you‚Äôll need to run the following from within your project repo (amend the submodule path if necessary):\n$ git submodule update --remote --recursive -- src/R/box\nThis will update the reference which Git uses to fetch the submodule contents. You‚Äôll need to commit the updated reference to your project repo (i.e.¬†add and commit your src/R/box ‚Äúdirectory‚Äù, which is actually just a small file containing that reference!)."
  },
  {
    "objectID": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html#conclusion",
    "href": "posts/2024-04-16-modular-r-code-for-analytical-projects-with-box/index.html#conclusion",
    "title": "Modular R code for analytical projects with {box}",
    "section": "Conclusion",
    "text": "Conclusion\nPlease do borrow, adapt, and improve on these ideas - if you try using this same pattern, or if you come up with any additions/alternatives to it, I‚Äôd really love to hear about it!\nAnd if you enjoyed this, I promise I‚Äôve got plenty more {box} content that I could find time to write about‚Ä¶"
  },
  {
    "objectID": "posts/2024-04-12-new-blog-who-dis/index.html",
    "href": "posts/2024-04-12-new-blog-who-dis/index.html",
    "title": "New blog, who dis?",
    "section": "",
    "text": "Hey folks, it‚Äôs been a while!\nI did the thing from the meme and, having logged on intending to write a short blog post, ended up moving my entire blog to a new platform.\nI have zero regrets about this - the previous handcrafted site was fun to build in the first place (many years ago now‚Ä¶); but I couldn‚Äôt face re-learning how to build & deploy with Jekyll againnnn on yet another laptop. So instead, here‚Äôs a shiny new Quarto-built site, tada!!\nParticular thanks go to Rob J Hyndman and Ma√´lle Salmon for their super handy posts about migrating comments from Disqus.\nThere are probably some rough edges that I‚Äôve missed - dead links, dodgy formatting‚Ä¶ please point me towards anything particularly ugly and I‚Äôll get it patched up.\nAnd I promise that more actual gloriously content-filled blog posts will actually happen now. Stay tuned!"
  },
  {
    "objectID": "posts/2018-09-29-afk-back-in-2-months/index.html",
    "href": "posts/2018-09-29-afk-back-in-2-months/index.html",
    "title": "AFK, back in 2 months",
    "section": "",
    "text": "A year before I sat down here and started writing this sentence, I was about three months into a year-long work placement at Mango. I loved what I was doing, I loved the people I was doing it with, and I was generally having a great time.\nBut at some point last spring, people started to ask me when I was leaving. I couldn‚Äôt tell whether it was because they knew that at some point I‚Äôd have to go back to university to complete my course, or because they‚Äôd had enough of me already and wanted me to go away. Either way, whenever I mentioned that I was thinking about taking advantage of what will probably be the last long summer holiday of my life, everyone told me the same thing: I‚Äôd be a fool not to.\nTherefore after months of meticulous planning, early one mid-July morning myself and a friend - summoning as much 18th-century spirit as possible - set off to complete a Grand Tour of the continent.\nOver the course of 55 days, we visited 22 countries, we covered over 6000 miles of European highway, and due to the fact that I was very busy being on holiday, I wrote precisely 0 lines of code.\nSo I‚Äôm not writing about some cool project I‚Äôve done, or some amazing new tech I‚Äôve been researching, or really anything at all to do with code or a computer. Sorry. I suppose this piece should really be called ‚Äúsome things I learned which definitely have completely nothing at all to do with my job‚Äù."
  },
  {
    "objectID": "posts/2018-09-29-afk-back-in-2-months/index.html#some-things-i-learned-which-definitely-have-completely-nothing-at-all-to-do-with-my-job",
    "href": "posts/2018-09-29-afk-back-in-2-months/index.html#some-things-i-learned-which-definitely-have-completely-nothing-at-all-to-do-with-my-job",
    "title": "AFK, back in 2 months",
    "section": "Some things I learned which definitely have completely nothing at all to do with my job",
    "text": "Some things I learned which definitely have completely nothing at all to do with my job\n\n1. Head for high ground\nThis is probably what you learn on day one in Army Commander School.\nYou‚Äôre in charge. The furious battle is raging on all sides. Suddenly, you realise that you are in serious danger of being completely overwhelmed. This is a good time to employ a tactic commonly referred to as ‚Äúrunning away‚Äù.\nBut if this is a battle you want (or need!) to win, it‚Äôs probably not a good idea to run away forever. Your opponent isn‚Äôt going to hang around for a while wondering where you‚Äôve gone, and then just decide ‚Äúactually, yeah, fair enough, we lost, never mind‚Äù.\nInstead, you should run away to somewhere nearby, but as high up as possible. This gives you a chance to widen your view: you can see where you‚Äôve been, where you want to go, what‚Äôs going on right now, and how those three things relate to each other. After assessing the situation from this elevated position, it is much easier to see what needs to be done and to refocus your efforts accordingly.\nIf you want to be a top-level Army Commander one day, you can learn more from this. On your next conquest (or, in my case, unfamiliar city) find that high ground and go on a quick reconnaissance mission as early as possible: identify your goals, think about the best way to get to them, and scan the horizon for any threats (or scary grey clouds) which might be on their way towards you.\n Once you have the high ground, it‚Äôs over\n\n\n2. Record what you‚Äôve done\nOn the whole, we humans are pretty smart. We‚Äôre good at figuring out how to do stuff, and once we‚Äôve figured out what to do, we‚Äôre good at actually doing it.\nHaving said that, the same is true of other primates. And crows. And dolphins. And beavers. And so on, and so on.\nThe reason why we are smarter is that we have an awesome thing called ‚Äúlanguage‚Äù. Language lets us share our ideas and our experiences with other humans, so that they don‚Äôt have to come up with the same ideas or go through the same experiences in order to have the same knowledge.\nEven better: at some point a few thousand years ago, someone figured out how to convert language into something physical. As a result, those of us who are alive right now have access to virtually all the knowledge developed by all of humankind since that point.\nSO WHY YOU NO USE IT? Write down everything! Write down what you‚Äôve done, and how you‚Äôve done it, and why you‚Äôve done it, and why you‚Äôve done it like that, and everything that went wrong before you got it right, and everything you think it could lead to.\nDo it for yourself, in anticipation of the moment when in six months‚Äô time you realise you‚Äôve forgotten where you were or who you were with or what the name of that street was.\nDo it for other people, so that they don‚Äôt have to drive round eastern Prague four times trying to find the car park which was marked in the wrong place on the map.\nDo it for the people who will stumble across your hastily scrawled notes years from now and, with a sudden flash of inspiration, will use them as the foundation to build myriad new and wonderful things.\n My memory is terrible, but I wrote down all the embarrassing stories so that they‚Äôll never be forgotten\n\n\n3. Respect experience\nAsking questions is a really really good thing to do. It‚Äôs one of the best ways to learn about things and you should never be afraid to ask about something you don‚Äôt understand.\nHowever, it‚Äôs important to remember one thing: ‚Äúalways ask‚Äù is not the same as ‚Äúalways ask right now‚Äù.\nIf someone with more experience than you tells you to do something, and if you know that there is almost certainly a good reason, then even if you don‚Äôt know what that reason is‚Ä¶ you should probably do the thing.\nWait until the pressure has eased a bit before demanding an explanation. You should still ask for one, but perhaps when everyone‚Äôs a little bit less stressed.\n\n\n4. Call a spade a spade\nNames can be controversial.\nPavement or sidewalk? Biscuit or cookie? Dinner or tea or supper? Bun or bap or roll? GIF or GIF?\nBut there are some names that virtually everyone agrees on. In particular, this tends to happen if it is important that everyone agrees on the name.\nFor example, ‚Äúpolice‚Äù is an important concept: it represents protection, order, assistance, and a bunch of other useful words. Pretty much all European languages have almost exactly the same spelling and pronunciation for ‚Äúpolice‚Äù as English does.\n How to say ‚Äúpolice‚Äù in the 18 different European languages which we came across during our trip\nThis means that if you speak any one of these languages, you can travel to any place where they speak any one of the others; and even if you‚Äôre in an unfamiliar environment where your understanding is limited, you aren‚Äôt completely on your own. If you need help, you can yell ‚ÄúPOLICE!‚Äù, and someone in a uniform will probably come running.\nUnless you‚Äôre in Hungary, because Hungarian is very strange.\n‚Ä¶ actually, someone will come running even in Hungary, because virtually everyone speaks English as a second language. They have to, because very few people choose to learn Hungarian as a second language - it‚Äôs only really spoken in Hungary, and as previously mentioned, it really is very strange. Nevertheless, it is the first language of around 13 million people, so there‚Äôs a reasonable chance that at some point you‚Äôll need to find a friendly Hungarian to do some translation for you.\nI suppose there are two points to take from this little section. Firstly, if you call things by more or less the same name as everyone else does, then this will usually help to improve shared understanding and will aid communication. Secondly, people who can speak multiple languages - and especially less widely-spoken languages - are super super valuable!\n\n\n5. Call a spade a spade, but that doesn‚Äôt mean you should assume/demand that everyone else is going to call every single item in their toolshed by exactly the same names as you call all the things which you have in YOUR toolshed\nJust to add an important caveat to the previous section: sure, it‚Äôs helpful if someone speaks the same language as you, and even more exciting if you realise they speak it with the same accent. But once you‚Äôve traded your initial stories, that gets boring quite quickly.\nPlus, you‚Äôre definitely going to struggle to make new friends if you go around loudly insisting that everyone speaks to you in your language, and getting angry or patronising people if they get something ‚Äúwrong‚Äù. Socialise, compromise, learn.\n\n\n6. New is often exciting, but exciting doesn‚Äôt have to be new\nHumans have been around for a while now, which means we‚Äôve already gone to most places. If you want to go somewhere no-one else has been before then your options are already fairly limited. If you add the complication of getting there in the first place, and the fairly high probability that you won‚Äôt find anything particularly interesting there anyway, then it begins to look like a bit of a daunting prospect.\nHowever.\nYou don‚Äôt have to go somewhere no-one else has been before. You can go to the same places and do the same things that someone else has already done, and as long as you‚Äôre enjoying yourself, it really doesn‚Äôt matter that someone has been there and done it before. There‚Äôs always a slightly different route to the next place, or a slightly different angle to view something from, or something to take inspiration from when you‚Äôre planning your next adventure.\nMaybe one day in the future, you‚Äôll decide that you want a bigger challenge. Then you can dust off your old maps and start thinking about making that expedition out into the middle of nowhere. But there are plenty of other wonderful places to go and things to do first - and honestly, some of those places really are well worth a visit.\n If you see an awesome thing that someone else has already done, don‚Äôt be afraid to recreate it yourself (or to take photos of your friend recreating it)\n\n\n7. Get out there and do stuff\nThere is so much out there.\nNo really, there is SO MUCH out there.\nGo to places. Meet people. Talk to those people, then find more people. Read stuff, write stuff, look at things, show your friends, share opinions, debate stuff, be creative, demand feedback, ask questions, learn things, challenge yourself, pass on your passion, and while you‚Äôre busy doing all that never let anyone take away the thing that makes you you.\nGo right now and carry on being awesome."
  },
  {
    "objectID": "posts/2017-01-13-banescarparking-an-r-package/index.html",
    "href": "posts/2017-01-13-banescarparking-an-r-package/index.html",
    "title": "BANEScarparking - an R package",
    "section": "",
    "text": "Happy new year everyone!\n\n\nJanuary exams are imminent, so I should be revising. Therefore I have written an R package‚Ä¶\n\n\nAdmittedly it is a little bit niche, but hopefully it will prove useful for the machine learning project I‚Äôm working on with BMLM (I have mentioned this project before - it was part of my motivation for TDTVOD).\n\n\nThe package isn‚Äôt currently on CRAN (one day maybe!) but v0.1.1 can be found on GitHub here:\n\nhttps://github.com/owenjonesuob/BANEScarparking\n\nand you can install it with\n\ndevtools::install_github(\"owenjonesuob/BANEScarparking\")\n\n\nUPDATE (23/01/2018)\n\n\nThere have been loads of awesome updates to BANEScarparking since it was first released!\n\n\nAnd BANEScarparkinglite, the dataless version, totally is on CRAN :D\n\n\nYou can find it here: https://cran.r-project.org/package=BANEScarparkinglite\n\n\nSo if you ever feel an urgent desire to analyse Bath & North East Somerset‚Äôs historical parking records, you know what to do."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I‚Äôm Owen üëã I‚Äôm a data scientist/engineer living in Cardiff, United Kingdom. This is a collection of projects I have spent time on while I was supposed to be doing other things."
  },
  {
    "objectID": "posts/2016-10-12-saving-loading-rstudio-settings-with-command-prompt/index.html",
    "href": "posts/2016-10-12-saving-loading-rstudio-settings-with-command-prompt/index.html",
    "title": "Saving/loading RStudio settings with Command Prompt",
    "section": "",
    "text": "This one doesn‚Äôt really deserve the title of ‚Äòproject‚Äô, but I thought I‚Äôd share it anyway.\n\n\nI use RStudio to work with R. I am a huge fan of this piece of software for many reasons, one of which is the ability to change various settings of the code editor, such as its appearance and code auto-completion settings.\n\n\nWhen using RStudio on the computers at the university, any changes made to these settings are wiped each time a user logs out of the machine. I quickly became tired of having to tick and untick multiple boxes each time I started a new RStudio session (call me lazy if you will; I prefer ‚Äúeconomical‚Äù), so I decided to see if I could work out how to stop this from happening.\n\n\nIt turns out RStudio saves its settings in a folder called RStudio-Desktop, which is stored in the Local AppData section of the hard drive - which, on the university computers, is cleared for each fresh log-in.\n\n\nHaving logged in and changed the RStudio settings to my liking, I copied this folder to my personal drive (stored on the server rather than locally, so not wiped between sessions). I then wrote a short set of Command Prompt instructions to copy this folder into the Local AppData folder:\n\n{% highlight text %} cd /d C: xcopy H:/e a\n\n&lt;p&gt;(The &lt;code&gt;a&lt;/code&gt; at the end is a response, &lt;code&gt;All&lt;/code&gt;, to Command Prompt's question as to whether to overwrite files of the same name, in case I opened RStudio before running the code thus creating a new folder of the same name.)&lt;/p&gt;\n&lt;p&gt;This string of commands can be run from a desktop shortcut. The path below opens a Command Prompt window (and the &lt;code&gt;/c&lt;/code&gt; closes it when finished) and runs the above commands in under half a second.&lt;/p&gt;\n\n{% highlight text %}\nC:\\Windows\\System32\\cmd.exe /c cd /d C:\\Users\\olj23\\AppData\\Local &amp;\n    xcopy H:\\dos\\R\\Settings /e &amp; a\n\nNow whenever I want to open RStudio I can just click the neighbouring shortcut first et voila, my settings are loaded!\n\n\n&lt;img src=‚Äú&gt;\n\nMaybe I should have studied graphic design"
  },
  {
    "objectID": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html",
    "href": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html",
    "title": "H2O.ai: Going for a paddle",
    "section": "",
    "text": "Note: I originally wrote this post for the Mango Solutions blog, and they have kindly allowed me to repost it here alongside the rest of my work! You can find the original post here."
  },
  {
    "objectID": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html#a-quick-disclaimer",
    "href": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html#a-quick-disclaimer",
    "title": "H2O.ai: Going for a paddle",
    "section": "\nA quick disclaimer\n",
    "text": "A quick disclaimer\n\n\nThis post isn‚Äôt called H2O.ai: Going for the 100m freestyle world record. I‚Äôm not trying to win a Kaggle competition. I‚Äôm not carrying out detailed, highly-controlled benchmarking tests. I‚Äôm not, in fact, claiming to be doing anything particularly useful at all. This is just me, just playing around with some code, just for the fun of it."
  },
  {
    "objectID": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html#h-to-o-dot-what",
    "href": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html#h-to-o-dot-what",
    "title": "H2O.ai: Going for a paddle",
    "section": "\nH-to-O dot what?\n",
    "text": "H-to-O dot what?\n\n\n‚ÄúI‚Äôve never even heard of that,‚Äù you might be thinking. Well, don‚Äôt worry - neither had I until very recently. Let‚Äôs start by having a look at their website.\n\n\n\nH2O is the world‚Äôs leading open source deep learning platform. H2O is used by over 100,000 data scientists and more than 10,000 organizations around the world. - www.h2o.ai\n\n\n\nIn other words, H2O - which has been around since way back in 2011 - is a widely-used platform for carrying out machine learning and data analysis, which is optimised for working wth large datasets.\n\n\nThe primary focus really is on the machine learning. There is a wide selection of algorithms to choose from, ranging from random forests and neural networks to PCA and Word2vec (a full list can be found in the documentation).\n\n\nH2O is a language in its own right, although under the hood it is largely written in Java. This makes it incredibly flexible: you can use it with R or Python, link into Spark and Hadoop, or use the built-in ‚ÄúFlow‚Äù interface. I‚Äôll cover some of this functionality in the rest of this post."
  },
  {
    "objectID": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html#demo-random-forest",
    "href": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html#demo-random-forest",
    "title": "H2O.ai: Going for a paddle",
    "section": "\nDemo: Random Forest\n",
    "text": "Demo: Random Forest\n\n\nLet‚Äôs test out a couple of H2O‚Äôs algorithms. To start with, we‚Äôll use a random forest for a classification task.\n\n\nI‚Äôm going to use the ‚ÄúCovertype‚Äù dataset from the UCI ML datasets archive. This dataset contains nearly 600000 records representing different locations in the USA. The challenge is to predict tree cover type (spruce/fir, pine, aspen etc.) using only cartographic variables (elevation, slope, soil type etc.).\n\n\nSo effectively, the plan is to use a random forest to classify‚Ä¶ yep, random forests.\n\n\n\nData import\n\n\nWithout further ado, let‚Äôs get the data into R and have a look at it. Note: for now, I‚Äôm going to be using the R interface to H2O, but later we‚Äôll try out the Flow interface too.\n\n# Load some packages for data manipulation\nlibrary(dplyr)\nlibrary(readr)\n\n# Data from http://archive.ics.uci.edu/ml/datasets/Covertype\ncovtype &lt;- read_csv(\"~/H2O_exploration/covtype.csv\", col_names = FALSE) %&gt;%\n    # Convert some columns to factor\n    mutate_at(11:55, funs(factor(.)))\n\n# Have a look at what we've got\nstr(covtype)\n## Classes 'tbl_df', 'tbl' and 'data.frame':    581012 obs. of  55 variables:\n##  $ X1 : int  2596 2590 2804 2785 2595 2579 2606 2605 2617 2612 ...\n##  $ X2 : int  51 56 139 155 45 132 45 49 45 59 ...\n##  [... more columns...]\n##  $ X10: int  6279 6225 6121 6211 6172 6031 6256 6228 6244 6230 ...\n##  $ X11: Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 2 2 2 2 ...\n##  [... loads more columns...]\n##  $ X54: Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ X55: Factor w/ 7 levels \"1\",\"2\",\"3\",\"4\",..: 5 5 2 2 5 2 5 5 5 5 ...\n\nAn explanation of the dataset is available on the download page. The CSV file itself doesn‚Äôt contain any column names, and the ones R creates for us aren‚Äôt particularly helpful; but actually, it won‚Äôt really matter. We only need to take note of a couple of things:\n\n\n\nThe first 10 columns are numeric variables representing various geographical aspects of the location in question. Average yearly rainfall, degree of slope, number of minutes of sunlight between 3 and 4 o‚Äôclock on the second Wednesday of August during a leap year, and so on.\n\n\nColumns 11-14 are binary categorical variables signalling whether the forest is within one of four ‚ÄúWilderness Areas‚Äù. So if the forest is in one of these areas, then the column representing that area will contain a 1; otherwise each column will contain a 0.\n\n\nColumns 15-54 are binary categorical variables representing soil type: i.e.¬†Column 11 is storing the answer to ‚Äúis this area Soil Type 1?‚Äù, where the answers ‚Äúyes‚Äù or ‚Äúno‚Äù are represented by 1 or 0 respectively. So each record has a 1 in precisely one of these columns, and 0 in the other columns.\n\n\nColumn 55 is the important one: it‚Äôs the label indicating what the cover type actually is. This is the target variable which we‚Äôre going to try to predict! There are 7 different possible labels, representing spruce/fir, aspen, Ponderosa pine etc.\n\n\n\nWe converted columns 11-55 to factors when we imported the data, so we can crack on with some machine learning right away.\n\n\n\n\nThe h2o R package\n\n\nH2O can be downloaded on its own, but if you‚Äôre using it with R it‚Äôs easier to install the h2o package (which is available on CRAN). You‚Äôll also need to have Java installed.\n\n\nThen you can go ahead and load the package into R, and before you can do anything else you‚Äôll need to get an H2O cluster up and running using h2o.init().\n\nlibrary(h2o)\nh2o.init()\n##  Connection successful!\n## \n## R is connected to the H2O cluster: \n##     H2O cluster uptime:         2 hours 54 minutes \n##     H2O cluster version:        3.10.5.3 \n##     H2O cluster version age:    1 month and 15 days  \n##     H2O cluster name:           H2O_started_from_R_ojones_mlh028 \n##     H2O cluster total nodes:    1 \n##     H2O cluster total memory:   1.50 GB \n##     H2O cluster total cores:    4 \n##     H2O cluster allowed cores:  4 \n##     H2O cluster healthy:        TRUE \n##     H2O Connection ip:          localhost \n##     H2O Connection port:        54321 \n##     H2O Connection proxy:       NA \n##     H2O Internal Security:      FALSE \n##     R Version:                  R version 3.4.0 (2017-04-21)\n\nThis shows us some information about the session we‚Äôve just created. Notice the cluster is running on a Java Virtual Machine (JVM), and in my case I‚Äôm running on 1 node (my laptop) and 4 cores. If you‚Äôre lucky enough to have a larger setup then you can edit these settings to suit you.\n\n\nThe only function we‚Äôve used so far gives a good indication of how the package works: the general form of functions is h2o.(‚Ä¶), and more often than not the  is replaced by what you would guess you should put there. At first, we want to initialise a cluster, so we use h2o.init. The function names tend to be quite intuitive, so after you‚Äôve seen them once or twice it‚Äôs very easy to remember them.\n\n\nOn with the show! Recall that our covtype dataset is currently sitting happily in an R dataframe. However, we‚Äôre only accessing our H2O cluster through R (remember it‚Äôs running behind the scenes on a JVM), so we need to pass the data on to the cluster in a format it can work with.\n\n\nFortunately this is very easy: as.h2o converts a dataframe to an ‚ÄúH2OFrame‚Äù. In the R session, it‚Äôs stored as an environment, but actually this is just a pointer to the data structure which has been created by H2O.\n\nrwet &lt;- as.h2o(covtype)\n\nWe‚Äôre going to want to assess the performance of our random forest once we‚Äôve trained it, so we‚Äôll split our data into training and test sets using h2o.splitFrame - intuitive names, remember?\n\nrwet_split &lt;- h2o.splitFrame(wet, ratios = 0.75, seed = 820)\n\nThe observations are randomly divided into sets according to the ratios specfied. Our split is 75:25, but ratios can be a vector too: if, for example, you wanted to create a cross-validation set too, then you could specify ratios = c(0.6, 0.2) to get a 60:20:20 split.\n\n\nNow we can train our classifier. We want a random forest, so we use - you guessed it - h2o.randomForest. Remember that the label for each observation is in the 55th column, so we set the target variable y to 55. Our training dataframe is the first set from our split data, and then you can include a whole load of other optional parameters. I‚Äôm not going to go into huge detail here; I‚Äôm just going to set the number of trees to train, and the number of folds for cross-validation. But if you‚Äôre super keen on making sure your trees are balanced or on fiddling around with per-class sample rates, then rest assured that you can do so. If you don‚Äôt think balancing trees will do your back any good then fear not: H2O does a pretty good job of using sensible defaults for any parameters you don‚Äôt explicitly include.\n\nh2o_rf &lt;- h2o.randomForest(y = 55, training_frame = wet_split[[1]],\n                              ntrees = 1, nfolds = 2)\n\nYou might argue that one tree a forest doth not make, and I would agree with you. This is just a demonstration of the syntax - we‚Äôll make a proper forest in a minute!\n\n\nOnce the model has finished training, we can calculate some useful performance metrics using h2o.performance - I‚Äôm passing in the model we‚Äôve just trained, and the second part of our dataset (which is our test set).\n\nrh2o.performance(h2o_rf, wet_split[[2]])\n## H2OMultinomialMetrics: drf\n## \n## Test Set Metrics: \n## =====================\n## \n## MSE: (Extract with `h2o.mse`) 0.1253515\n## RMSE: (Extract with `h2o.rmse`) 0.3540501\n## Logloss: (Extract with `h2o.logloss`) 1.204014\n## Mean Per-Class Error: 0.256692\n## Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;, &lt;data&gt;)`)\n## =========================================================================\n## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n##            1     2    3   4    5    6    7  Error               Rate\n## 1      42484  9651   12   1   98   37  442 0.1942 =  10,241 / 52,725\n## 2       5576 64313  457   7  266  268   96 0.0940 =   6,670 / 70,983\n## 3        167   526 7609  93   13  469    2 0.1430 =    1,270 / 8,879\n## 4         27    34  129 463    1   59    0 0.3506 =        250 / 713\n## 5        146  1144   44   0 1030    2    3 0.5652 =    1,339 / 2,369\n## 6        114   634  584  40    6 3006    7 0.3154 =    1,385 / 4,391\n## 7        557   105    4   1    1   15 4401 0.1343 =      683 / 5,084\n## Totals 49071 76407 8839 605 1415 3856 4951 0.1505 = 21,838 / 145,144\n## \n## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;, &lt;data&gt;)`\n## =======================================================================\n## Top-7 Hit Ratios: \n##   k hit_ratio\n## 1 1  0.849542\n## 2 2  0.969492\n## 3 3  0.978669\n## 4 4  0.979165\n## 5 5  0.979200\n## 6 6  0.979200\n## 7 7  1.000000\n\nThis gives us numeric metrics such as mean square error; a confusion matrix of predicted vs actual classes, with per-class error rates calculated for us; and a ‚Äúhit ratio table‚Äù.\n\n\nFor each observation, the classifier works out how likely it is that the observation belongs to each of the 7 classes. This allows it to make a ‚Äúbest guess‚Äù at the actual class (which is the ‚Äúprediction‚Äù), but it can also make a second-best guess, a third-best, and so on.\n\n\n\n\n\nThe (k)th row of the hit ratio table gives us the proportion of observations which are correctly classified within the top (k) guesses the classifier makes. So (k=1) is the accuracy of the classifier - i.e.¬†the proportion of observations which are correctly predicted - (k=2) is the proportion which are within the first two guesses the classifier makes, and so on. The actual class will definitely be in the top 7 guesses (there are only 7 possible classes!) so for (k=7) the hit ratio is 100%.\n\n\nIf your classification task is binomial then you can calculate many other relevant metrics (precision, F1 score, false negative rate etc.) using the selection of h2o.metric functions which are available.\n\n\nWe‚Äôll come back to this random forest soon, but first I‚Äôm going to have a little look at H2O Flow."
  },
  {
    "objectID": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html#h2o-flow",
    "href": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html#h2o-flow",
    "title": "H2O.ai: Going for a paddle",
    "section": "\nH2O Flow\n",
    "text": "H2O Flow\n\n\nH2O Flow is H2O‚Äôs own browser-based interface. Getting started is easy:\n\n\n\nInitialise an H2O cluster (e.g.¬†by running h2o.init() in R)\n\n\nGo to localhost:54321 in browser\n\n\nUnplug keyboard\n\n\n\nHonestly. You can do that. You don‚Äôt need it.\n\n\n\n\n\nFlow is based on a notebook-style layout. Code is added to cells, which are then run in sequence. You can write raw H2O code yourself, but it‚Äôs much easier to use the drop-down menus and buttons and let Flow compile the code for you.\n\n\nThe really nice thing about Flow is that it allows you to see exactly what your options are at each stage. When you finish one operation, it gives you a selection of things which you might want to do next. When you are able to change options or parameters, it lists them all out for you so you can see which ones are available and which ones you might want to change.\n\n\n\n\n\nIf you like writing code and you already know exactly what you‚Äôre planning on doing, then the R interface is quicker to use and much more concise. But I‚Äôd encourage you to play around with Flow for a while at some point, because you‚Äôll probably discover something that you hadn‚Äôt used before.\n\n\nAlso, it‚Äôs worth clarifying that when you‚Äôre using H2O, your data is always on your cluster (in my case, my laptop, but it could be your big server in the basement). Flow uses the browser for display, but it‚Äôs not pinging a server - it‚Äôs all hosted locally (hence localhost!). So you don‚Äôt need to worry about data being intercepted or anything.\n\n\n\nComparison: randomForest and caret\n\n\nGoing back to our random-forest random forest from earlier, we can set up the R equivalent of the H2O forest using the randomForest and caret packages. Specifically, caret is great for dealing with the ML process as a whole - splitting data, managing cross-validation etc. - and in this case it uses randomForest to actually train a classifier.\n\n\nI‚Äôm putting the code for this into a little function called do_rf: it just records the time taken to set up and train the forest, and then returns this time along with performance metrics for the forest.\n\ndo_rf &lt;- function(ntrees, nfolds) {\n\n    # Record time at start\n    start &lt;- Sys.time()\n    \n    # Make/train the forest\n    library(caret)\n    set.seed(820)\n\n    caret_split &lt;- createDataPartition(covtype$X55, p = 0.75, list = FALSE, times = 1)\n    \n    rf &lt;- train(X55 ~ ., data = covtype[caret_split, ],\n               trControl = trainControl(method = \"cv\", number = 2),\n               method = \"rf\", ntree = ntrees)\n    \n    # Stop the clock\n    exec_time &lt;- Sys.time() - start\n    \n    # Make predictions\n    preds &lt;- predict(rf, covtype[-caret_split, ])\n    \n    # Return runtime and performance metrics\n    list(exec_time, confusionMatrix(preds, covtype$X55[-caret_split]))\n}\n\nTechnically, cross-validation isn‚Äôt really necessary for a random forest, since the algorithm isn‚Äôt prone to overfitting. However, it‚Äôs a useful example to show how the entire ML workflow is integrated into H2O (using cross-validation is as simple as setting the nfolds parameter), whereas in R you really need to use a ‚Äúmeta‚Äù-ML package such as caret or mlr to set up ML projects.\n\n\nLet‚Äôs run the do_rf function and see what happens. We‚Äôll use 10 trees and 2-fold cross-validation.\n\nrdo_rf(ntrees = 10, nfolds = 2) rconsole## [[1]] ## Time difference of 7.788886 mins ##  ## [[2]] ## Confusion Matrix and Statistics ##  ##           Reference ## Prediction     1     2     3     4     5     6     7 ##          1 50591  1564     1     0    29    11   200 ##          2  2208 68905   128     0   352    93    31 ##          3     2   119  8576    80    30   243     0 ##          4     0     0    43   583     0    18     0 ##          5    26   126    14     0  1955     4     2 ##          6     4    85   176    23     7  3972     0 ##          7   129    26     0     0     0     0  4894 ##  ## Overall Statistics ##                                            ##                Accuracy : 0.9602           ##                  95% CI : (0.9592, 0.9612) ##                [... some more stats]\n\nPerformance is reasonably good - according to the UCI website a simple backprop neural net achieved only 70% accuracy on this dataset, so we‚Äôre well above that.\n\n\nWe can compare this with the performance of the H2O forest by writing a similar do_h2o function containing the code from earlier, then running this function and looking at the results.\n\nrh2o_results &lt;- do_h2o(ntrees = 10, nfolds = 2) print(h2o_results)\nrconsole ## [[1]] ## Time difference of 4.436078 mins ##  ## [[2]] ## H2OMultinomialMetrics: drf ##  ## Test Set Metrics:  ## ===================== ##  ## MSE: (Extract with `h2o.mse`) 0.09325105 ## RMSE: (Extract with `h2o.rmse`) 0.3053703 ## Logloss: (Extract with `h2o.logloss`) 0.3262433 ## Mean Per-Class Error: 0.183704 ## Confusion Matrix: Extract with `h2o.confusionMatrix(&lt;model&gt;, &lt;data&gt;)`) ## ========================================================================= ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class ##            1     2    3   4    5    6    7  Error               Rate ## 1      45661  6932    8   0    8    4  112 0.1340 =   7,064 / 52,725 ## 2       2887 67758  188   2   55   71   22 0.0454 =   3,225 / 70,983 ## 3          0   438 8284  16    0  141    0 0.0670 =      595 / 8,879 ## 4          0     0  133 558    0   22    0 0.2174 =        155 / 713 ## 5         38  1143   38   0 1147    3    0 0.5158 =    1,222 / 2,369 ## 6          7   357  499  24    1 3503    0 0.2022 =      888 / 4,391 ## 7        507    22    0   0    0    0 4555 0.1041 =      529 / 5,084 ## Totals 49100 76650 9150 600 1211 3744 4689 0.0942 = 13,678 / 145,144 ##  ## Hit Ratio Table: Extract with `h2o.hit_ratio_table(&lt;model&gt;, &lt;data&gt;)` ## ======================================================================= ## Top-7 Hit Ratios:  ##   k hit_ratio ## 1 1  0.905763 ## 2 2  0.994791 ## 3 3  0.999290 ## 4 4  0.999862 ## 5 5  0.999972 ## 6 6  0.999986 ## 7 7  1.000000\n\nIt turns out that in this case, H2O is faster than R, but the performance is actually worse. In. This. Case. This is not a solid benchmarking test, and of course it would be possible to improve the performance of both of the implementations through some parameter adjustment, but I‚Äôm not going to dive into all that. This is just a paddle, remember.\n\n\n\n\nComparison: sparklyr\n\n\nPerhaps the best-known ML platform is Apache Spark, and seeing as there‚Äôs an R package to interface with it - sparklyr - I thought I‚Äôd take a brief tangent in order to try it out.\n\n\nIt turned out I was being a bit optimistic. It took me about 10 minutes to set up the H2O random forest for the first time; it took a few hours and lots of yelling at incomprehensible Java-related error messages before I managed to get compatible versions of Spark and Java configured correctly. In fairness this was probably more my fault than Spark‚Äôs‚Ä¶\n\n\nOnce it was finally set up, the ML process was quite similar to H2O (although I must admit that initially I found the syntax a little more difficult to get my head around).\n\n```r library(sparklyr) library(dplyr)\nsc &lt;- spark_connect(master = ‚Äúlocal‚Äù, version = ‚Äú2.0.0‚Äù)\nspark_read_csv(sc, ‚Äúspark_covtype‚Äù, ‚Äúcovtype.csv‚Äù, header = FALSE)\ncovtype_tbl &lt;- tbl(sc, ‚Äúspark_covtype‚Äù)\npartition &lt;- sdf_partition(covtype_tbl, train = 0.75, test = 0.25, seed = 820) train_tbl &lt;- partition\\(train\ntest_tbl &lt;- partition\\)test\nml_rf &lt;- ml_random_forest(train_tbl, V55 ~ ., max.depth = 20, max_bins = 32, num.trees = 20, type = ‚Äúclassification‚Äù)\npreds &lt;- sdf_predict(ml_rf, test_tbl)\nml_classification_eval(preds, ‚ÄúV55‚Äù, ‚Äúprediction‚Äù, ‚Äúaccuracy‚Äù)rconsole## [1] 0.8295277&lt;p&gt;At the time of writing there is no facility in &lt;code&gt;sparklyr&lt;/code&gt; for cross-validation. Having said that, the Spark MLlib library &lt;em&gt;does&lt;/em&gt; include cross-validation, and &lt;code&gt;sparklyr&lt;/code&gt; is still under active development, so this is something which may be added in future.&lt;/p&gt; &lt;p&gt;Running a &lt;code&gt;do_spark(ntrees = 10)&lt;/code&gt; function reveals a faster time but much lower accuracy than either R or H2O, but this isn‚Äôt surprising given that we‚Äôre not using cross-validation so we‚Äôre doing less than half the work.&lt;/p&gt; &lt;div id=\"results-comparison\" class=\"section level4\"&gt; &lt;h4&gt;Results comparison&lt;/h4&gt; &lt;table&gt; &lt;thead&gt; &lt;tr class=\"header\"&gt; &lt;th align=\"left\"&gt;&lt;/th&gt; &lt;th align=\"right\"&gt;Accuracy&lt;/th&gt; &lt;th align=\"right\"&gt;Time (mins)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr class=\"odd\"&gt; &lt;td align=\"left\"&gt;&lt;code&gt;randomForest&lt;/code&gt; and &lt;code&gt;caret&lt;/code&gt;&lt;/td&gt; &lt;td align=\"right\"&gt;0.960&lt;/td&gt; &lt;td align=\"right\"&gt;7.8&lt;/td&gt; &lt;/tr&gt; &lt;tr class=\"even\"&gt; &lt;td align=\"left\"&gt;&lt;code&gt;h2o&lt;/code&gt;&lt;/td&gt; &lt;td align=\"right\"&gt;0.906&lt;/td&gt; &lt;td align=\"right\"&gt;4.4&lt;/td&gt; &lt;/tr&gt; &lt;tr class=\"odd\"&gt; &lt;td align=\"left\"&gt;&lt;code&gt;sparklyr&lt;/code&gt; &lt;em&gt;(no X-val)&lt;/em&gt;&lt;/td&gt; &lt;td align=\"right\"&gt;0.830&lt;/td&gt; &lt;td align=\"right\"&gt;1.9&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=\"conversion-with-rsparkling\" class=\"section level3\"&gt; &lt;h3&gt;Conversion with &lt;code&gt;rsparkling&lt;/code&gt;&lt;/h3&gt; &lt;p&gt;If your dataset is already sitting in Spark (or you're accessing it using Spark) and you want to use H2O‚Äôs algorithms on it, c‚Äôest possible. The &lt;code&gt;rsparkling&lt;/code&gt; package allows you to convert a Spark frame to an H2OFrame, which you can then attack with &lt;code&gt;h2o&lt;/code&gt;‚Äôs functions (e.g. you can take advantage of H2O's cross-validation).&lt;/p&gt;r library(rsparkling)\noptions(rsparkling.sparklingwater.version = ‚Äú2.0.11‚Äù)\nsc &lt;- spark_connect(‚Äúlocal‚Äù, version = ‚Äú2.0.0‚Äù)\nrsp_train &lt;- as_h2o_frame(sc, train_tbl, strict_version_check = FALSE) rsp_test &lt;- as_h2o_frame(sc, test_tbl, strict_version_check = FALSE)\nrsp_rf &lt;- h2o.randomForest(y = 55, training_frame = rsp_train, nfolds = 2, ntrees = 10)```\n\nRight, we‚Äôve been talking about random forests for quite a long time now. I‚Äôd say we‚Äôre certainly in over our knees, and by my definition that means we‚Äôre wading, not paddling. So I‚Äôm going to move on to a new algorithm and a new dataset."
  },
  {
    "objectID": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html#demo-neural-nets-for-mnist",
    "href": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html#demo-neural-nets-for-mnist",
    "title": "H2O.ai: Going for a paddle",
    "section": "\nDemo: Neural nets for MNIST\n",
    "text": "Demo: Neural nets for MNIST\n\n\nTime for a real classic - according to Kaggle, the de facto ‚Äúhello world‚Äù dataset of computer vision. The MNIST dataset is a collection of 60000 images of handwritten digits. Each image is 28x28 pixels in size, and is represented by a 28x28 matrix of values representing the intensity of each pixel.\n\n\n\n\n\nThis matrix is then ‚Äúunrolled‚Äù, row by row, into a 784-length vector; and these vectors can be used to train a neural network to classify new images.\n\n\n\nnnet\n\n\nFirst let‚Äôs get our data into R and create a split for training and test sets.\n\n```r library(dplyr) library(caret)\ntrain &lt;- readr::read_csv(‚Äú~/H2O_exploration/train.csv‚Äù) %&gt;% # Make the label a factor mutate(label = as.factor(label)) %&gt;% # Pixel intensities are currently on 0-255 scale; # rescale on 0-1 by dividing by 255 mutate_if(is.numeric, funs(./255))\nset.seed(528491) part &lt;- createDataPartition(1:nrow(train), p = 0.75, list = FALSE)&lt;p&gt;The &lt;code&gt;nnet&lt;/code&gt; package comes as part of the standard R installation and can be used to set up a simple feed-forward neural network. Here we‚Äôll use a 25-unit hidden layer, and we‚Äôll stop optimising after 100 iterations if we haven‚Äôt already run out of steam by then.&lt;/p&gt; &lt;p&gt;As before, we can wrap this code in a little timer function:&lt;/p&gt;r library(nnet)\ndo_rnet &lt;- function() {\nstart &lt;- Sys.time()\n\n# Predict label based on all other features (i.e. all the pixels)\n# We have to set a high MaxNWts to avoid a memory error\nrnet &lt;- nnet(label ~ ., data = train, subset = part,\n             size = 25, maxit = 100, MaxNWts = 20000)\n\nexec_time &lt;- Sys.time() - start\n\npreds &lt;- predict(rnet, train[-part, -1], type = \"class\")\n\nlist(exec_time, confusionMatrix(preds, train$label[-part]))\n}\ndo_rnet()rconsole ## [[1]] ## Time difference of 33.20143 mins ## ## [[2]] ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 2 3 4 5 6 7 8 9 ## 0 977 0 8 7 2 15 9 3 4 3 ## 1 0 1093 5 3 5 1 0 3 11 2 ## 2 4 10 977 17 7 5 11 15 7 3 ## 3 5 7 23 982 5 29 2 8 26 8 ## 4 1 3 7 0 964 1 5 8 2 33 ## 5 8 4 4 19 3 880 14 6 9 7 ## 6 4 0 3 3 11 14 999 1 7 1 ## 7 0 5 7 14 9 4 2 1025 3 20 ## 8 9 8 8 20 2 19 7 6 937 9 ## 9 4 2 3 13 32 11 1 17 7 973 ## ## Overall Statistics ##\n## Accuracy : 0.934\n## 95% CI : (0.9291, 0.9387) ## [‚Ä¶ etc.]&lt;p&gt;The accuracy ain‚Äôt much to shout about - the top MNIST classifiers nowadays are well above 99% - but it isn‚Äôt too bad for such a small implementation. It takes a little while to get there though.&lt;/p&gt; &lt;/div&gt; &lt;div id=\"h2o\" class=\"section level3\"&gt; &lt;h3&gt;H2O&lt;/h3&gt; &lt;p&gt;Let‚Äôs see how H2O‚Äôs implementation compares (they follow the relatively recent trend of calling neural networks ‚Äúdeep learning‚Äù classifiers). Note that we‚Äôre using the same setup here as we did for &lt;code&gt;nnet&lt;/code&gt;, i.e. one hidden layer of 25 units, although I couldn‚Äôt figure out how to limit the number of steps. Also note that the syntax is almost identical to when we implemented the random forest earlier; we‚Äôve just swapped out &lt;code&gt;randomForest&lt;/code&gt; for &lt;code&gt;deeplearning&lt;/code&gt;.&lt;/p&gt;r do_h2o_net &lt;- function() {\nstart &lt;- Sys.time()\n\nlibrary(h2o)\nh2o.init()\n\nwetter &lt;- as.h2o(train)\nwetter_split &lt;- h2o.splitFrame(wetter, ratios = 0.75, seed = 528491)\n\nh2o_net &lt;- h2o.deeplearning(\n    x = 2:785,\n    y = 1,\n    training_frame = wetter_split[[1]],\n    hidden = 25\n)\n\nexec_time &lt;- Sys.time() - start\n\nlist(exec_time, h2o.performance(h2o_net, wetter_split[[2]]))\n}\nresults &lt;- do_h2o_net() resultsrconsole## [[1]] ## Time difference of 1.802627 mins ## ## [[2]] ## H2OMultinomialMetrics: deeplearning ## ## Test Set Metrics: ## ===================== ## ## MSE: (Extract with h2o.mse) 0.0657653 ## RMSE: (Extract with h2o.rmse) 0.2564475 ## Logloss: (Extract with h2o.logloss) 0.5847708 ## Mean Per-Class Error: 0.07453935 ## Confusion Matrix: Extract with h2o.confusionMatrix(&lt;model&gt;, &lt;data&gt;)) ## ========================================================================= ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class ## 0 1 2 3 4 5 6 7 8 9 Error ## 0 1029 0 1 5 3 6 4 5 6 5 0.0329 ## 1 0 1189 2 6 3 1 3 1 5 0 0.0174 ## 2 8 12 967 17 6 4 15 16 13 2 0.0877 ## 3 1 6 25 1005 1 51 3 16 16 8 0.1122 ## 4 4 6 9 1 976 1 3 9 2 31 0.0633 ## 5 3 5 6 38 5 844 19 2 19 6 0.1088 ## 6 7 5 6 2 8 14 971 2 11 0 0.0536 ## 7 5 2 15 9 5 4 3 1010 2 23 0.0631 ## 8 2 27 5 23 7 15 7 4 929 8 0.0954 ## 9 10 2 4 7 36 13 2 35 5 913 0.1110 ## Totals 1069 1254 1040 1113 1050 953 1030 1100 1008 996 0.0735 ## Rate ## 0 = 35 / 1,064 ## 1 = 21 / 1,210 ## 2 = 93 / 1,060 ## 3 = 127 / 1,132 ## 4 = 66 / 1,042 ## 5 = 103 / 947 ## 6 = 55 / 1,026 ## 7 = 68 / 1,078 ## 8 = 98 / 1,027 ## 9 = 114 / 1,027 ## Totals = 780 / 10,613 ## ## Hit Ratio Table: Extract with h2o.hit_ratio_table(&lt;model&gt;, &lt;data&gt;) ## ======================================================================= ## Top-10 Hit Ratios: ## k hit_ratio ## 1 1 0.926505 ## 2 2 0.972581 ## 3 3 0.985490 ## 4 4 0.992556 ## 5 5 0.995006 ## 6 6 0.996608 ## 7 7 0.998116 ## 8 8 0.998775 ## 9 9 0.999152 ## 10 10 1.000000```\n\nWoah! The accuracy is virtually identical, but we manage it in under 2 minutes, rather than over half an hour.\n\n\nJust for the sake of it, I also ran a TensorFlow convolutional neural net in Python to see how performance compared. The accuracy was higher than both of the previous implementations and the training process was nearly as fast as H2O had been. However, an upgraded H2O net with two 50-unit hidden layers (as simple as swapping out hidden = 25 for hidden = c(50, 50) in the previous code) surpassed even the convnet, both in terms of accuracy and speed.\n\n\n\nNeural nets: results comparison\n\n\n\n\n\n\n\n\n\nAccuracy\n\n\nTime (mins)\n\n\n\n\n\n\nnnet\n\n\n(100 steps, 25 hidden units)\n\n\n0.934\n\n\n33.0\n\n\n\n\nH2O deeplearning\n\n\n(??? steps, 25 hidden units)\n\n\n0.927\n\n\n1.8\n\n\n\n\nTensorFlow\n\n\n(500 steps, 2*conv/relu/FC)\n\n\n0.950\n\n\n3.7\n\n\n\n\nH2O deeplearning\n\n\n(??? steps, 2*50 hidden units)\n\n\n0.956\n\n\n2.2\n\n\n\n\n\nThere‚Äôs another slightly subtle point here, but it‚Äôs a very important one. Notice that when we increase the complexity of the H2O net, the impact on training time is very small.\n\n\nHowever, swapping one 25-unit hidden layer for two 50-unit hidden layers means a massive increase in computational complexity. It‚Äôs not just because we have 4 times the number of neurons; that extra hidden layer means another stage of backpropagation, which is very computationally expensive. So actually, from the relatively small increase in training time, we see that the ML process scales very well (for this medium-sized dataset at least) and we can infer that a large chunk of those 2 minutes is in fact spent on data conversion."
  },
  {
    "objectID": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html#h2o-or-h2no",
    "href": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html#h2o-or-h2no",
    "title": "H2O.ai: Going for a paddle",
    "section": "\nH2O or H2No?\n",
    "text": "H2O or H2No?\n\n\nSo, should you consider using H2O for your ML applications? Let‚Äôs review what we‚Äôve discovered.\n\n\n\nH2Pros\n\n\n\nMultiple interfaces available. If you use R or Python you can seamlessly integrate it into your existing workflows.\n\n\nIntuitive to use. The R interface uses sensible, memorable function names. The Flow interface guides you through the process and shows you e v e r y t h i n g.\n\n\nML scales very well. More data? No problem. Complicated algorithm? No problem.\n\n\nFine control is possible. Loads of parameters to fiddle around with. Flow shows you exactly what they are. If the ‚ÄúAdvanced‚Äù ones aren‚Äôt hard-core enough for you, there‚Äôs an ‚ÄúExpert‚Äù section too.\n\n\nIntegrated ML workflow. (sparklyr, we‚Äôre waiting!)\n\n\n\n\n\nH2Woes\n\n\n\nLimited selection of algorithms. I mean, ‚Äúlimited‚Äù here is relative. If you need to use Gaussian mixtures or your application desperately requires Latent Dirichlet Allocation, you‚Äôd better head on over to Spark. But H2O covers most things you are likely to use on a regular basis.\n\n\nData conversion can be H2Slow. But again, this is relative. If you have to spend 2 minutes converting your data to an H2OFrame, but this saves you 30 minutes of algorithm runtime, you‚Äôre probably prepared to do it!\n\n\nSome algorithms are black box methods. But‚Ä¶ ¬Ø_(„ÉÑ)_/¬Ø. The algorithms which are ‚Äúblack-box‚Äù are still black-box whichever implementation you‚Äôre using, so this probably isn‚Äôt a downside to H2O per se - it‚Äôs more of a general ML ‚Äúproblem‚Äù (whether it‚Äôs an actual problem often depends on your point of view). And H2O is open source, so if you want to you can go and dig around in the source code.\n\n\n\nSo if you‚Äôre looking for a way to speed up your ML training times by replacing R algorithm implementations, in my opinion H2O is a great choice of ML framework."
  },
  {
    "objectID": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html#bon-voyage",
    "href": "posts/2017-08-23-h2o-ai-going-for-a-paddle/index.html#bon-voyage",
    "title": "H2O.ai: Going for a paddle",
    "section": "\nBon voyage!\n",
    "text": "Bon voyage!\n\n\nI might have mentioned once or twice that this was just a paddle (sorry if you‚Äôre bored of the theme, but I promise I‚Äôm nearly done!). I really have only touched on the basics here: there‚Äôs loads more to H2O than what I‚Äôve covered in this post. If you‚Äôre interested in GPU-based deep learning, you might want to go and read about Deep Water. If Spark and H2O sounds like a super combination, check out Sparkling Water. If you‚Äôre keen on producing streamlined data analysis applications, have a look at Steam. And of course the documentation for H2O, and for all those other H2O.ai enterprises, goes into much greater detail than I possibly could.\n\n\nOff you go then! It‚Äôs been fun paddling together, but now I‚Äôll leave you to do your own thing. Go and dry off, or carry on frolicking in the waves, or get started training for your 100m freestyle world record attempt. Me? I‚Äôm off for an ice cream."
  },
  {
    "objectID": "posts/2016-09-30-digit-recognition-building-a-neural-network-from-scratch-in-r/index.html",
    "href": "posts/2016-09-30-digit-recognition-building-a-neural-network-from-scratch-in-r/index.html",
    "title": "Digit Recognition - Building a neural network from scratch in R",
    "section": "",
    "text": "Update: On 3rd March 2017, I presented a workshop based on this project at the Bath Machine Learning Meetup. You can see the event page here!\n\n\nMotivation for this project\nA few months ago I was introduced to data science and machine learning by a friend. Since then I have developed a great interest in the field, and recently my imagination has been captured by the topic of neural networks.\nCombining this, a desire to improve my programming skills, my irrational fondness for R as a programming language, and a digit recognition challenge hosted online by Kaggle, I decided to create, train and utilize a simple neural network - entirely from scratch - within R.\n\n\n\nThe neural network: methodology and notes\n\nSetting up the environment\nI have used two packages other than those included with base R: reshape2 and ggplot2, both of which are widely-used packages that can be installed from the CRAN repository. These are used within the visualize function (and its variants) to simplify the plotting process; I shall explain exactly how this is done later on.\nlibrary(reshape2)\nlibrary(ggplot2)\n\n# Set seed, for reproducibility of 'random' samples found later\nset.seed(1234)\n\n\nFunctions\nI have broken down the construction and training of the neural network into several functions. These functions are:\n\ncomputeCost\ncomputeGrad\nloadData\nmakeSets\noptLambda\noptNeurons\npredict\nrandomInitParams\nsigmoid\nsigmoidGradient\nvisualize\nvisualizeBW\nvisualizeMulti\n\nI will explain the purpose of each of them at the time that they are used in the project.\n\n\nInspecting the data\nThe data is provided in two .csv files. We will talk about test.csv later - for now we‚Äôll focus on the first file, train.csv, which contains 42000 rows of data in 785 columns. Each row corresponds to one 28x28 pixel grayscale image of a handwritten digit from 0 to 9. The first column is a label indicating which digit is represented in the image. The next 784 columns are an ‚Äòunrolled‚Äô representation of the image: the first 28 entries represent the first row of the image, the next 28 represent the second row, and so on. Each entry has a value from 0 to 255 representing the ‚Äòintensity‚Äô of the pixel it represents. Let‚Äôs examine the first row of train.csv:\n##   label pixel0 pixel1 \"...\" pixel352 pixel353 \"...\" pixel782 pixel783\n## 1     1      0      0   ...      240       81   ...        0        0\nIt is difficult to see what the image is from this raw data alone. Instead, we can visualize the data with a plot: this is the purpose of the visualize function.\n# visualize recreates a square image from the unrolled matrix that represents it\n\nvisualize &lt;- function(imgvec) {\n\n    n &lt;- length(imgvec)\n    \n    # Reshape the vector into a matrix\n    img &lt;- matrix(imgvec, sqrt(n))\n    \n    # Reformat the data for plotting\n    imgmelt &lt;- melt(img)\n    \n    p &lt;- ggplot(imgmelt, aes(x = Var1, y = -Var2 + sqrt(n) + 1)) +\n        geom_raster(aes(fill = imgmelt$value)) +\n        scale_fill_gradient(low = \"white\", high = \"black\", guide = FALSE) +\n        labs(x = NULL, y = NULL, fill = NULL)\n  \n    print(p)       \n}\nThe image vector, not including the label, is reshaped into a square matrix img. This matrix is then ‚Äòmelted‚Äô into a long-format dataframe using reshape2‚Äôs melt function: this replaces the spatial position of each pixel in the image with two attributes Var1 and Var2, representing the row and column of the pixel. This dataframe is passed to ggplot and each pixel is plotted on a scale from white (pixels with value 0) to black (pixel of highest value; usually 255).\n# Read in first training image, except the label in the first column, and\n# convert to numeric vector\nimg &lt;- as.numeric( read.csv(\"data/train.csv\", header = TRUE, nrows = 1)[-c(1)] )\n\nvisualize(img)\n\nIt‚Äôs now clear to us that this image is of the digit 1 - as confirmed by its label. We will look at some more examples shortly, but first we will read the data into R.\n\n\nLoading and tidying the data\nThe function loadData is used to load the training and test data into the R workspace (up to an optional cutoff point):\n# loadData reads in train and test data and stores them in the workspace\n\nloadData &lt;- function(directory = \"data\", cutoff) {\n    \n    if(missing(cutoff)) {\n        \n        print(\"Reading training examples...\")\n        train &lt;&lt;- read.csv( paste0(directory, \"/train.csv\"))\n        \n        print(\"Reading test examples...\")\n        test &lt;&lt;- read.csv( paste0(directory, \"/test.csv\"))\n        \n    } else {\n        \n        print(\"Reading training examples...\")\n        train &lt;&lt;- read.csv( paste0(directory, \"/train.csv\"), nrows = cutoff)\n        \n        print(\"Reading test examples...\")\n        test &lt;&lt;- read.csv( paste0(directory, \"/test.csv\"), nrows = cutoff )\n        \n    }\n    \n    print(\"Data loaded!\")\n}\nloadData()\n## [1] \"Reading training examples...\"\n## [1] \"Reading test examples...\"\n## [1] \"Data loaded!\"\nFor now we‚Äôll focus on the train data, which will be used to train and evaluate the performance of the neural network. The data are currently in a large dataframe where each row corresponds to an image; the first column contains the label, and the subsequent columns contain the pixel values, as described earlier.\nThe function makeSets performs two jobs. First it separates the labels from the image data. Each value in the image data is scaled from 0 to 1 since this improves the network‚Äôs performance. The data is then divided into three matrices: xtrain, xval and xtest. These contain 60%, 20% and 20% of the examples respectively. The labels are split into numeric vectors ytrain, yval and ytest in the same way. Note that the first entry of ytrain is the label for the first row of xtrain, and so on. The new matrices and vectors are stored in the workspace. Note: Labels of 0 are replaced with 10 in the y~ vectors - this is to avoid computation issues which may arise later (since R indexes vectors from 1 rather than 0).\n# makeSets creates training, cross-validation and test sets from a set of data\n\nmakeSets &lt;- function(dataset) {\n    \n    m &lt;- nrow(dataset)\n    \n    \n    x &lt;- as.matrix(dataset[, -c(1)])\n    \n    # Scale values from 0 to 1 since this improves performance\n    x &lt;- x / max(x)\n    \n    xtrain &lt;&lt;- x[1:floor(m*0.6), ]\n    \n    xval &lt;&lt;- x[(floor(m*0.6) + 1):floor(m*0.8), ]\n                        \n    xtest &lt;&lt;- x[(floor(m*0.8) + 1):m, ]\n    \n    \n    y &lt;- as.numeric(dataset[, 1])\n    \n    # Turn 0-labels into 10-labels; this will simplify later computation\n    y[y == 0] &lt;- 10\n\n    ytrain &lt;&lt;- y[1:floor(m*0.6)]\n    \n    yval &lt;&lt;- y[(floor(m*0.6) + 1):floor(m*0.8)]\n    \n    ytest &lt;&lt;- y[(floor(m*0.8) + 1):m]\n    \n}\nmakeSets(train)\nEach of these sets has a specific purpose: xtrain (the training set) will be used to train the network, xval (the cross-validation set) will be used to adjust the network‚Äôs hyper-parameters (see the next section), and xtest will be used to evaluate the overall performance of the network.\nNow that the data are in a nicer format for calculations, we can look at some more examples of images represented in xtrain using visualizeMulti, a slight adaptation of the visualize function.\n# visualizeMulti recreates images from the unrolled matrices that represent them\n\nvisualizeMulti &lt;- function(imgvecs) {\n    \n    print(\"Converting data type...\")\n    \n    # R drops a 1-row matrix to a vector by default; undo this, if it occurs\n    imgvecs &lt;- as.matrix(imgvecs)\n    \n    # Number of images\n    m &lt;- dim(imgvecs)[1]\n    # Number of pixels per image\n    n &lt;- dim(imgvecs)[2]\n    \n    imgs &lt;- vector(mode = \"list\", length = m)\n    \n    # Reformat each image vector as a square matrix\n    for(i in 1:m) {\n        imgs[[i]] &lt;- matrix(imgvecs[i, ], sqrt(n))\n    }\n    \n    #Use reshape2's melt function to convert into a long-form data frame\n    imgmelt &lt;- melt(imgs)\n    \n    print(\"Plotting images...\")\n\n    p &lt;- ggplot(imgmelt) + facet_wrap(~L1) + \n        geom_raster(aes(x = Var1, y = (-Var2 + sqrt(n) + 1),\n                        fill = imgmelt$value)) +\n        scale_fill_gradient(high = \"black\", low = \"white\", guide = FALSE) +\n        labs(x = NULL, y = NULL, fill = NULL) +\n        theme(strip.background = element_blank(),\n              strip.text.x = element_blank(),\n              panel.margin = unit(0, \"lines\"),\n              panel.background = element_blank(),\n              panel.border = element_blank())\n    \n    print(p)\n    \n    print(\"Visualization complete!\")\n}\ns &lt;- sample(1:dim(xtrain)[1], 25)\n\nvisualizeMulti(xtrain[s, ])\n## [1] \"Converting data type...\"\n## [1] \"Plotting images...\"\n\n## [1] \"Visualization complete!\"\n\n\nSetting hyper-parameters\nThe hyper-parameters of the network are responsible for the structure of the network and for fine-tuning its performance. There are two hyper-parameters which will remain fixed for this network: these are input_layer_size and output_layer_size. The input layer is just the raw input into the network, which in this case is the 784-length vector representing an image. The output layer will give the ‚Äòresult‚Äô of that input, which we want to be a value from 1 to 10 representing the digit in the image (recall 0 is labelled as 10).\nThe neural network will have two other hyper-parameters which can be adjusted later using the cross-validation set xval. The first of these is hidden_layer_size, which sets how many neurons will be present in the hidden layer of the network and therefore how many features will be learned by the network. The second is lambda, which will control the ‚Äòstrictness‚Äô of regularization of the parameters: the higher the value, the harsher the penalty for overly specialized features. This will help prevent the network overfitting the training data, and then performing poorly on the test data.\n# Fixed-value hyper-parameters\ninput_layer_size &lt;- dim(xtrain)[2]\noutput_layer_size &lt;- 10\n\n# Hyper-parameters to be adjusted later\nhidden_layer_size &lt;- 25\nlambda &lt;- 1\n\n\nInitializing parameters\nThe parameters, or weights, of the network are what allows it to learn. To avoid multiple neurons learning the same features, these weights must be initialized to small random numbers, rather than just 0. The function randomInitParams performs this task.\n# randomInitParams outputs a random parameter vector for reshaping into Theta1\n# and Theta2\n\nrandomInitParams &lt;- function(input_layer_size, hidden_layer_size,\n                             output_layer_size) {\n    \n    # Initialize an epsilon (the following expression produces the recommended\n    # epsilon value for the sigmoid activation function)\n    epsilon &lt;- sqrt(6) / sqrt(input_layer_size + output_layer_size)\n    \n    # Create a vector with enough entries to fill the parameter matrices Theta1\n    # and Theta2 , where each entry is somewhere between epsilon and -epsilon\n    n &lt;- ((input_layer_size + 1) * hidden_layer_size) +\n        ((hidden_layer_size + 1) * output_layer_size)\n    \n    runif(n, min = -epsilon, max = epsilon)\n}\ninit_params &lt;- randomInitParams(input_layer_size, hidden_layer_size,\n                                output_layer_size)\n\n\nForward propagation and the cost function\nAn underlying principle of machine learning is to minimize a cost function (or objective function) by adjusting parameters. The calculation of the cost for this network is handled by computeCost.\n# computeCost computes the cost function of the network\n\ncomputeCost &lt;- function(init_params, input_layer_size, hidden_layer_size,\n                           output_layer_size, X, y, lambda) {\n    \n    #=============== Parameters and useful variables ===============#\n    \n    # Reshape init_params into Theta1 and Theta2\n    k &lt;- (input_layer_size + 1) * hidden_layer_size\n    Theta1 &lt;- matrix(init_params[1:k], hidden_layer_size, input_layer_size + 1)\n    Theta2 &lt;- matrix(init_params[(k + 1):length(init_params)],\n                     output_layer_size, hidden_layer_size + 1)\n    \n    # m is the number of training examples\n    m &lt;- dim(X)[1]\n    \n    \n    #=============== (Unregularized) forward propagation ===============#\n    \n    # Add bias unit to each example\n    a1 &lt;- cbind(rep(1, m), X)\n    \n    # Calculate raw hidden layer output\n    z2 &lt;- a1 %*% t(Theta1)\n    \n    # Apply activation (sigmoid) function, and add bias unit for input to next \n    # layer\n    a2 &lt;- cbind(rep(1, m), sigmoid(z2))\n    \n    # Calculate raw output layer output \n    z3 &lt;- a2 %*% t(Theta2)\n    \n    # Apply sigmoid function: a3 is the output of the network\n    a3 &lt;- sigmoid(z3)\n    \n    \n    # Initialize an identity matrix\n    diag_matrix &lt;- diag(output_layer_size)\n    \n    # Calculate cost using logistic regression cost function\n    J &lt;- (-1/m) * sum( log(a3) * t(diag_matrix[, y]) +\n                           log(1 - a3) * t((1 - diag_matrix[, y])) )\n    \n    # Cost function regularization (add sum of squares of all weights)\n    J &lt;- J + lambda/(2*m) * ( sum(Theta1[, -c(1)] ^ 2) +\n                                  sum(Theta2[, -c(1)] ^ 2) )\n    \n    J\n}\nThe first part of the function takes the init_params generated earlier and reshapes them into two matrices, Theta1 and Theta2. The rows of these matrices are, in effect, the neurons of the network.\nIn the middle section of computeCost we see how the network classifies an input as one of the 10 outputs through a process known as forward propagation. Each input vector (i.e.¬†each row of the input matrix) has a bias unit (of value 1) added at the 0th position. Two steps are then performed to calculate the hidden layer‚Äôs output for a particular input.\nFirst, the dot product of the input vector X and each neuron in the hidden layer (i.e.¬†each row in Theta1) is calculated, and stored in a vector z1:\n\\[z_{1_i} = \\sum_{j=0}^{input\\_layer\\_size} \\theta_{ij}^{(1)}X_j\\]\nThis vector is then passed to the activation function. In this project the sigmoid function is used:\n# sigmoid calculates the sigmoid of z, g(z)\n\nsigmoid &lt;- function(z) {\n    \n    1 / (1 + exp(-z))\n}\nx &lt;- c(-10:10)\nplot(x, sigmoid(x), type = \"l\", col = \"red\")\n\nThis has the effect of mapping large positive values in \\(z_1\\) to 1 and large negative values to 0, and values close to zero (roughly within the range -2 to 2) are mapped more or less linearly onto the values between 0 and 1. The result of applying the sigmoid function to \\(z_3\\), called \\(a_3\\), is the output of the hidden layer for the original input.\nThe output of the hidden layer is the input to the final layer. A bias unit is again added at position 0, and then the same process is followed as before:\n\\[z_{2_i} = \\sum_{j=0}^{hidden\\_layer\\_size} \\theta_{ij}^{(2)}a_{2_j}\\] \\[a_{3_i} = sigmoid(z_{2_i})\\]\n\\(a_3\\) is the output of the final layer in the network, and so it is the overall output of the network. Note: Using matrix multiplication allows computeCost to perform these steps on multiple inputs simultaneously and is much faster computationally than using a for loop.\nOnce all the inputs have been forward-propagated through the network, the cost J of the network can be calculated. The cost is a measure of how ‚Äòwrong‚Äô the net was in its classification of each image. For each input vector, the difference between the actual and expected output of the net is calculated. For example, if the image was of the digit 3, then the expected output of the net is a length-10 vector where all entries are 0 except for a 1 at the 3rd position. The further away the actual output is from this expected output, the higher the cost for that input. The sum of the costs of all the inputs is stored in J.\nThere is one more step in the cost calculation. A extra term is added: a scaled sum of the squared value of every parameter in the network. This is the regularization term, the scale of which is controlled by the hyper-parameter lambda which was set earlier. The higher the value of lambda, the heavier the penalty for large parameters (and large parameters are generally a sign of overfitting).\n\n\nBackpropagation and partial derivatives\nThe process of training the network involves optimizing the cost function by adjusting the network‚Äôs parameters. This is done using R‚Äôs optim function - the full process is discussed in the next section. Before we do this, however, we will discuss another function - computeGrad.\n# computeGrad computes the partial derivatives of the cost function with respect\n# to each of the parameters in Theta1 and Theta2\n\ncomputeGrad &lt;- function(init_params, input_layer_size, hidden_layer_size,\n                           output_layer_size, X, y, lambda = 0) {\n    \n    \n    #=============== Parameters and useful variables ===============#\n\n    # Reshape init_params into Theta1 and Theta2\n    k &lt;- (input_layer_size + 1) * hidden_layer_size\n    Theta1 &lt;- matrix(init_params[1:k], hidden_layer_size, input_layer_size + 1)\n    Theta2 &lt;- matrix(init_params[(k + 1):length(init_params)],\n                     output_layer_size, hidden_layer_size + 1)\n    \n    # m is the number of training examples\n    m &lt;- dim(X)[1]\n    \n    \n    #=============== (Unregularized) forward propagation ===============#\n    \n    # Add bias unit to each example\n    a1 &lt;- cbind(rep(1, m), X)\n    \n    # Calculate raw hidden layer output\n    z2 &lt;- a1 %*% t(Theta1)\n    \n    # Apply activation (sigmoid )function, and add bias unit for input to next \n    # layer\n    a2 &lt;- cbind(rep(1, m), sigmoid(z2))\n    \n    # Calculate raw output layer output \n    z3 &lt;- a2 %*% t(Theta2)\n    \n    # Apply sigmoid function: a3 is the output of the network\n    a3 &lt;- sigmoid(z3)\n    \n    \n    # Initialize an identity matrix\n    diag_matrix &lt;- diag(output_layer_size)\n\n    \n    #=============== Backpropagation ===============#\n    \n    # Recall:\n    # a1 is the original input (or, 'output of first layer'), with bias units\n    # z2 is the raw output of the hidden layer\n    # a2 is the input to the 2nd layer, with bias units\n    # z3 is the raw output of the output layer\n    # a3 is sigmoid(z3) and is the final output of the network\n    \n    \n    # Using the identity matrix from earlier, form y_matrix: the i-th row of\n    # y_matrix is all zeroes except for a 1 at the y[i]-th position\n    y_matrix &lt;- diag_matrix[y, ]\n    \n    # Error terms are now calculated for each unit in each layer, starting with\n    # the output layer, where the error is the difference between the actual and\n    # expected outputs\n    d3 &lt;- a3 - y_matrix\n    \n    # The previous layer's error depends on the weights. The 'error' d is the\n    # derivative of the cost function\n    d2 &lt;- (d3 %*% Theta2[, -c(1)]) * sigmoidGradient(z2)\n    \n    # Note that the first (input) layer doesn't have any error, by definition\n    \n    # Compute Delta 'accumulators' for each layer\n    Delta1 &lt;- t(d2) %*% a1\n    Delta2 &lt;- t(d3) %*% a2\n    \n    \n    #=============== Gradient regularization ===============#\n    \n    # Compute regularization terms for each set of parameters, to be used\n    # shortly - note first column (biases) is not included in regularization\n    Theta1_reg &lt;- lambda/m * Theta1\n    Theta1_reg[, 1] &lt;- 0\n    Theta2_reg &lt;- lambda/m * Theta2\n    Theta2_reg[, 1] &lt;- 0    \n\n    # Add the regularization terms to the Delta matrices\n    Theta1_grad &lt;- 1/m * Delta1 + Theta1_reg\n    Theta2_grad &lt;- 1/m * Delta2 + Theta2_reg\n    \n    # Note: ThetaL_grad[i,j] is the partial derivative of J(ThetaL) with\n    # respect to ThetaL[i,j]\n\n    grad &lt;- c(as.vector(Theta1_grad), as.vector(Theta2_grad))\n\n    grad\n}\nThe first two sections are the same as the first sections of computeCost: the parameter vector is reshaped into matrices Theta1 and Theta2, and the input matrix is propagated forwards through the network.\nThen, however, we come to a new process: backpropagation. The aim of backpropagation is to assess how much of an effect each individual parameter has on the cost function. This is done by calculating the partial derivative of the cost function with respect to each parameter in turn, by feeding back the error from each unit in each layer. One stage of this process uses the function sigmoidGradient:\n# sigmoidGradient calculates the sigmoid gradient of z, g'(z)\n\nsigmoidGradient &lt;- function(z) {\n    \n    temp &lt;- 1 / (1 + exp(-z))\n    \n    temp * (1 - temp)\n}\nThis is the derivative of the sigmoid function, with respect to the input.\nOnce the backpropagation step is complete, the gradient for each parameter is regularized in a similar manner to the cost in computeCost - the larger the parameter, the greater the effect of the parameter on the network. Note: By convention, the bias parameters are not regularized. The output of the function is then a vector of the ‚Äòunrolled‚Äô forms of Theta1 and Theta2.\nThis is the derivative of the sigmoid function, with respect to the input. Once the backpropagation step is complete, the gradient for each parameter is regularized in a similar manner to the cost in computeCost - the larger the parameter, the greater the effect of the parameter on the network. Note: By convention, the bias parameters are not regularized. The output of the function is then a vector of the ‚Äòunrolled‚Äô forms of Theta1 and Theta2.\n\n\nTraining the network\nEverything is now in place for the training of the network. As mentioned earlier, the aim is to minimize the cost function by changing the parameters of the network. This is done using the optim function.\nIn this case, five arguments need to be passed to optim:\n\npar ‚Äì a vector of initial values for the parameters to be optimized over\nfn ‚Äì a function to be minimized, with first argument the vector of parameters over which minimization is to take place\ngr ‚Äì a function to return the gradient for the \"BFGS\", \"CG\" and \"L-BFGS-B\" methods\nmethod ‚Äì the method to be used\ncontrol ‚Äì a list of control parameters\n\nWe will use the following arguments:\n\n\n\nArgument name\nArgument value\n\n\n\n\npar\ninit_params\n\n\nfn\nanonymous function based on computeCost\n\n\ngr\nanonymous function based on computeGrad\n\n\nmethod\nL-BFGS-B\n\n\ncontrol\nlist(maxit = 50)\n\n\n\noptim_out &lt;- optim(init_params,\n                   function(p) computeCost(p, input_layer_size,\n                                           hidden_layer_size,\n                                           output_layer_size,\n                                           xtrain, ytrain, lambda),\n                   function(p) computeGrad(p, input_layer_size,\n                                           hidden_layer_size,\n                                           output_layer_size,\n                                           xtrain, ytrain, lambda),\n                   method = \"L-BFGS-B\", control = list(maxit = 50))\nThe two anonymous functions are required since only the first argument, the parameters vector, is changed in each case, whereas the rest of the arguments remain constant. The optimization method used, L-BFGS-B, seems to perform best for this application but other methods can be experimented with. A limit of 50 optimization iterations is set by control - this is to limit computation time, but again this can be tweaked if desired.\noptim gives several outputs - the first of these is the now-optimized vector of parameters (the second is the minimized cost). This vector can be reshaped into the two parameter matrices Theta1 and Theta2.\nnn_params &lt;- optim_out[[1]]\n\n# Reshape nn_params into Theta1 and Theta2\nk &lt;- (input_layer_size + 1) * hidden_layer_size\nTheta1 &lt;- matrix(nn_params[1:k], hidden_layer_size, input_layer_size + 1)\nTheta2 &lt;- matrix(nn_params[(k + 1):length(nn_params)],\n                 output_layer_size, hidden_layer_size + 1)\n\n\nMaking predictions\nNow that we have trained the network, we can see how well it does when making predictions for images it hasn‚Äôt seen before. The predict function makes these predictions. We‚Äôll assess the network‚Äôs performance using the cross-validation set xval.\n# predict gives the predicted values of input vectors in a matrix X\n\npredict &lt;- function(Theta1, Theta2, X) {\n    \n    m &lt;- dim(X)[1]\n    \n    # (Condensed code) forward propagation\n    h1 &lt;- sigmoid( cbind(rep(1, m), X) %*% t(Theta1) )\n    h2 &lt;- sigmoid( cbind(rep(1, m), h1) %*% t(Theta2) )\n    \n    # Set the position of the maximum output value to be the prediction\n    max.col( (h2) )\n}\npreds &lt;- predict(Theta1, Theta2, xval)\nWe can now compare the predicted labels to the actual labels of each image, and calculate the percentage accuracy of classification the network has achieved.\nsprintf(\"Accuracy: %.1f%%\", sum(preds == yval) * 100 / dim(xval)[1])\n## [1] \"Accuracy: 93.1%\"\nThis initial runthrough achieves an accuracy of above 93%, which is already reasonably impressive!\n\n\nVisualizing the hidden layer\nIt would be interesting to see exactly what features the neurons in the hidden layer have learned to look for in the input images. We can visualize these features by passing Theta1, without the first column of bias parameters, to visualizeMulti:\nvisualizeMulti(Theta1[, -c(1)])\n## [1] \"Converting data type...\"\n## [1] \"Plotting images...\"\n\n## [1] \"Visualization complete!\"\nIt is possible to make out some shapes and curves, but it is interesting to see the difference between the features the network learns and the features humans might use to describe a digit.\n\n\nMistakes\nIt is also worth seeing some of the images the network didn‚Äôt manage to classify correctly.\nwrong &lt;- which(preds != yval)\ns &lt;- sample(wrong, 25)\n\nvisualizeMulti(xval[s, ])\n## [1] \"Converting data type...\"\n## [1] \"Plotting images...\"\n\n## [1] \"Visualization complete!\"\nIn some cases it is difficult for a human to say with absolute certainty which digit is represented, so it is understandable that the network makes some errors. However, it may be possible to improve the network‚Äôs performance through adjustments to the hyper-parameters set earlier.\nIn some cases it is difficult for a human to say with absolute certainty which digit is represented, so it is understandable that the network makes some errors. However, it may be possible to improve the network‚Äôs performance through adjustments to the hyper-parameters set earlier.\n\n\nAdjusting lambda\nWe can try adjusting each of the non-fixed hyper-parameters in turn to see if we can improve the network‚Äôs performance.\nFirst we can train the network with several different values of lambda and plot the accuracy of the resulting network against these values. optLambda performs this task.\n# optLambda can be used to help visualize which value of lambda will give the\n# best performance for the network\n\noptLambda &lt;- function(lambdavec) {\n    \n    accuracy &lt;- vector(mode = \"numeric\", length = length(lambdavec))\n    \n    for(i in 1:length(lambdavec)) { \n        \n        # Train the network (using a smaller subset of xtrain to reduce\n        # computation time)\n        optim_out &lt;- optim(init_params,\n                           function(p) computeCost(p, input_layer_size,\n                                                   hidden_layer_size,\n                                                   output_layer_size,\n                                                   xtrain[1:5000, ],\n                                                   ytrain[1:5000], lambdavec[i]),\n                           function(p) computeGrad(p, input_layer_size,\n                                                   hidden_layer_size,\n                                                   output_layer_size,\n                                                   xtrain[1:5000, ],\n                                                   ytrain[1:5000], lambdavec[i]),\n                           method = \"L-BFGS-B\", control = list(maxit = 50))\n        \n        nn_params &lt;- optim_out[[1]]\n        \n        # Reshape nn_params into Theta1 and Theta2\n        k &lt;- (input_layer_size + 1) * hidden_layer_size\n        Theta1 &lt;- matrix(nn_params[1:k], hidden_layer_size, input_layer_size + 1)\n        Theta2 &lt;- matrix(nn_params[(k + 1):length(nn_params)],\n                         output_layer_size, hidden_layer_size + 1)\n        \n        preds &lt;- predict(Theta1, Theta2, xval)\n        \n        # Calculate prediction accuracy and store\n        accuracy[i] &lt;- sum(preds == yval) * 100 / dim(xval)[1]\n    }\n    \n    plot(lambdavec, accuracy, pch = 4)\n}\noptLambda( c(0.1, 0.5, 1, 2, 3, 4, 5, 6, 7) )\n\nIt seems that our initial value of 1 for lambda is not the best value we could be using; there is no clear trend in the results, but it seems a larger value (perhaps around 4) will give us a greater accuracy. We will adjust lambda with this improvement shortly.\n\n\nAdding more neurons\nBy increasing the number of neurons in the hidden layer, we increase the number of features the network can look for in an image, although at the cost of increased computation time. We can see the effect of adding more neurons to the hidden layer using optNeurons.\n# optNeurons can be used to see the effect of adding more neurons to the hidden\n# layer\n\noptNeurons &lt;- function(neuronvec) {\n    \n    accuracy &lt;- vector(mode = \"numeric\", length = length(neuronvec))\n    compTime &lt;- vector(mode = \"numeric\", length = length(neuronvec))\n    \n    for(i in 1:length(neuronvec)) { \n        \n        # Initialize parameters\n        init_params &lt;- randomInitParams(input_layer_size, neuronvec[i],\n                                        output_layer_size)\n        \n        # Start the clock\n        ptime_start &lt;- proc.time()\n        \n        # Train the network (using a smaller subset of xtrain to reduce\n        # computation time)\n        optim_out &lt;- optim(init_params,\n                           function(p) computeCost(p, input_layer_size,\n                                                   neuronvec[i],\n                                                   output_layer_size,\n                                                   xtrain[1:5000, ],\n                                                   ytrain[1:5000], lambda),\n                           function(p) computeGrad(p, input_layer_size,\n                                                   neuronvec[i],\n                                                   output_layer_size,\n                                                   xtrain[1:5000, ],\n                                                   ytrain[1:5000], lambda),\n                           method = \"L-BFGS-B\", control = list(maxit = 50))\n        \n        # Stop the clock\n        ptime_stop &lt;- proc.time()\n        \n        nn_params &lt;- optim_out[[1]]\n        \n        # Reshape nn_params into Theta1 and Theta2\n        k &lt;- (input_layer_size + 1) * neuronvec[i]\n        Theta1 &lt;- matrix(nn_params[1:k], neuronvec[i], input_layer_size + 1)\n        Theta2 &lt;- matrix(nn_params[(k + 1):length(nn_params)],\n                         output_layer_size, neuronvec[i] + 1)\n        \n        preds &lt;- predict(Theta1, Theta2, xval)\n        \n        # Calculate prediction accuracy and store\n        accuracy[i] &lt;- sum(preds == yval) * 100 / dim(xval)[1]\n        \n        # Store computation time\n        compTime[i] &lt;- (ptime_stop - ptime_start)[3]\n    }\n    \n    plot(neuronvec, accuracy, pch = 4, ylim = c(82, 92))\n    text(neuronvec, accuracy, labels = compTime, cex = 0.8, pos = 1)\n}\noptNeurons( c(15, 25, 35, 45, 55, 65) )\n\nIt can be seen that adding extra neurons consistently increases the accuracy of the network - however, the rate of improvement slows after about 35 neurons have been added, and the computation time increases significantly at each stage due to the ever-larger matrices involved. Therefore I shall for now continue to use a 25-unit hidden layer, since the training process otherwise becomes rather tedious, but for the final training run I shall use a 50-unit layer since this will give a slight increase to the accuracy score.\n\n\nOne more attempt at improvement\nAt one point or another I became curious as to whether the network would better learn to recognize features if the images were first converted into pure black-and-white images, where each pixel in the image is either ‚Äòon‚Äô or ‚Äòoff‚Äô. visualizeBW performs this transformation.\n# visualizeBW creates a black-and-white only visual representation of an image\n    \nvisualizeBW &lt;- function(imgvec, tolerance = 0.5) {\n    \n    imgvec[imgvec &lt; tolerance] &lt;- 0\n    imgvec[imgvec &gt;= tolerance] &lt;- 1\n    \n    visualize(imgvec)\n}\nThe following plots show the result of calling visualizeBW with different tolerance levels on an example image. The leftmost plot is the result of using visualize; the centre plot is the result of visualizeBW with tolerance = 0.5 (the default); and the rightmost plot has tolerance = 0.8.\nimg &lt;- xtrain[10, ]\n\np1 &lt;- visualize(img)\np2 &lt;- visualizeBW(img)\np3 &lt;- visualizeBW(img, tolerance = 0.8)\nlibrary(grid)\ngrid.newpage()\npushViewport(viewport(layout = grid.layout(1, 3)))\nprint(p1[[3]], vp = viewport(layout.pos.row = 1, layout.pos.col = 1))\nprint(p2[[3]], vp = viewport(layout.pos.row = 1, layout.pos.col = 2))\nprint(p3[[3]], vp = viewport(layout.pos.row = 1, layout.pos.col = 3))\n\nWe can try training the network on black-and-white versions of the xtrain images:\ntolerance &lt;- 0.5\n\nxtrainBW &lt;- xtrain\nxvalBW &lt;- xval\nxtrainBW[xtrainBW &lt; tolerance] &lt;- 0\nxtrainBW[xtrainBW &gt;= tolerance] &lt;- 1\nxvalBW[xvalBW &lt; tolerance] &lt;- 0\nxvalBW[xvalBW &gt;= tolerance] &lt;- 1\n\noptim_out &lt;- optim(init_params,\n                   function(p) computeCost(p, input_layer_size,\n                                           hidden_layer_size,\n                                           output_layer_size,\n                                           xtrainBW, ytrain, lambda),\n                   function(p) computeGrad(p, input_layer_size,\n                                           hidden_layer_size,\n                                           output_layer_size,\n                                           xtrainBW, ytrain, lambda),\n                   method = \"L-BFGS-B\", control = list(maxit = 50))\n\nnn_params &lt;- optim_out[[1]]\n\n# Reshape nn_params into Theta1 and Theta2\nk &lt;- (input_layer_size + 1) * hidden_layer_size\nTheta1 &lt;- matrix(nn_params[1:k], hidden_layer_size, input_layer_size + 1)\nTheta2 &lt;- matrix(nn_params[(k + 1):length(nn_params)],\n                 output_layer_size, hidden_layer_size + 1)\n\npreds &lt;- predict(Theta1, Theta2, xvalBW)\n\nsprintf(\"Accuracy: %.1f%%\", sum(preds == yval) * 100 / dim(xvalBW)[1])\n## [1] \"Accuracy: 92.2%\"\nIt turns out that the network actually performs worse on these black-and-white images (even using different tolerance values) than on the unaltered images. This is perhaps unsurprising: by limiting each pixel to being only ‚Äòon‚Äô or ‚Äòoff‚Äô, we lose a lot of the information contained in the image, and so reduce the amount of information available for the network to train on. From now on, then, I shall only use the original unaltered images for training.\n\n\nUpdating hyper-parameters\nHaving seen the effects of changing each hyper-parameter earlier, we can now update the hyper-parameters to obtain the best performance from the network.\nlambda &lt;- 2\nhidden_layer_size &lt;- 50\n\n\nRe-evaluating performance\nThe network can now be re-trained with the updated hyper-parameters, and its performance evaluated on xtest - which has not been used up to this point to prevent it from influencing the training process in any way (this is another measure to prevent overfitting and hence an overly-optimistic accuracy estimate).\noptim_out &lt;- optim(init_params,\n                   function(p) computeCost(p, input_layer_size,\n                                           hidden_layer_size,\n                                           output_layer_size,\n                                           xtrain, ytrain, lambda),\n                   function(p) computeGrad(p, input_layer_size,\n                                           hidden_layer_size,\n                                           output_layer_size,\n                                           xtrain, ytrain, lambda),\n                   method = \"L-BFGS-B\", control = list(maxit = 50))\n\nnn_params &lt;- optim_out[[1]]\n\n# Reshape nn_params into Theta1 and Theta2\nk &lt;- (input_layer_size + 1) * hidden_layer_size\nTheta1 &lt;- matrix(nn_params[1:k], hidden_layer_size, input_layer_size + 1)\nTheta2 &lt;- matrix(nn_params[(k + 1):length(nn_params)],\n                 output_layer_size, hidden_layer_size + 1)\n\npreds &lt;- predict(Theta1, Theta2, xtest)\n\nsprintf(\"Accuracy: %.1f%%\", sum(preds == ytest) * 100 / dim(xtest)[1])\n## [1] \"Accuracy: 95.0%\"\nThis is significantly better than the previous accuracy, so our effort has not been in vain!\n\n\nMaking predictions for unlabelled images\nThe network is now as accurate as it reasonably can be in its classification of images, so it is time to try it out on test - the unlabelled images we loaded earlier.\n# test images are still in a dataframe, so convert them to matrix format\ntestMat &lt;- as.matrix(test)\n\n# Re-train the network one last time - this time we'll use all the available\n# training data, since we have already adjusted our hyper-parameters\nX &lt;- as.matrix( train[, -c(1)] )\nX &lt;- X / max(X)\nY &lt;- as.numeric( train[, c(1)] )\nY[Y == 0] &lt;- 10\n\n# Train the network for the last time - maxit increased for better optimization\noptim_out &lt;- optim(init_params,\n                   function(p) computeCost(p, input_layer_size,\n                                           hidden_layer_size,\n                                           output_layer_size,\n                                           X, Y, lambda),\n                   function(p) computeGrad(p, input_layer_size,\n                                           hidden_layer_size,\n                                           output_layer_size,\n                                           X, Y, lambda),\n                   method = \"L-BFGS-B\", control = list(maxit = 100))\n\nnn_params &lt;- optim_out[[1]]\n\n# Reshape nn_params into Theta1 and Theta2\nk &lt;- (input_layer_size + 1) * hidden_layer_size\nTheta1 &lt;- matrix(nn_params[1:k], hidden_layer_size, input_layer_size + 1)\nTheta2 &lt;- matrix(nn_params[(k + 1):length(nn_params)],\n                 output_layer_size, hidden_layer_size + 1)\n\n# Make predictions for testMat\npredictions &lt;- predict(Theta1, Theta2, testMat)\npredictions[predictions == 10] &lt;- 0\n\n# Reformat into dataframe\nfinal &lt;- data.frame(ImageId = c(1:dim(testMat)[1]), Label = predictions)\n\n# Write to .csv file\nwrite.csv(final, file = \"submission.csv\", row.names = FALSE)\nThe file submission.csv is the one I uploaded to the competition page on Kaggle, achieving a score of 0.94086 (i.e.¬†about 94.1% accuracy) - just about high enough to make it into the top 1000 submissions.\n\n\n\n\nProject evaluation\nAlthough not the best submission to the competiton by quite a stretch, and certainly too inaccurate to use in any important applications, I am very pleased with the outcome of this project. By modern-day standards, a single-hidden-layer fully-connected network is very primitive, and yet for a network of such simple architecture the performance is reasonable.\nOverall this has been an enjoyable project to put together. Writing the code was challenging but has reinforced my understanding of the structure and mechanical workings of the basic neural network model upon which many new designs and improvements are based. The compilation of this write-up was also a challenge, partly because I had no previous experience with R Markdown (the formatting language used to create the document) and partly because I have tried throughout to explain, more or less from first principles, exactly what process I was following and more importantly why I was following it. Certainly this has been a useful exercise for me - and hopefully it might prove useful to anyone else who happens to come across my work!"
  },
  {
    "objectID": "posts/2016-10-09-placement-year-salaries-getting-data-from-html/index.html",
    "href": "posts/2016-10-09-placement-year-salaries-getting-data-from-html/index.html",
    "title": "Placement Year Salaries - Getting data from HTML",
    "section": "",
    "text": "Context\nOn this site‚Äôs homepage I allude to the fact that there are ‚Äòother things‚Äô I am generally supposed to be doing instead of putting together these projects. One such ‚Äòother thing‚Äô is the time-devouring task of researching work placements (I am hoping to complete a year-long placement from next summer as part of my degree).\nSeveral job descriptions which have caught my attention have something in common (other than generally being data science related): they don‚Äôt list a salary. At least, not explicitly. Instead they advertise a ‚Äúcompetitive‚Äù salary or, on occasion, hint that applications to the position should include ‚Äúsalary expectations‚Äù.\nAfter seeing this for the third or fourth time, I realised I didn‚Äôt have any ‚Äúsalary expectations‚Äù. So once again abandoning the task I had planned on dedicating the weekend to, I set forth on this project.\n\n\n\nFinding the average placement year salary\n\nSource of information\nA quick Google search of ‚Äòplacement year uk‚Äô brings up RateMyPlacement as the first result. This website claims to be ‚Äúthe TripAdvisor‚Äù for placements - the following paragraph can be found on their ‚ÄòAbout‚Äô page:\n\nOur website is specifically targeted towards undergraduates, offering current university students the jobs they need to get their careers started. RateMyPlacement is the leader in this niche market, advertising more undergraduate placements, internships, vacation schemes and insights than any other website.\n\nI decided to use this website, rather than a more general job-listing site such as Indeed, as my source since it guarantees that any jobs listed will be of the sort I am looking for - specifically, placement jobs, rather than full-time or the ambiguous ‚Äòtemporary‚Äô or ‚Äòcontract‚Äô jobs listed on many other sites.\n\n\nLoading packages into R\nI have used the magrittr package‚Äôs ‚Äòpipe‚Äô operator (%&gt;%) rather extensively, since I find it greatly improves the readability of the code. It is particularly useful when performing many sequential operations on the same object, as will be seen shortly.\nThe RCurl and XML packages are used to deal with the initiall y-overwhelming HTML code.\nlibrary(magrittr)\nlibrary(RCurl)\nlibrary(XML)\n\n\n‚ÄòDetective work‚Äô\nTo begin with, we need to have a look at our target website and work out exactly where the ‚Äòinteresting‚Äô parts are located. I‚Äôm using Google Chrome, but many other web browsers will be equally suitable for carrying out the following steps.\nThis is the page that initially loads:\n\nIt is clearly a list of search results, although we currently don‚Äôt exactly what form they are in - i.e.¬†if they are laid out in an HTML table (which would simplify matters) or not (which is more likely!). We can find out using the developer console, which can be opened by pressing F12.\nThe ‚ÄòInspect element‚Äô button at the top left of the console can be used to quickly locate the section of HTML code responsible for a given feature on the page. Let‚Äôs have a look at one of the search results.\n\nEach result is within a pair of &lt;div&gt; tags (no tables in sight, unfortunately). Within each result there are several pieces of information. Only one of these is of interest to us (for this project at least): the salary. A little further inspection shows that this information is contained in one element of an unordered list, specifically - and rather confusingly - in the &lt;li&gt; element which has class=location, presumably due to some previous website redesign.\n\nThere is the option to refine the search results further, but for now we‚Äôll use the unfiltered set of results. It is worth noting that we don‚Äôt need to filter by duration since the site uses pro rata annual salary figures for all listed placements.\n\n\nExtracting salaries\nNow we know where the information we want is, we can begin the process of importing it into R and tidying it up. I will use the %&gt;% operator here, and eventually the aim is to end up with a numeric vector containing each job‚Äôs (mean) salary - so I will work with an object called salaries, even though initially it is very different to its final form.\nThe first stage is to load the raw HTML file into R. The getURL function (from the RCurl package) imports the HTML document as a single character string - which needless to say is very long (nchar(salaries) reveals it is 205604 characters long, to be precise) and not particularly readable.\nsalaries &lt;-\n        \"https://www.ratemyplacement.co.uk/search?show=jobs\" %&gt;%\n        getURL()\nsubstring(salaries, 100001, 101000)\n## [1] \"           &lt;h2 class=\\\"Job-titleHeader\\\"&gt;Engineering Coordinator Placement&lt;/h2&gt;\\n            &lt;/a&gt;\\n            &lt;p class=\\\"Search-industry\\\"&gt;&lt;i class=\\\"icon-company\\\"&gt;&lt;/i&gt;\\n                Volvo Group UK&lt;/p&gt;\\n            &lt;ul class=\\\"Search-labels\\\"&gt;\\n                &lt;li class=\\\"deadline\\\"&gt;&lt;span\\n                            class=\\\"icon-calendar\\\"&gt;&lt;/span&gt; 31st Oct 2016                &lt;/li&gt;\\n                &lt;li class=\\\"location\\\"&gt;&lt;span class=\\\"icon-salary\\\"&gt;&lt;/span&gt;\\n                                            Competitive\\n                                    &lt;/li&gt;\\n                &lt;li class=\\\"duration\\\"&gt;&lt;span class=\\\"icon-company\\\"&gt;&lt;/span&gt; Placement Year (10 Months+)&lt;/li&gt;\\n                &lt;li class=\\\"company\\\"&gt;&lt;span class=\\\"icon-location\\\"&gt;&lt;/span&gt; West Midlands&lt;/li&gt;\\n            &lt;/ul&gt;\\n        &lt;/div&gt;\\n        &lt;div class=\\\"icon-cta\\\"&gt;\\n\\n                        \\n                &lt;a href=\\\"#\\\" class=\\\"circle-cta my-wishlist tracker login\\\"\\n                   data-message=\\\"You need to be logged in to add a job to your tracker\\\"\\n           \"\nLuckily, the XML package can be used to manipulate this awkward mass of text into something much easier to work with in R. First, htmlParse is used to parse the HTML code so that each set of tags is represented as a ‚Äònode‚Äô. The result of this can then be used in xpathApply with two other arguments.\nThe first is an XPath query //li[@class='location'], where:\n\n//li finds all ‚Äòlist element‚Äô (&lt;li&gt;) nodes\n[@class='location'] finds the subset of these nodes which have a class attribute of location (recall that this is where the salaries are stored)\n\nThe second is what we want to return from each of these nodes, which in this case is its xmlValue. The values are returned by xpathApply in list form.\nsalaries &lt;-\n    salaries %&gt;%\n    htmlParse() %&gt;%\n    xpathApply(\"//li[@class='location']\", xmlValue)\n## [[1]]\n## [1] \"\\n                                            Competitive\\n                                    \"\n## \n## [[2]]\n## [1] \"\\n                                            Competitive\\n                                    \"\n## \n## [[3]]\n## [1] \"\\n                                            ¬£18,000 - ¬£19,999                                    \"\nWe now have the data we want - although each salary is still in an untidy character vector. To tidy each element we can first remove any of the following from the string:\n\n\\n\n[space]\n¬£\n,\n\nEach salary is either not given (listed as ‚ÄúCompetitive‚Äù) or is given as a range. We can split each character vector at the ‚Äò-‚Äô symbol (if it is present) to obtain separate character vectors containing the lower and upper values of the range.\nsalaries &lt;-\n    salaries %&gt;%\n    lapply(gsub, pattern = \"\\\\n| |¬£|,\", replacement = \"\") %&gt;%\n    lapply(strsplit, split = \"-\")\nsalaries[1:3]\n## [[1]]\n## [[1]][[1]]\n## [1] \"Competitive\"\n## \n## \n## [[2]]\n## [[2]][[1]]\n## [1] \"Competitive\"\n## \n## \n## [[3]]\n## [[3]][[1]]\n## [1] \"18000\" \"19999\"\nInitially I had planned on calculating the midway point for each range of salaries. However there is an easier option, given that the (eventual) vector of salaries will be averaged anyway. Averaging each pair of range values, and then averaging all of the resulting values, is equivalent to simply averaging all the range values:\n\\[\\frac{\\frac{(a^{(1)}_1 + a^{(1)}_2)}{2} + \\frac{(a^{(2)}_1 + a^{(2)}_2)}{2} + \\ldots + \\frac{(a^{(n)}_1 + a^{(n)}_2)}{2}}{n} = \\frac{(a^{(1)}_1 + a^{(1)}_2) + \\ldots + (a^{(n)}_1 + a^{(n)}_2)}{2n}\\]\n\nSo we can simply unlist each pair of range values and then unlist the whole salary list to obtain a character vector. Using as.numeric then changes this to a numeric vector, where all the ‚ÄúCompetitive‚Äù (i.e.¬†unlisted) salaries are coerced to NA. The mean of this vector (ignoring NAs) is the average salary for all the positions listed on the webpage.\nsalaries &lt;-\n    salaries %&gt;%\n        lapply(unlist) %&gt;%\n        unlist() %&gt;%\n        as.numeric()\n## Warning in function_list[[k]](value): NAs introduced by coercion\nsalaries[1:10]\n##  [1]    NA    NA 18000 19999    NA    NA    NA    NA 16000 17999\nmean(salaries, na.rm = TRUE)\n## [1] 17307.19\n\n\nMultiple pages: creating URLs\nNow that we can get the mean salary for one page, we can consider the fact that there are actually multiple pages of positions - and with positions being added and removed continuously, the number of pages we need to get data from could change.\nThe first task is to find how many pages of results we need to look at. As before, we start by importing and parsing the first page of results, but this time we will use a different XPath query.\nFirst we must have another look at the webpage - but this time we are interested in the ‚Äòpages‚Äô bar at the bottom of the list of results:\n\nWe want the page number that appears in the button immediately to the left of the ‚ÄòNext page‚Äô button. We can construct the XPath query //li[@class='pagination-last']/preceding::li[1]:\n\n//li finds all ‚Äòlist element‚Äô nodes on the page\n[@class='pagination-last'] finds the subset of these nodes with class attribute pagination-last (there is only one such node, which is the ‚ÄòNext page‚Äô button)\n/preceding::li[1] finds, working backwards from the current node, the 1st ‚Äòlist element‚Äô node\n\nThe xmlValue of this node is the number appearing in the last numbered button - i.e.¬†the number of results pages. It is currently in character vector form and inside a list (courtesy of xpathApply), but it can be extracted easily enough.\nnum_pages &lt;-\n    \"https://www.ratemyplacement.co.uk/search?show=jobs\" %&gt;%\n    getURL() %&gt;%\n    htmlParse() %&gt;%\n    xpathApply(\"//li[@class='pagination-last']/preceding::li[1]\", xmlValue) %&gt;%\n    unlist() %&gt;%\n    as.numeric()\nnum_pages\n## [1] 10\nTo manually navigate to a different results page we simply need to add page=[page number]& to the search string in the page address; so we can make a list of all the results pages as follows.\nurls &lt;- function(x) paste0(\"https://www.ratemyplacement.co.uk/search?page=\",\n                           x, \"&show=jobs\")\n\npages &lt;- lapply(1:num_pages, urls)\n&gt;## [[1]]\n## [1] \"https://www.ratemyplacement.co.uk/search?page=1&amp;show=jobs\"\n## \n## [[2]]\n## [1] \"https://www.ratemyplacement.co.uk/search?page=2&amp;show=jobs\"\n## \n## [[3]]\n## [1] \"https://www.ratemyplacement.co.uk/search?page=3&amp;show=jobs\n\n\nFinding the mean salary\nWe can now run the salary-extracting code on each page in turn. We‚Äôll store the full list salaries from each page for later analysis.\nNote: There are three sponsored results which appear at the top of each results page - these are the same on all pages, so are not counted for any pages except the first (i.e.¬†for all but the first page, the first 4 elements of salaries are removed, corresponding to two ‚ÄúCompetitive‚Äù listings and one numerical listing - the make-up of the sponsored listings at the time of writing).\npage_salaries &lt;- vector(mode = \"list\", length = 0)\npage_avg_salary &lt;- vector(mode = \"numeric\", length = num_pages)\n\nfor (i in 1:num_pages) {\n    \n    salaries &lt;-\n        pages[[i]] %&gt;%\n        getURL() %&gt;%\n        htmlParse() %&gt;%\n        xpathApply(\"//li[@class='location']\", xmlValue) %&gt;%\n        lapply(gsub, pattern = \"\\\\n| |¬£|,\", replacement = \"\")\n    # Pause to add list of salaries to page_salaries\n    page_salaries &lt;- append(page_salaries, salaries)\n    # Resume pipe\n    salaries &lt;-\n        salaries %&gt;%\n        lapply(strsplit, split = \"-\") %&gt;%\n        lapply(unlist) %&gt;%\n        unlist() %&gt;%\n        # Ignore 'Coercion to NA' warnings since this is expected\n        as.numeric() %&gt;%\n        {if (i != 1)  tail(., -4) else .}\n\n    page_avg_salary[i] &lt;- mean(salaries, na.rm = TRUE)\n}\nSo now we have a list of all salaries, and vector of the average salaries of all the positions on each results page. If we calculate the mean of this vector then we will have the mean salary across all the positions.\nmean(page_avg_salary)\n## [1] 17878.68\nWe can also visualize the proportions of different salaries being offered:\npage_salaries &lt;-\n    page_salaries %&gt;%\n    unlist() %&gt;%\n    as.factor()\n\np &lt;- plot(page_salaries, main = \"Rates of pay\", ylim = c(0, 400),\n         xlab = \"Salary range\", ylab = \"No. of positions\", cex.names = 0.5)\n\ntext(p, y = summary(page_salaries), label = summary(page_salaries), pos = 3,\n     cex = 0.7)\n\nThis reveals how nearly twice as many positions are advertised without an explicit salary as those advertised with one. (It also reveals that one of the salaries coerced to NA earlier was in fact ‚ÄúUnpaid‚Äù, rather than ‚ÄúCompetitive‚Äù. Although it shows I was a little careless earlier when I stated that all non-numeric elements were ‚ÄúCompetitive‚Äù, it doesn‚Äôt have any effect on the calculations.)\n\n\nLondon-specific salaries\nAlthough I do have a general dislike for London, I am slowly coming to terms with the fact that many companies with data science positions are young start-ups, and the vast majority of them are located in the capital. As such, and knowing that London is more expensive than other areas of the country, I thought I would re-run the code on the placements listed with locations ‚ÄòCentral London and City‚Äô and ‚ÄòGreater London‚Äô only. This extra filtering is done by adding &location=CTY,GTL to the end of the HTTP address. The results are as follows:\nmean(page_avg_salary_L)\n## [1] 19015.6\nThis is, as expected, higher than the nationwide figure.\nA quick look at the relevant plot reveals some more information:\n\nNotice how the highest-paid positions are all in London (between the nationwide and London plots, there is no difference in the bars for the two highest salary bands). The significant decrease is in the number of lower-than-average-salary positions.\n\n\n\n\nProject evaluation\nIt turns out that the average annual placement position salary is just under ¬£18,000, increasing to slightly over ¬£19,000 for London-based placements.\nThis was my first attempt at a project using non-tabluar web-based data. Moreover, I had no previous experience with the HTML-manipulating packages in R, and as per usual a large amount of reading and experimentation was necessary before I managed to achieve what I was aiming for. Perhaps it is a little ambitious to call this a ‚Äòweb scraping‚Äô project, but I believe it has given me some of the fundamental knowledge needed to put together more complicated projects involving web data in the future. An added bonus was my discovery of the XPath language, which might have come in handy for one of my previous projects.\nAll in all, I shall consider this a weekend well spent, although it has no doubt come at the cost of a few late-night maths sessions later in the week."
  },
  {
    "objectID": "posts/2017-01-21-snippets-in-rstudio/index.html",
    "href": "posts/2017-01-21-snippets-in-rstudio/index.html",
    "title": "Snippets in RStudio",
    "section": "",
    "text": "I know, I know, this is probably old news to must regular users of RStudio. But these things are so useful - how had I not heard about them before?\n\n\nIn short, ‚Äúcode snippets‚Äù are little abbreviations that you can use when writing your code. You type a few letters and then hit TAB, in the same way you do to autocomplete functions; but a snippet can be used to insert a whole code template, for example, a for loop or switch statement.\n\n\nAnd what‚Äôs more, you can add your own snippets! Within the Code tab of the global options menu, there‚Äôs a little ‚ÄúEdit snippets‚Äù button:\n\n\n\nAnd then you can see the built-in snippets (in a whole bunch of languages), and add your own. This post from RStudio explains the syntax (and, indeed, explains the whole concept of snippets better than I have done here!).\n\n\nIn my case, I wanted a quick way to add section dividers to my R scripts: so I‚Äôve defined three new snippets for ‚Äúsingle-‚Äù and ‚Äúdouble-divider‚Äù and for a ‚Äúheading‚Äù divider.\n\n\n\nWhich work like so:\n\n\n\nType two letters, hit TAB. Much quicker than holding down the equals key!"
  },
  {
    "objectID": "posts/2016-10-21-derivation-of-backpropagation-plus-a-network-diagram/index.html",
    "href": "posts/2016-10-21-derivation-of-backpropagation-plus-a-network-diagram/index.html",
    "title": "Derivation of backpropagation, plus a network diagram",
    "section": "",
    "text": "By creating a digit-recognizing neural network - and then writing up the process I followed in order to do so - I was aiming not just to create a working net, but to solidify my understanding of the concepts and calculations involved. Generally speaking, I would say I achieved this.\n\n\nHowever, there was one particular step which I found a little mysterious: backpropagation. To quote myself:\n\n\n\nThe aim of backpropagation is to assess how much of an effect each individual parameter has on the cost function. This is done by calculating the partial derivative of the cost function with respect to each parameter in turn, by feeding back the error from each unit in each layer.\n\n\n\nAlthough I understood the overall principle, at that point I was keen to carry on coding the rest of the network, and I didn‚Äôt have the patience or inclination to dive into the underlying maths. In my code I simply used formulae from the internet (cross-checking multiple sources, of course, to establish the correctness of these formulae; during my A-levels, the term we often used for such things was ‚Äòproof by popular consensus‚Äô).\n\n\nI was recently flicking through some notes and doodles I had made over the summer, and came across a page full of partial derivatives, or rather, attempts at them; and I decided it was about time I looked a little deeper into backpropagation.\n\n\nNote on notation: \n\n\n\n(J()) is the cost function (J) with respect to all parameters () in the net\n\n\n(^{(k-1)}) is the set of parameters used to ‚Äòtransition‚Äô from layer ((k-1)) to layer (k)\n\n\n(z^{(k)}) is the ‚Äòraw‚Äô output from layer (k)\n\n\n(a^{(k)}) is the ‚Äòactivated‚Äô version of (z^{(k)})\n\n\n\nSee the diagram at the bottom of the page for a visual representation of how these all relate to each other.\n\n\n\nMission accomplished!\n\n\n\nWhile I had my tablet out, I also thought it might be useful to create a visual representation of the single-hidden-layer network from my earlier project, in order to help me see how the various vectors and matrices fit together:"
  },
  {
    "objectID": "posts/2018-05-16-rstudio-on-google-cloud-dataproc/index.html",
    "href": "posts/2018-05-16-rstudio-on-google-cloud-dataproc/index.html",
    "title": "Using RStudio on a Google Cloud Dataproc cluster",
    "section": "",
    "text": "A few months ago, I was working on a really cool project with Google.\nWithout going into too much detail, we were extracting structured information from unstructured text and image data, and then using that new structured information to build a recommender system and some other fun ML stuff.\nAnyway, a fairly major aspect of this project was the availability of Google Cloud Platform resources. Given that I was expecting to do a lot of heavy image processing, I thought it would be useful to get a Dataproc cluster set up for a bit of extra oomph.\nDataproc uses Spark to manage the underlying cluster, and by default the cluster machines are set up with Python (and PySpark). However, I had already done a bunch of work in R on my own computer, and I was hoping to get going quickly on the cloud cluster without having to translate everything into Python first. Therefore I resolutely decided to get RStudio and sparklyr up and running on my shiny new Dataproc cluster.\nUnfortunately‚Ä¶ it seemed like nobody had actually done this before!\nAfter a lot of digging, bodging and cussing, I did eventually succeed - and before I was able to feel too bad about spending so long on it (longer than it would have taken to rewrite my work in Python!), our colleague at Google said something along the lines of:\n\n‚ÄúHey, this is pretty cool. Do you think we could write this up properly?‚Äù\n\nSure, why not! I put together some brief comments and dodgy screenshots; shortly afterwards the project finished, and I promptly forgot any of this had ever happened.\nThen a couple of days ago I received an email. The wonderful people at Google have worked their magic and turned my just-functional setup process into a full Cloud Solutions writeup! I am indebted to them for doing so, and you can read it here:\n\nRunning Rstudio Server on a Cloud Dataproc Cluster"
  },
  {
    "objectID": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html",
    "href": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html",
    "title": "Markov and Churchill: an exploration of predictive text",
    "section": "",
    "text": "Let me introduce the ‚Äúpredictive text game‚Äù. Play along with me if you like!\nOpen up a new message on your phone, type in a random word to get started (I‚Äôm choosing ‚Äúthe‚Äù), and then just keep hitting the top ‚Äúsuggested word‚Äù that appears above your keyboard. Here‚Äôs my result:\n\nThe following user says thank you for your time and consideration and I will be in the evening of the time to get the same as the first time I am not sure if you have any questions or concerns please visit the lockouts page and the other day and I will ve in the evening of the time to‚Ä¶\n\nI think it‚Äôs pretty cool that your phone can make a good guess at what you‚Äôre going to write next. I mean, my phone‚Äôs magnum opus is pretty boring and repetitive - hopefully you‚Äôll get something a bit more interesting! But I‚Äôm just impressed that it can do this at all."
  },
  {
    "objectID": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#introduction",
    "href": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#introduction",
    "title": "Markov and Churchill: an exploration of predictive text",
    "section": "",
    "text": "Let me introduce the ‚Äúpredictive text game‚Äù. Play along with me if you like!\nOpen up a new message on your phone, type in a random word to get started (I‚Äôm choosing ‚Äúthe‚Äù), and then just keep hitting the top ‚Äúsuggested word‚Äù that appears above your keyboard. Here‚Äôs my result:\n\nThe following user says thank you for your time and consideration and I will be in the evening of the time to get the same as the first time I am not sure if you have any questions or concerns please visit the lockouts page and the other day and I will ve in the evening of the time to‚Ä¶\n\nI think it‚Äôs pretty cool that your phone can make a good guess at what you‚Äôre going to write next. I mean, my phone‚Äôs magnum opus is pretty boring and repetitive - hopefully you‚Äôll get something a bit more interesting! But I‚Äôm just impressed that it can do this at all."
  },
  {
    "objectID": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#memoryless",
    "href": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#memoryless",
    "title": "Markov and Churchill: an exploration of predictive text",
    "section": "Memoryless",
    "text": "Memoryless\nImagine you wake up in a massive field, and all you have with you is a single coin. You have no idea how you got there (the last thing you remember is your mate ordering an obscene number of J√§gerbombs) but you don‚Äôt care because it seems to be the perfect place for you to demonstrate what a random walk is.\nYou stand up, wait a moment for the pain behind your eyes to subside, take a step forwards and toss the coin. Tails. You turn right, take a step forwards, and toss the coin again. Heads, You turn left, take a step forwards, and toss the coin again‚Ä¶\n\nThis is an example of a memoryless stochastic process - a stochastic process because your coin-toss results form a series of observations of a random variable, and memoryless because each toss is independent of all the tosses that came before it. ‚ÄúMemoryless stochastic process‚Äù is a bit of a mouthful though, so we call them Markov chains (after the mathematician Andrey Markov, who studied them in detail in the early 20th Century)."
  },
  {
    "objectID": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#markov-chains",
    "href": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#markov-chains",
    "title": "Markov and Churchill: an exploration of predictive text",
    "section": "Markov chains",
    "text": "Markov chains\nFormally, if it is given that a Markov system is currently in a certain state, then the next state of the system is independent of all previous states. We sometimes talk about ‚Äúpast‚Äù and ‚Äúfuture‚Äù states, because often the series of random variables forming the Markov chain are indexed by timesteps - for example, if you took one step each second while going for your random walk, you would naturally index the first coin toss as ‚Äú1‚Äù, the second as ‚Äú2‚Äù, and so on. You‚Äôd also need to be seriously good at coin-tossing."
  },
  {
    "objectID": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#predictive-text",
    "href": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#predictive-text",
    "title": "Markov and Churchill: an exploration of predictive text",
    "section": "Predictive text",
    "text": "Predictive text\nSo, how do Markov chains fit in with predictive text?\nWell, in the simplest possible case, we set up the system by making a big dictionary which links every word to all the words which might come after it.\n\nThe current ‚Äústate‚Äù of the system is the last word that was typed. We get to the next state by picking one of the associated words at random.\n\nActually, because words usually do depend on the words that have come before them, predictive text uses the previous \\[n\\] words (usually 2ish) as the current state."
  },
  {
    "objectID": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#lets-do-some-code-already",
    "href": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#lets-do-some-code-already",
    "title": "Markov and Churchill: an exploration of predictive text",
    "section": "Let‚Äôs do some code already",
    "text": "Let‚Äôs do some code already\nOkay okay. We‚Äôre going to use Python to build a ‚ÄúMarkovGenerator‚Äù class: a MarkovGenerator object will take a bunch of text as input and set up the whole predictive-text-style Markov system we‚Äôve just been talking about.\nFirst we need to build our ‚Äúcache‚Äù of states and potential next-states. To do that, first we‚Äôll grab all the groups of \\[n\\] consecutive words in our text. The first \\[n-1\\] of these words are the ‚Äúkey‚Äù, and the last word is the ‚Äúvalue‚Äù. For example, let‚Äôs take the sentence:\nRain down rain down come on rain down on me\n \nChoosing \\[n=3\\], this would give us\nrain down rain\ndown rain down\nrain down come\ndown come on\ncome on rain\non rain down\nrain down on\ndown on me\nSo taking the first 2 words as a key, and the third word as a possible value, we get\nrain down  =&gt;  rain, come, on\ndown rain  =&gt;  down\ndown come  =&gt;  on\ncome on    =&gt;  rain\non rain    =&gt;  down\ndown on    =&gt;  me\nThis is what tuples() and build_cache() will do in the class we‚Äôre building.\nThen once we have our MarkovGenerator object, we‚Äôll want to use it to generate some text:\nStart with:   on rain  =&gt;  down                             (only possible choice)\n                      rain down  =&gt;  come                   (chosen at random from [rain, come, on])\n                                down come  =&gt; on            (only possible choice)\n                                         come on  =&gt;  rain  (only possible choice)\n                   \nResult:       on rain down come on rain\nSo, let‚Äôs make the class.\n# We're going to need to make some random choices later\nimport random\n\nclass MarkovGenerator(object):\n    \n    # When we create a \"MarkovGenerator\" object, these things happen:\n    def __init__(self, text, tuple_size=3):\n        \n        # Take the input string, and get individual words by splitting it up\n        self.text = text\n        self.words = self.text.split()\n        \n        # How many words are taken for the current state? (3 by default)\n        self.tuple_size = tuple_size\n        \n        # Create an empty dictionary, then fill it using build_cache (see below)\n        self.cache = {}\n        self.build_cache()\n        \n        \n    # Get all collections of n words\n    def tuples(self):\n        \n        for i in range(self.tuple_size-1, len(self.words)):\n            \n            # https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do-in-python\n            yield (self.words[i-self.tuple_size+k] for k in range(1, self.tuple_size+1))\n            \n            \n    # Build the cache dictionary        \n    def build_cache(self):\n\n        for tup in self.tuples():\n            \n            # The first n-1 words, as a Python tuple, are the key\n            tup = list(tup)\n            key = tuple(w for w in tup[:-1])\n            \n            if key in self.cache:\n                \n                # If the key's already there, add the last word of\n                # the tuple to the key's list of values\n                self.cache[key].append(tup[-1])\n                \n            else:\n                # Otherwise add a new entry to the cache\n                self.cache[key] = [tup[-1]]\n                \n                \n    # Generate an output text string            \n    def generate_text(self, length):\n        \n        # Choose a random set of n-1 consecutive words to start\n        seed = random.randint(0, len(self.words)-self.tuple_size)\n        current = [self.words[seed+k] for k in range(0, self.tuple_size)]\n        \n        # Set up a list for our generated words\n        gen_words = []\n        \n        # length is how many words we want in the output\n        for i in range(length):\n            \n            # Pop the first word off the current tuple, and store it\n            gen_words.append(current.pop(0))\n            \n            # Use the rest of the tuple as the key to choose the next\n            # word (randomly!), and stick that word onto the end of the tuple\n            current.append(random.choice(self.cache[tuple(current)]))\n        \n        # Stick all the generated words together and return!\n        return \" \".join(gen_words)"
  },
  {
    "objectID": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#the-greatest-briton",
    "href": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#the-greatest-briton",
    "title": "Markov and Churchill: an exploration of predictive text",
    "section": "The Greatest Briton",
    "text": "The Greatest Briton\nWe‚Äôve got our generator, but before it can generate anything we need to give it something to read!\nHow about some material from the greatest Brit in history, as voted for by the Great British public in a BBC television poll in 2002?\n\nHmm.\nStart with:   oh yes  =&gt;  oh                         (only possible choice)\n                      yes oh  =&gt;  yes                (only possible choice)\n                               oh yes  =&gt;  oh ...    (only possible choice)\n\nResult:       oh yes oh yes oh yes oh yes oh yes ...\nNo.¬†Let‚Äôs not do that. Let‚Äôs stick to the original plan."
  },
  {
    "objectID": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#soup-and-cigars",
    "href": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#soup-and-cigars",
    "title": "Markov and Churchill: an exploration of predictive text",
    "section": "Soup and cigars",
    "text": "Soup and cigars\nChurchill‚Äôs parliamentary speeches are available online on the website of The Churchill Society. I‚Äôm going to use the beautifulsoup4 Python module to do some web scraping.\nfrom bs4 import BeautifulSoup\nimport requests\nIn order to get the speeches we‚Äôll need to figure out the layout of each page, so let‚Äôs just look at one speech for now.\nlocusts = \"http://www.churchill-society-london.org.uk/Locusts.html\"\nresponse = requests.get(locusts)\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n# Let's have a look...\nsoup\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;The Churchill Society London. Churchill's Speeches.&lt;/title&gt;\n&lt;x-sas-window bottom=\"768\" left=\"57\" right=\"868\" top=\"138\"&gt;\n&lt;meta content=\"text/html; \ncharset=utf-8\" http-equiv=\"Content-Type\"/&gt;\n&lt;/x-sas-window&gt;&lt;/head&gt;\n\n[... loads more, I've truncated!]\n\n&lt;h4&gt;&lt;center&gt;&lt;font face=\"Times\" size=\"-1\"&gt;12 November 1936&lt;/font&gt;\n&lt;/center&gt;&lt;/h4&gt;\n&lt;blockquote&gt;&lt;p&gt;&lt;font face=\"Times\"&gt;I have, with some friends, put an\nAmendment on the Paper. It is the same as the Amendment which I\nsubmitted two years ago, and I have put it in exactly the same terms\nbecause I thought it would be a good thing to remind the House of\nwhat has happened in these two years. Our Amendment in November 1934\nwas the culmination of a long series of efforts by private Members\n\n[... more...]\n\n&lt;/p&gt;&lt;/td&gt;&lt;/tr&gt;\n&lt;/table&gt;&lt;/p&gt;&lt;/blockquote&gt;\n&lt;p&gt;&lt;center&gt; &lt;/center&gt;&lt;/p&gt;\n&lt;p&gt;&lt;center&gt; &lt;/center&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nWe‚Äôll use HTML tags to narrow down our selection as much as possible. The content we want is contained within &lt;blockquote&gt; tags, and each paragraph is wrapped in &lt;font&gt; tags.\n# Get the text from between each pair of &lt;font&gt; tags within the &lt;blockquote&gt; tags\nspeech_raw = [passage.get_text() for passage in soup.blockquote.find_all(\"font\")]\n\n# Join all the paragraphs together into one long string\nspeech_raw = \" \".join(speech_raw)\n\n# \"Chop off\" the irrelevant info at the bottom\nspeech_raw = speech_raw.split(\"...................\")[0]\n\n# Have a look\nspeech_raw[:200]\n'I have, with some friends, put an\\nAmendment on the Paper. It is the same as the Amendment which I\\nsubmitted two years ago, and I have put it in exactly the same terms\\nbecause I thought it would be a g'\nThis is looking good, except we have some randomly scattered newline characters (\\n) mixed in with the great man‚Äôs words. We‚Äôll use a regular expression to tidy them up.\nimport re\n\n# Compile the regex: maybe some whitespace, then \\n, then maybe more whitespace\nnewline = re.compile(\"\\s*\\\\n\\s*\")\n\n# Sub each instance of the regex for a single space\nspeech = newline.sub(\" \", speech_raw)\n\n# Have a look\nspeech[:200]\n'I have, with some friends, put an Amendment on the Paper. It is the same as the Amendment which I submitted two years ago, and I have put it in exactly the same terms because I thought it would be a g'\nMarvellous.\nNow we turn our attention to the site‚Äôs index page, and the links to the individual speech pages.\n\nThe speech pages are all accessed by these ‚ÄúOPENSml.Jpeg‚Äù image buttons. So we need to get all the link (&lt;a&gt;) elements that contain one of these buttons, and then take the href attribute from each of these link elements.\nspeech_index = \"http://www.churchill-society-london.org.uk/SpchIndx.html\"\nresponse = requests.get(speech_index)\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n# Find the \"OPENSml\" images; get their parents (the links); get the href attribute\nspeech_urls = [url.parent[\"href\"] for url in soup.find_all(\"img\", src=\"OPENSml.Jpeg\")]\nspeech_urls\n['Webmap.html',\n 'Locusts.html',\n 'Austria.html',\n 'Munich.html',\n 'RusnEnig.html',\n 'Joybells.html',\n 'BdTlTrsS.html',\n 'BeYeMofV.html',\n 'Dunkirk.html',\n 'UnknWarr.html',\n 'FnstHour.html',\n 'thefew.html',\n 'ToCzechP.html',\n 'LaFrance.html',\n 'DthChbln.html',\n 'GutTools.html',\n 'LngHrdWr.html',\n 'CptsSoul.html',\n 'NEVER.html',\n 'Congress.html',\n 'HoCJan42.html',\n 'EndoBegn.html',\n 'InvaFrnc.html',\n 'DthRovlt.html',\n '13May45.html',\n 'EndofWar.html',\n 'YrVictry.html',\n 'Revw1945.html',\n 'Fulton.html',\n 'astonish.html',\n 'WSCHague.html',\n 'HonAmrcn.html',\n 'Honour.html']\nNow we want to go to each of these pages (except the first one, which is a link to the site map) and get the speech, just like we did for the ‚ÄúLocusts‚Äù speech a minute ago.\nspeech_list = []\n\nfor url in speech_urls[1:]:\n    \n    # Append the local url to the site's base url\n    full_url = \"http://www.churchill-society-london.org.uk/%s\" % url\n    \n    # Get the page\n    response = requests.get(full_url)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    \n    # Add the speech to the list\n    speech_raw = [passage.get_text() for passage in soup.blockquote.find_all(\"font\")]\n    speech_raw = \" \".join(speech_raw).split(\"...................\")[0]\n    \n    newline = re.compile(\"\\s*\\\\n\\s*\")\n    speech = newline.sub(\" \", speech_raw)\n    \n    speech_list.append(speech)\nFinally, let‚Äôs stick all these speeches together into one massive long Churchillian superspeech.\nspeeches_str = \"\".join(speech_list)\nspeeches_str\n'I have, with some friends, put an Amendment on the Paper. It is the same as the Amendment which I submitted two years ago, and I have put it in exactly the same terms because I thought... [truncated]"
  },
  {
    "objectID": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#you-ask-what-is-our-aim-i-can-answer-in-one-word-victory",
    "href": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#you-ask-what-is-our-aim-i-can-answer-in-one-word-victory",
    "title": "Markov and Churchill: an exploration of predictive text",
    "section": "You ask, what is our aim? I can answer in one word: Victory",
    "text": "You ask, what is our aim? I can answer in one word: Victory\nThe time has come to create our Churchill-inspired MarkovGenerator.\nwinston = MarkovGenerator(speeches_str, tuple_size=3)\nFingers crossed. Here we go.\ntext = winston.generate_text(length=300)\ntext\n\n‚Äúand to warn His Majesty‚Äôs Government, I make no promises. In no way have I mitigated the sense of the Government had begun. I am not at all costs, victory, in spite of the world to see, the military industries and communications of Germany and to be worthy of their devotion and their courage. The hospital ships, which brought off many thousands of millions of armed and trained men, for whom he works and contrives. The awful ruin of Europe, including-I say deliberately-the German and Italian resources. This is what the cost, who never flinched under the German Air Force, had to be, incomparably the strongest possible resistance from the west, the enemy wherever he may have in respect of long-distance bombing aeroplanes and that France has yet been given to us to draw more readily upon our shipping virtually undiminished, and with the comfort of knowing that his country had ever formed. So in doing this and for the very large part of what happened at sea, and a war leader. I felt encompassed by an exaltation of spirit which was not denied to their care. But all depends now upon the routes nearer home, and to its close. In its place we are to stir the English-speaking Commonwealths be added to one of the war. I should like to do. That is what they are being slowly compressed, and we see with our shipping virtually undiminished, and with the magnificent efforts of the nation free as soon as possible. Secondly, the presence of these Balkan countries where only one which has overtaken her and we shall fight in France, and what might now shower immeasurable material blessings upon mankind, may even do him some good. It certainly offers a measure of agreement between us, and we shall defend every‚Äù ‚Äî winston, July 2017\n\nAdmittedly, it‚Äôs not quite up to the standard of the man himself. But you can feel his presence behind the semi-nonsense. Maybe."
  },
  {
    "objectID": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#to-improve-is-to-change-to-be-perfect-is-to-change-often",
    "href": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#to-improve-is-to-change-to-be-perfect-is-to-change-often",
    "title": "Markov and Churchill: an exploration of predictive text",
    "section": "To improve is to change; to be perfect is to change often",
    "text": "To improve is to change; to be perfect is to change often\nI lied earlier.\nIn reality, your modern phone‚Äôs predictive text isn‚Äôt a truly random Markov process.\nThat‚Äôs because your phone is clever: not only does it store the possibilities for the following word, but also the relative likeliness of each word being the one you want. For example:\nfish and  =&gt;  chips, 0.999\n              quinoa, 0.001\n              ...\n              \nAnd then it might suggest the top 3 words in this list to you while you‚Äôre typing.\nIndeed, sometimes your phone is even cleverer than that. If you choose one of the suggestions more often than another, it will update the probabilites to better represent your personal use. Or if you ignore its suggestions and type something that isn‚Äôt in its cache already, it will add your new word to the cache and remember it for next time.\nNevertheless, the Markov process we‚Äôve explored in this post is there in the background, and without it, predictive text technology wouldn‚Äôt have reached the point where it is today. And we wouldn‚Äôt have any of the other more amusing applications of Markov chains for text generation either."
  },
  {
    "objectID": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#footnote-lockouts-page",
    "href": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#footnote-lockouts-page",
    "title": "Markov and Churchill: an exploration of predictive text",
    "section": "Footnote: Lockouts page?",
    "text": "Footnote: Lockouts page?\nAt the top of this post, when I played the predictive text game, the phrase ‚Äúlockouts page‚Äù produced itself. I was fairly sure I‚Äôd never typed that. What even is a lockouts page?\nWhen I tried to find an answer for that question, I found something interesting‚Ä¶\n\nIt seems my ‚Äúpersonal‚Äù predictive text suggestions aren‚Äôt actually very personalised. I‚Äôm actually quite relieved, because this discovery has reassured me that if I am indeed really boring and repetitive then everyone else using an Android phone is too."
  },
  {
    "objectID": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#bonus-define-funny",
    "href": "posts/2017-07-23-markov-and-churchill-an-exploration-of-predictive-text/index.html#bonus-define-funny",
    "title": "Markov and Churchill: an exploration of predictive text",
    "section": "Bonus: define ‚Äúfunny‚Äù",
    "text": "Bonus: define ‚Äúfunny‚Äù\nWe can make a MarkovGenerator object with any text string we like. So I tried out a couple of others, including Webster‚Äôs Unabridged English Dictionary. Here are some of my favourite ‚Äúdefinitions‚Äù the webster MarkovGenerator came up with.\n\nANNECTENT An*nec‚Äùtent, a.\nEtym: [Pref. re- + center.]\nDefn: To predict or foretell; characterized by inflammation of the sun.\nANTHOLOGIST An*thol‚Äùo*gist, n.\nDefn: The quality of being persecuted.\nBLACKBIRDING Black‚Äùbird*ing, n.\n(Chem.) Defn: The quality or state of being health.\nHORNPIKE Horn‚Äùpike`, n.\nDefn: The supraoccipital bone.\nIMPLAUSIBILITY Im*plau`si*bil‚Äùi*ty, n.\nDefn: A delicate person; a swell neighborhood.\nJEWELER Jew‚Äùel*er, n.\nEtym: [Cf. Chatter.]\n\nTo decorate with stucco, or fine hair, for removing substances from organic material and spiritual rulers, now used ironically or contemptuously.\n\nMESSENGER Mes‚Äùsen*ger, n.\nEtym: [Ar.]\nDefn: An instrument which is assumed to pervade all space.\nPOTPOURRI Pot`pour`ri‚Äù, n.\nEtym: [L., silkworm.\nSee Silkworm."
  },
  {
    "objectID": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html",
    "href": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html",
    "title": "Multiple Names for Datasets in R Packages",
    "section": "",
    "text": "‚ÄúI am known by many names, but you may call me‚Ä¶ Tim.‚Äù - Graham Chapman\nLast year1 the {mangoTraining} package got a fair bit of much-needed TLC, and we discovered a problem which at first glance didn‚Äôt seem too tricky to solve, but which ended up leading down a very interesting rabbit hole.\nThe package is primarily a ‚Äúdata package‚Äù, i.e.¬†a package which exists purely to provide datasets to users, and contains nearly 30 datasets of various sizes which are used in Mango‚Äôs training courses and referred to in books such as ‚ÄúR in 24 Hours‚Äù.\nWe tend to teach the latest best practices, so our training courses are updated on a regular basis.2 Unfortunately once a physical book has been released into the wild, it‚Äôs not so easy to update it in the same way; so whenever we make changes to our material, we have to be careful not to break backwards compatibility!\nThis is how we stumbled upon the ‚Äúmultiple names for datasets in R packages‚Äù problem. Suppose we have a dataset in {mangoTraining} which was historically called demoData, but we now favour snake_case rather than lowerCamelCase in our recent material: how can we also use the name demo_data to refer to the dataset, so that we can use the new name in our new material without breaking compatibility with existing material?\nAnd since we generally want to keep packages as small as possible3: can we do this without including multiple copies of the same dataset in the package?\nThis allows us to set out three requirements which cover what we‚Äôre trying to achieve:"
  },
  {
    "objectID": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#adding-more-data-files",
    "href": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#adding-more-data-files",
    "title": "Multiple Names for Datasets in R Packages",
    "section": "Adding more data files",
    "text": "Adding more data files\nI‚Äôm going to start by creating a new package, and then and immediately adding a dataset which I would like to include - which is incredibly simple these days, thanks to the marvellous {usethis} package!\n usethis::create_tidy_package(\"datapkg\")\n \n # ... and then once the new session has opened:\n diamonds &lt;- data(\"diamonds\", package = \"ggplot2\")\n usethis::use_data(diamonds)\nFollowing this, we can see that our dataset has been saved into the data/ directory as a .rda file4, meaning our users can load this dataset into their session using data(\"diamonds\", package = \"datapkg\").\nThe name of the file has been created, quite sensibly, from the name of the R object we saved into it. Renaming the file isn‚Äôt a good idea, because the object it contains will still have the old name! For example, if we manually renamed data/diamonds.rda to data/diamantes.rda and rebuilt/reinstalled our package, then if we ran\ndata(\"diamantes\", package = \"datapkg\")\nwe would get our data loaded into our session; BUT it would still be in an object called diamonds! This is because .rda files store an entire R environment, possibly containing multiple objects: when we load that environment into our session, all the objects are just dumped into the session, exactly as they were when they were saved.5\nSo if we want a different name for our dataset, we actually need to rename the object itself before saving it:\ndiamantes &lt;- diamonds\nusethis::use_data(diamantes)\nThis is all useful context, but we haven‚Äôt actually satisfied our first requirement yet! Our users can now get either diamonds or diamantes from our package; but that‚Äôs because the package contains the same dataset twice, in two separate .rda files (diamonds.rda and diamantes.rda). What we would ideally like to do is to have multiple names pointing towards the same underlying dataset, so that we only have to have one copy of that dataset in our package but it can be called in multiple ways."
  },
  {
    "objectID": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#multiple-names-for-functions",
    "href": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#multiple-names-for-functions",
    "title": "Multiple Names for Datasets in R Packages",
    "section": "Multiple names for functions",
    "text": "Multiple names for functions\nIt‚Äôs really easy to do something along those lines with functions, because we can store a function in a variable and then just reassign that variable to our heart‚Äôs content.6\nTo prove the point:\nf &lt;- function(x, y) x+y\ng &lt;- f\ng\nfunction(x, y) x+y\nThis can be really useful when you want to implement a piece of functionality in just one place, but then to allow the user to access the same functionality via more than one name. Here‚Äôs an example from {tibble}: the ‚Äútype check‚Äù function for tibble objects is named is_tibble(), in line with tidyverse naming conventions; but the authors also provide an alias, is.tibble(), which matches base R naming conventions (is.numeric(), is.data.frame() etc) in anticipation of users trying that pattern first.\nis.tibble &lt;- function(x) {\n  deprecate_warn(\"2.0.0\", \"is.tibble()\", \"is_tibble()\")\n\n  is_tibble(x)\n}\nYou can see this in the context of the source code here. If a user calls is.tibble(), the responsibility is passed straight on to is_tibble() after a nudge towards the ‚Äúpreferred‚Äù tidyverse syntax."
  },
  {
    "objectID": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#multiple-names-for-datasets",
    "href": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#multiple-names-for-datasets",
    "title": "Multiple Names for Datasets in R Packages",
    "section": "Multiple names for datasets?",
    "text": "Multiple names for datasets?\nLet‚Äôs review again the key points we would like to achieve:\n\nThe package must contain only a single copy of the full dataset\nThe dataset must be accessible using at least two different names\nEach name must work correctly with data(), i.e.¬†data(\"x\", package = \"pkg\") must load an object called x into the session\n\nThat third point is quite important for the usability of the package. In fact, if we spend some time thinking about the third point here, we actually get a big push in the right direction!\nFrom the Details section of the documentation for data():\n\nCurrently, four formats of data files are supported:\n\nfiles ending ‚Äò.R‚Äô or ‚Äò.r‚Äô are source()d in, with the R working directory changed temporarily to the directory containing the respective file. (data ensures that the utils package is attached, in case it had been run via utils::data.)\nfiles ending ‚Äò.RData‚Äô or ‚Äò.rda‚Äô are load()ed. ‚Ä¶\n\n\nWe‚Äôre familiar with the second format, .rda, but maybe we didn‚Äôt know about the first! So it looks like we might be able to create a .R file which somehow loads the dataset which we have already saved elsewhere‚Ä¶\nLet‚Äôs make a file called data/diamantes.R, which - as we‚Äôve just learned! - will be run using source() if our user calls data(\"diamantes\", package = \"datapkg\").\nNow all we have to do is figure out how to ‚Äúcopy‚Äù our diamonds dataset into an object called diamantes.\nLet‚Äôs naively try a similar approach to the one we‚Äôve seen used for functions. I‚Äôll add the following line to data/diamantes.R:\ndiamantes &lt;- diamonds\nBut attempting to build the package quickly results in a nasty error‚Ä¶\n==&gt; Rcmd.exe INSTALL --no-multiarch --with-keep.source datapkg\n\n* installing to library 'C:/.../R/win-library/4.0'\n* installing *source* package 'datapkg' ...\n** using staged installation\n** R\n** data\n*** moving datasets to lazyload DB\n** byte-compile and prepare package for lazy loading\nError in eval(exprs[i], envir) : object 'diamonds' not found\nError: unable to load R code in package 'datapkg'\nExecution halted\nERROR: lazy loading failed for package 'datapkg'\n* removing 'C:/.../R/win-library/4.0/datapkg'\n* restoring previous 'C:/.../R/win-library/4.0/datapkg'\n\nExited with status 1.\nNote the object 'diamonds' not found!\nThis is because datasets aren‚Äôt like other R objects: as r-pkgs puts it,\n\nObjects in data/ are always effectively exported (they use a slightly different mechanism than NAMESPACE but the details are not important).\n\nThat ‚Äúslightly different mechanism‚Äù is our problem - we can‚Äôt simply refer to datasets as internal objects within our package! So we‚Äôll have to try a different approach‚Ä¶"
  },
  {
    "objectID": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#lets-read-that-again",
    "href": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#lets-read-that-again",
    "title": "Multiple Names for Datasets in R Packages",
    "section": "Let‚Äôs read that again",
    "text": "Let‚Äôs read that again\nRemember what the documentation for data() said? (We only looked at it a minute ago!)\nWe‚Äôve started to take advantage of\n\n1. files ending ‚Äò.R‚Äô or ‚Äò.r‚Äô are source()d in, with the R working directory changed temporarily to the directory containing the respective file\n\nalready - and now\n\n2. files ending ‚Äò.RData‚Äô or ‚Äò.rda‚Äô are load()ed\n\nis also going to be helpful, for getting hold of our already-saved data!\nLet‚Äôs edit data/diamantes.R so that it contains just the following line:\nload(\"diamonds.rda\")\nNote that we are using a relative path to diamonds.rda because, as informed by the docs, our working directory is now the directory where data/diamantes.R lives, i.e.¬†data/!\nNow our package does build and install, but we do get an ominous warning in the output‚Ä¶\n[...]\n** data\n*** moving datasets to lazyload DB\nWarning: object 'diamonds' is created by more than one data call\n** byte-compile and prepare package for lazy loading\n[...]\n‚Ä¶ which makes sense. When we call data(\"diamantes\", package = \"datapkg\"), we simply end up calling the same thing as data(\"diamonds\", package = \"datapkg\"), so we end up with an object called diamonds in our session rather than diamantes. We‚Äôre closer, but we‚Äôre not there yet!\nMaybe we can rearrange data/diamantes.R slightly to rename things for us‚Ä¶\nload(\"diamonds.rda\")\ndiamantes &lt;- diamonds\nThis seems very close. We get the same warning about diamonds when we rebuild the package, but now when we call data(\"diamantes\", package = \"datapkg\") we do get an object called diamantes‚Ä¶ we just also get one called diamonds! This is because when we source data/diamantes.R, we end up with two objects in the environment at the end of the script (diamonds and diamantes), both of which are then pulled into our R session.\nSo we can add one more line to fix this problem:\nload(\"diamonds.rda\")\ndiamantes &lt;- diamonds\nrm(diamonds)\nAnd now we have achieved our goal! No more warning message when we build, and both diamonds and diamantes work in the way we wanted."
  },
  {
    "objectID": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#now-do-it-in-one-line",
    "href": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#now-do-it-in-one-line",
    "title": "Multiple Names for Datasets in R Packages",
    "section": "Now do it in one line",
    "text": "Now do it in one line\nThree lines was too many for you?? Okay - we can replace the current contents of data/diamantes.R with:\ndiamantes &lt;- local(get(load(\"diamonds.rda\")))\nWorking from the inside out:\n\nload(\"diamonds.rda\") loads diamonds into our environment, and returns a vector of names of all loaded objects, i.e.¬†c(\"diamonds\")\nget() retrieves the object with a name of \"diamonds\" from the current environment, since we didn‚Äôt provide a different environment via the pos argument - i.e.¬†we now have the actual data frame which the diamonds variable name refers to\nlocal() means all of that happened in its own little throwaway environment - so we end up with the data frame object which was returned from get(), and everything else is just thrown away, including the diamonds object which had been created by load()\nAnd finally, we assign that data frame into an object called diamantes, which is the only object we created in our main environment and therefore the only object which will be created when we call data(\"diamantes\", package = \"datapkg\")"
  },
  {
    "objectID": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#documentation",
    "href": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#documentation",
    "title": "Multiple Names for Datasets in R Packages",
    "section": "Documentation",
    "text": "Documentation\nOf course, it‚Äôs important to document our datasets - again we can refer to the relevant chapter of the ever-so-useful r-pkgs for guidance! The documentation should live in R/data.R: we only have to write one set of documentation, and then we can use the @rdname Roxygen tag to include the other names!\n#' Prices of 50,000 round cut diamonds.\n#'\n#' A dataset containing the prices and other attributes of almost 54,000\n#' diamonds.\n#'\n#' @format A data frame with 53940 rows and 10 variables:\n#' \\describe{\n#'   \\item{price}{price, in US dollars}\n#'   \\item{carat}{weight of the diamond, in carats}\n#'   ...\n#' }\n#' @source \\url{http://www.diamondse.info/}\n\"diamonds\"\n\n#' @rdname diamonds\n\"diamantes\""
  },
  {
    "objectID": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#epilogue",
    "href": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#epilogue",
    "title": "Multiple Names for Datasets in R Packages",
    "section": "Epilogue",
    "text": "Epilogue\nSo far I‚Äôve noticed two important things to bear in mind when using this approach:\n\nWe don‚Äôt have to stop at two names - if we want to, we can keep making new data/*.R files which point back to the same .rda file! HOWEVER, we shouldn‚Äôt have two files with names which differ only in capitalisation, e.g.¬†diamantes.R and DiaMantes.R - this is not possible on Windows, and not a great idea on other platforms! (In this particular case, we could have diamantes.rda with diamonds.R and DiaMantes.R instead)\nIf you build your package using the --resave-data flag, any data/*.R files will be sourced and resaved as .rda files as part of the build process, rendering all your hard work useless!7\n\nI‚Äôd be keen to hear whether anyone else has success with this method too, or if there are better ways to achieve the same thing! We seemed to make it through the CRAN submission process for {mangoTraining} with no problems at all; but I‚Äôm curious as to how reproducible this is. In particular I would love to know whether the CRAN builds use --resave-data or not, and whether that would scupper this method for packages where resaving takes us over the package size limit!"
  },
  {
    "objectID": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#footnotes",
    "href": "posts/2021-01-30-multiple-names-for-datasets-in-r-packages/index.html#footnotes",
    "title": "Multiple Names for Datasets in R Packages",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, it‚Äôs taken me nearly a year to write up this post. Please forgive me, I‚Äôve been very busy surviving.‚Ü©Ô∏é\nKeeping content up to date can be quite a lot of work in itself! So in fact it was only quite recently that we committed 100% to following the tidyverse style guide wherever possible.‚Ü©Ô∏é\nWhile keeping packages small is a good idea anyway (especially if you‚Äôre using version control!), an extra motivating factor here is the 5MB upload limit for CRAN packages as detailed on the CRAN policies page. You can imagine a situation where we want to use 2 different names for a 3MB dataset!‚Ü©Ô∏é\nAs we would expect from {usethis}, this completely aligns with the rules and conventions on saving datasets within R packages, set out in r-pkgs and elsewhere.‚Ü©Ô∏é\nIt took me a REALLY long time to get my head around this at first - has anyone else had an experience like trying to run diamantes &lt;- data(\"diamonds\"), and then getting really frustrated when diamantes is not the data frame you were expecting? I found that so unintuitive. Actually nowadays when I need to save a single R object, I tend to use .rds rather than .rda for exactly that reason. However, .rda is the convention for R packages, so we‚Äôll stick with it‚Ä¶ grrrr‚Ä¶‚Ü©Ô∏é\nThis is because functions in R are ‚Äúfirst-class objects‚Äù, i.e.¬†you can give them a name, save them in variables, pass them into other functions as parameters‚Ä¶ I took this for granted until recently, when I found out that you can‚Äôt do this so easily in many languages - for example, Java!‚Ü©Ô∏é\nAnd possibly resulting in two identically-named files, if you had e.g.¬†hello.rda and HeLLO.R.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2019-06-13-nvidia-drivers-on-fedora/index.html",
    "href": "posts/2019-06-13-nvidia-drivers-on-fedora/index.html",
    "title": "Debugging: Installing NVIDIA Drivers on Fedora",
    "section": "",
    "text": "TL;DR: I had mega issues with getting NVIDIA graphics to work on Fedora. This is a play-by-play of my debugging process, and ultimately the solution turned out to be rather simple."
  },
  {
    "objectID": "posts/2019-06-13-nvidia-drivers-on-fedora/index.html#context",
    "href": "posts/2019-06-13-nvidia-drivers-on-fedora/index.html#context",
    "title": "Debugging: Installing NVIDIA Drivers on Fedora",
    "section": "Context",
    "text": "Context\nSometime last year my company decided to upgrade its IT assets, and consequently sold off a bunch of old laptops to interested employees.\nI had been toying with the idea of getting a spare laptop anyway in order to play around with a Linux-based operating system, so this was the perfect opportunity. I managed to nab myself a chunky old Dell Latitude and, much to the frustration of my friend in IT who had spent the afternoon setting up a fresh Windows installation for me, asked him to borrow a USB stick and then immediately wiped the hard drive so that I could install Fedora.\nImmediately, however, karma struck. I was attempting to use Fedora Media Writer to install the OS onto the laptop; but every time I successfully booted to the installation GUI, the laptop would freeze completely and I‚Äôd have to turn it off via the power button.\nEventually I succeeded by means of the installer‚Äôs ‚Äúbasic graphics mode‚Äù. However, after rebooting, it became clear that this had come with a catch: my screen was now stuck at a low resolution. Of course, I wanted to take advantage of all 1080 of my available p‚Äôs, so set about looking for the issue.\nBear in mind that this was more or less my first experience with a Linux OS. I knew how to do simple command line things, but I didn‚Äôt really know how Linux was set up, or how to change boot parameters, or what on earth a GRUB was, or any of those other fun low-level system things. So after a LOT of searching and reading, I eventually discovered the /etc/default/grub file and the nomodeset boot parameter. ‚ÄúHah, problem solved!‚Äù I exclaimed, exterminating nomodeset from wherever it appeared. I rebooted the laptop: it froze immediately.\nBy this time I‚Äôd had enough for one day, so I shut the lid and forgot about it."
  },
  {
    "objectID": "posts/2019-06-13-nvidia-drivers-on-fedora/index.html#diagnosis-and-solution",
    "href": "posts/2019-06-13-nvidia-drivers-on-fedora/index.html#diagnosis-and-solution",
    "title": "Debugging: Installing NVIDIA Drivers on Fedora",
    "section": "Diagnosis and solution",
    "text": "Diagnosis and solution\nFast-forward an ENTIRE YEAR and finally, I had the time and inclination to take another look at the problem. For good measure, I decided to start with a fresh installation of the latest version the OS, Fedora 30.\nOnce again I had to run the installer in ‚Äúbasic graphics‚Äù mode, and so I was faced with lower-than-ideal screen resolution.\nI started by upgrading the OS with a sudo dnf update, which took a while, as expected.\nThen I spent several days trying ALL SORTS of fixes and tweaks and changes that I found on the internet. The laptop contains a NVIDIA graphics card, and apparently there are a whooooole load of issues with using Fedora and GNOME display manager (Fedora‚Äôs default graphical system) on NVIDIA hardware. I added repositories and installed drivers. I changed boot parameters. I modified configuration files. I added things to blacklists, and removed things from blacklists, and installed different display managers, and regenerated initramfs, and banged on the desk while laughing hysterically, and all sorts of other weird and wonderful things; but all to no avail.\nOn the plus side, I was learning a lot about Linux. I learned about virtual terminals when I accidentally disabled GDM and ended up with a bootscreen which perpetually flashed ‚ÄúStarting GNOME display manager‚Äù. I discovered the GRUB boot menu, and runlevels when I got fed up with trying to type my password letter by letter whenever the bootscreen flashed up for half a second at a time.\nEventually I ended up wiping and reinstalling the OS one more time, and then sat down with a cup of tea in front of a runlevel-3 command line, determined to get to the bottom of the issue once and for all, with logic and patience.\nFirst things first: make sure the NVIDIA graphics are actually being used, right? So I checked the list of loaded modules for anything containing ‚Äúnvidia‚Äù:\nlsmod | grep nvidia\nOh. Nothing came back from that.\nAh yes, I thought. Duh. I‚Äôll need to install the NVIDIA drivers.\nThere seem to be a couple of choices for where to get those drivers, but the repository which seemed to come up most often was RPM Fusion. So first we add the repo, then we install the driver according to the instructions on their website (https://rpmfusion.org/Howto/NVIDIA):\nsudo dnf install https://download1.rpmfusion.org/free/fedora/rpmfusion-free-release-$(rpm -E %fedora).noarch.rpm https://download1.rpmfusion.org/nonfree/fedora/rpmfusion-nonfree-release-$(rpm -E %fedora).noarch.rpm\n\nsudo dnf install xorg-x11-drv-nvidia akmod-nvidia\nCool. Reboot and try again.\nlsmod | grep nvidia\nNothing, nada, zilch. I double-checked that the NVIDIA driver had installed:\nmodinfo nvidia\nThat gave me a bunch of info, so the NVIDIA driver was definitely available‚Ä¶ so if not NVIDIA, what was being used for graphics?\nI remembered reading something about the fallback graphics system, which would be used if the desired system failed to load properly. Suspecting that this might be the case, I made sure:\nlsmod | grep nouveau\nThis time I got several lines of output! So the problem wasn‚Äôt that the NVIDIA graphics module was misbehaving, but rather that it wasn‚Äôt loading at all.\nThis was progress! Cue a lot more playing around with boot parameters etc‚Ä¶\nEventually I realised that I could just force the system to load the NVIDIA module right there and then, using the modprobe command:\nmodprobe nvidia\n## modprobe: ERROR: could not insert 'nvidia': no such device\nWait. No no no wait, what?! But modinfo said that oh you I just aaaaaaaaaaaaaaaaaaAAAAAAAAAAAAAAAAAAAA (‚îõ‚úß–î‚úß)‚îõÂΩ°‚îª‚îÅ‚îª\nOne quick cuppa later, gathered and composed, I tried again and asked for a little bit more information:\nmodprobe nvidia -v\nI had also recently come across dmesg for displaying kernel messages, so thought I‚Äôd try that out too‚Ä¶\ndmesg | less\nAh. Some wild NVIDIAs appeared pretty near the bottom of that output‚Ä¶ let‚Äôs have a closer look‚Ä¶\n## NVRM: The NVIDIA NVS4200M GPU installed in this system is\n## NVRM:  supported through the NVIDIA 390.xx Legacy drivers. Please\n## NVRM:  visit http://www.nvidia.com/object/unix.html for more\n## NVRM:  information.  The 430.14 NVIDIA driver will ignore\n## NVRM:  this GPU.  Continuing probe...\n## NVRM: No NVIDIA graphics adapter found!\nOh oh oh oh oh oh oh oh oh oh oh oh oh OH.\nSo if I just‚Ä¶\nsudo dnf remove xorg-x11-drv-nvidia\n‚Ä¶ and then‚Ä¶\ndnf list --repo rpmfusion-nonfree | grep nvidia | grep 390\n‚Ä¶ and then‚Ä¶\nsudo dnf install xorg-x11-drv-nvidia-390xx\nOne reboot later, and EVERYTHING WAS FINE. Everything. No freezing. No digging around in boot config files. And it‚Äôs been fine ever since. It‚Äôs like nothing ever happened."
  },
  {
    "objectID": "posts/2019-06-13-nvidia-drivers-on-fedora/index.html#review",
    "href": "posts/2019-06-13-nvidia-drivers-on-fedora/index.html#review",
    "title": "Debugging: Installing NVIDIA Drivers on Fedora",
    "section": "Review",
    "text": "Review\nReally this was all just rather frustrating at the time. I will freely admit that on more than one occasion during this saga I loudly exclaimed ‚ÄúI hate computers‚Äù to the little stuffed toy cat that sits on my desk. I say that quite often, as friends and colleagues will testify, and I stand by it. I really do hate them with a burning passion.\nHowever, if there‚Äôs one thing I hate more than computers, it‚Äôs giving up on something once I‚Äôve gotten my teeth into it. Even in the depths of despair, I could recognise that I was at least learning a lot of potentially useful information; and ultimately, that was enough to keep me going until I eventually hit that dmesg breakthrough.\nI suppose it was a valuable experience, maybe, perhaps, in some ways. Maybe. Anyway, I figured it was worth writing about in case anyone is having similar issues, or in case anyone wanted to read a heartwarming story about courage in the face of adversity or something."
  },
  {
    "objectID": "posts/2016-11-22-30-days-30-visualizations-1-dataset-part-3/index.html",
    "href": "posts/2016-11-22-30-days-30-visualizations-1-dataset-part-3/index.html",
    "title": "30 Days, 30 Visualizations, 1 Dataset: Days 11-15",
    "section": "",
    "text": "Day 11 (22/11/16): Type of ‚Äústrange‚Äù occupancies\n\n\nLooking at the dataframe of unusual occupancies from yesterday, I‚Äôll put each record into one of four categories, in order to more easily see which records are definitely wrong, which are questionable, etc.\n\n```rtype &lt;- vector(‚Äúcharacter‚Äù, nrow(df3))\ntype[df3\\(Occupancy == 0] &lt;- \"Zero\"\ntype[df3\\)Occupancy &lt; 0] &lt;- ‚ÄúNegative‚Äù type[df3\\(Occupancy == df3\\)Capacity] &lt;- ‚ÄúFull‚Äù type[df3\\(Occupancy &gt; df3\\)Capacity] &lt;- ‚ÄúOverfilled‚Äù\ndf3$Type &lt;- factor(type, levels = c(‚ÄúOverfilled‚Äù, ‚ÄúFull‚Äù, ‚ÄúZero‚Äù, ‚ÄúNegative‚Äù))\np &lt;- ggplot(df3, aes(x = LastUpdate, y = ‚Äú‚Äú)) + geom_point(aes(colour = Type)) + facet_wrap(~ Name, nrow = 4) + scale_colour_manual(values = rev(c(‚Äùred‚Äù, ‚Äúblack‚Äù, ‚Äúgreen‚Äù, ‚Äúblue‚Äù))) + ylab(‚Äú‚Äú) + ggtitle(‚ÄùType of \"Strange\" Occupancies‚Äù) + theme(plot.title = element_text(size = rel(1.5)))\np&lt;img src=\"day11.jpeg\" /&gt; &lt;p&gt;We can see, for example, that all the definitely-wrong negative-occupancy records are from SouthGate General - maybe the council should look into upgrading the sensors there‚Ä¶&lt;/p&gt; &lt;hr&gt; &lt;h3&gt;Day 12 (23/11/16): Hours of over- and negative occupancy&lt;/h3&gt; &lt;p&gt;We had another Bath ML Meetup tonight - we didn‚Äôt have too long to talk about the project, but plans are being made‚Ä¶&lt;/p&gt; &lt;p&gt;Using the dataframe from yesterday, I‚Äôll have a quick look at when the particularly dubious records - the overfilled and negative records, that is - are generally recorded. I‚Äôm curious as to whether, for example, overfilled records are more common at lunchtime than overnight.&lt;/p&gt;rdf4 &lt;- filter(df3, Type == ‚ÄúOverfilled‚Äù | Type == ‚ÄúNegative‚Äù) %&gt;% mutate(Time = hour(LastUpdate) + minute(LastUpdate)/60)\np &lt;- ggplot(df4, aes(x = Time, y = ‚Äú‚Äú)) + geom_point(alpha = 0.01) + facet_wrap(~ Type, nrow = 2) + scale_x_continuous(breaks = seq(0, 24, 4)) + ggtitle(‚ÄùHours of over- and negative occupancy‚Äù) + ylab(‚Äú‚Äú) + theme(plot.title = element_text(size = rel(1.5)), panel.background = element_blank())\np&lt;img src=\"day12.jpeg\" /&gt; &lt;p&gt;As it turns out, the overfilled records are spread over the whole day, with a decrease only in the early morning. Negative records tend to be from overnight, although there are some during the day too.&lt;/p&gt; &lt;hr&gt; &lt;h3&gt;Day 13 (24/11/16): Spread of negative occupancies&lt;/h3&gt; &lt;p&gt;I‚Äôm wondering how the negative occupancies are distributed - we‚Äôve seen that there are some very large negative values, but I‚Äôm curious as to how many of these there are, compared to more modest values. My reasoning here is that there are probably a lot of small negative values due to physical, quickly-resolved sensor errors, and then fewer very large negative values due to errors in the uploading of data or some other issue.&lt;/p&gt;rdf4 &lt;- filter(df3, Type == ‚ÄúNegative‚Äù)\n\nHaving sneakily looked at plot already, I‚Äôll points in certain ranges\ncounts &lt;- c(tally(filter(df4, Occupancy &lt; -10000))[2], tally(filter(df4, Occupancy &gt;= -10000, Occupancy &lt;= -1000))[2], tally(filter(df4, Occupancy &gt; -1000))[2]) %&gt;% sapply(sum)\np &lt;- ggplot(df4, aes(y = ‚Äú‚Äú, x = Occupancy)) + geom_point() + ggtitle(‚ÄùSpread of negative occupancies‚Äù) + ylab(‚Äú‚Äú) + annotate(‚Äùtext‚Äù, x = c(-14500, -2000, -100), y = c(rep(1.2, 3)), label = counts)\np&lt;img src=\"day13.jpeg\" /&gt; &lt;hr&gt; &lt;h3&gt;Day 14 (25/11/16): Massively negative occupancies&lt;/h3&gt; &lt;p&gt;Yesterday‚Äôs plot revealed a small cluster of points at around -15000. Let‚Äôs have a more detailed look at these points, and see if we can work out ‚Äòhow‚Äô they are wrong - for example, if they are all the same value, which might suggest it is a deliberate ‚Äòindicator‚Äô value used by the council to denote a closed car park.&lt;/p&gt;rdf4 &lt;- filter(df3, Occupancy &lt; -10000)\nsummary(df4\\(Name)```\n```rconsole##       Avon Street CP  Charlotte Street CP         Lansdown P+R\n##                    0                    0                    0\n##        Newbridge P+R         Odd Down P+R            Podium CP\n##                    0                    0                    0\n## SouthGate General CP    SouthGate Rail CP        test car park\n##                   26                    0                    0```\n```rrange(df4\\)Occupancy)rconsole## [1] -14787 -14638rtime_length(max(as.double(df4\\(LastUpdate)) - min(as.double(df4\\)LastUpdate)), unit = ‚Äúhour‚Äù)rconsole## [1] 0.02361111&lt;p&gt;So all of the points in this group are from SouthGate General CP, they span a range of values, and they were all recorded within a 2-hour period. Let‚Äôs have a look at the trend of these records.&lt;/p&gt;rp &lt;- ggplot(df4, aes(x = LastUpdate, y = Occupancy)) + geom_line() + ggtitle(‚ÄúSouthGate General CP: Occupancy on 05/01/2015‚Äù) + xlab(‚ÄúTime‚Äù)\np&lt;img src=\"day14.jpeg\" /&gt; &lt;p&gt;According to the records, the car park is emptying over this period - but the difference between 13:30 and 15:30 is only around 150 cars, which is a reasonable change over that period of time.&lt;/p&gt; &lt;p&gt;Perhaps there was an unexplained, but precise, shift of -15000ish in the Occupancy records for this brief 2-hour period - in which case the data might be salvageable‚Ä¶&lt;/p&gt; &lt;hr&gt; &lt;h3&gt;Day 15 (26/11/16): Trend of massively negative occupancies&lt;/h3&gt; &lt;p&gt;Let‚Äôs see whether the records from that strange Monday afternoon last January follow the trend of records from a typical Monday afternoon.&lt;/p&gt; &lt;p&gt;First I‚Äôll take all the records from Mondays &lt;em&gt;except&lt;/em&gt; for 05/01/2015 between 13:00 and 15:59, and average them out.&lt;/p&gt;rdf2 &lt;- select(df, Name, LastUpdate, Occupancy) %&gt;% filter(Name == ‚ÄúSouthGate General CP‚Äù, Occupancy &gt;= 0) %&gt;% filter(wday(LastUpdate) == 2) %&gt;% mutate(Time = update(LastUpdate, year = 1970, month = 1, day = 1)) %&gt;% mutate(Time = round_date(Time, ‚Äú10 minute‚Äù)) %&gt;% filter(hour(Time) == 13 | hour(Time) == 15) %&gt;% group_by(Time) %&gt;% summarize(newOcc = mean(Occupancy)) %&gt;% mutate(Period = ‚ÄúAverage‚Äù)&lt;p&gt;Now I‚Äôll get the dodgy records from 05/01/2015, shift them by adding a constant to each (initially 15000 - an educated guess), and combine these in a dataframe with the averaged records.&lt;/p&gt;rk &lt;- 15000\ndf3 &lt;- select(df, LastUpdate, Occupancy) %&gt;% filter(Occupancy &lt; -10000) %&gt;% mutate(Time = update(LastUpdate, year = 1970, month = 1, day = 1)) %&gt;% mutate(newOcc = Occupancy + k) %&gt;% select(Time, newOcc) %&gt;% mutate(Period = ‚Äú05/01/2015‚Äù)\ndf4 &lt;- rbind(df2, df3)&lt;p&gt;Now let‚Äôs plot!&lt;/p&gt;rp &lt;- ggplot(df4, aes(x = Time, y = newOcc)) + geom_line(aes(colour = Period)) + scale_fill_discrete(guide = guide_legend()) + ggtitle(‚ÄúSouthGate General CP: Adjusted‚Äù) + ylab(‚ÄúOccupancy‚Äù) + annotate(‚Äútext‚Äù, x = (max(df4\\(Time) - 1500), y = (min(df4\\)newOcc) + 80), label = paste0(‚ÄúShift: +‚Äù, k))\np``` \n\nWe can see that the shifted records do indeed follow the same pattern as the averaged records. A small adjustment to k confirms this.\n\n\n\nMaybe I can find a better value for this constant if I have a look at the records leading up to and away from the massively-negative period‚Ä¶\n\n\n\nOn another note:\n\n\nWOOOOOOAH, I‚ÄôM HALFWAY THERE!\n\n\nTo be completely honest, when I started this project just over 2 weeks ago I didn‚Äôt know how I would even get this far without running out of ideas. Here‚Äôs hoping I can keep it up for another 15 days!"
  },
  {
    "objectID": "posts/2024-04-27-use-deploy-keys-to-access-private-repos-within-github-actions/index.html",
    "href": "posts/2024-04-27-use-deploy-keys-to-access-private-repos-within-github-actions/index.html",
    "title": "Use deploy keys to access private repos within GitHub Actions",
    "section": "",
    "text": "Suppose you have a GitHub Actions workflow called your-workflow, within a repository called your-repo.\nThen suppose that for some reason (see Why would I ever need to do this?), within that workflow, you need to get hold of another repo - let‚Äôs call it your-extra-repo.\nThat‚Äôs easy enough if your-extra-repo is a public repo: you can ‚Äújust‚Äù use the GitHub-provided actions/checkout action multiple times within your workflow.\nBut what if it‚Äôs not a public repo?\nThere are a few possible approaches1 - I‚Äôm going to explain my preferred one here."
  },
  {
    "objectID": "posts/2024-04-27-use-deploy-keys-to-access-private-repos-within-github-actions/index.html#motivation",
    "href": "posts/2024-04-27-use-deploy-keys-to-access-private-repos-within-github-actions/index.html#motivation",
    "title": "Use deploy keys to access private repos within GitHub Actions",
    "section": "",
    "text": "Suppose you have a GitHub Actions workflow called your-workflow, within a repository called your-repo.\nThen suppose that for some reason (see Why would I ever need to do this?), within that workflow, you need to get hold of another repo - let‚Äôs call it your-extra-repo.\nThat‚Äôs easy enough if your-extra-repo is a public repo: you can ‚Äújust‚Äù use the GitHub-provided actions/checkout action multiple times within your workflow.\nBut what if it‚Äôs not a public repo?\nThere are a few possible approaches1 - I‚Äôm going to explain my preferred one here."
  },
  {
    "objectID": "posts/2024-04-27-use-deploy-keys-to-access-private-repos-within-github-actions/index.html#deploy-keys",
    "href": "posts/2024-04-27-use-deploy-keys-to-access-private-repos-within-github-actions/index.html#deploy-keys",
    "title": "Use deploy keys to access private repos within GitHub Actions",
    "section": "Deploy keys",
    "text": "Deploy keys\nA deploy key is an SSH key that you can attach to a single GitHub repository, and which provides access to just that repository.\nWe can use a deploy key as the core part of our solution. It‚Äôs an ideal choice here because it allows us to create a very specific access route:\n\nFrom the GitHub Actions runner which executes your-workflow\nTo the non-public your-extra-repo\nWith read-only permissions (you can add write permissions, but we shouldn‚Äôt for this particular purpose)\n\nHere‚Äôs how I usually set things up:\n\nCreate a new SSH keypair - it doesn‚Äôt matter what method you use to create it. I‚Äôd suggest using the following command in a terminal (a Linux terminal, or the RStudio Terminal, or Windows PowerShell‚Ä¶):\nssh-keygen -t ed25519 -f deploy\nIf you‚Äôre prompted, don‚Äôt set a passphrase. This will create two files in your current working called deploy.pub and deploy, containing the public and private parts respectively of a new SSH key.\nIn GitHub, navigate to your-extra-repo. In the Settings tab, find Security &gt; Deploy keys. Create a new deploy key:\n\nTitle - up to you, but I tend to call it something like your-repo-your-workflow2\nKey - this must be the public part of the SSH key you just created (you can open deploy.pub and copy the entire contents)\n\nNow navigate to the GitHub page for your-repo. In the Settings tab, find Security &gt; Secrets and variables &gt; Actions. Add a repository secret:\n\nName - again up to you, but I tend to use the pattern YOUR_EXTRA_REPO_DEPLOY_KEY\nValue - this must be the private part of the SSH key you just created (you can open deploy and copy the entire contents)\n\nDelete both parts of the key from wherever you created it (e.g.¬†delete deploy.pub and deploy) - we don‚Äôt need these any more!\n\nYou can follow these same steps to make more than one private repo accessible from your-workflow - if you do, you should create & use a different deploy key for each one."
  },
  {
    "objectID": "posts/2024-04-27-use-deploy-keys-to-access-private-repos-within-github-actions/index.html#why-would-i-ever-need-to-do-this",
    "href": "posts/2024-04-27-use-deploy-keys-to-access-private-repos-within-github-actions/index.html#why-would-i-ever-need-to-do-this",
    "title": "Use deploy keys to access private repos within GitHub Actions",
    "section": "Why would I ever need to do this?",
    "text": "Why would I ever need to do this?\nSo far, I‚Äôve come across two different cases where this trick can be handy!\n\nCase 1: git submodules\nSuppose your-repo contains a git submodule which lives in a private repo your-extra-repo, and that you need to get to something provided by that submodule within your-workflow.\n(For a concrete example of this, see my previous post about modular R code with {box}.)\nThe GitHub-provided actions/checkout action is typically used to check out the current repo within a GitHub Actions workflow. And at first glance, the solution looks simple.\nThe actions/checkout action takes some optional parameters:\n\nsubmodules - whether to check out submodules (default false)\n\nSo can we just specify submodules: true?\nUnfortunately not, because the submodule we want is in a private repo; we‚Äôll need to provide some way of verifying that we‚Äôre allowed to access it.\nAh-ha! we say, look, here‚Äôs another handy optional parameter:\n\nssh-key - if provided, it is used to fetch the specified repository (instead of fetching via HTTPS with the github.token generated for the workflow run)\n\nSo can we just pass our shiny new deploy key to this parameter via a GitHub secret?\nAgain, we quickly hit a problem: it seems that if you provide ssh-key, it is used for all git operations within the checkout action. So if we pass in a deploy key, we end up trying to use that deploy key to clone your-repo too, leading to failure (remember the whole point of a deploy key is that it allows access to one single repo, your-extra-repo in this case).\nSo ideally, we‚Äôd like to do a ‚Äúnormal‚Äù checkout for your-repo, but a special SSH checkout for your-extra-repo‚Ä¶\nThe trick isn‚Äôt too complicated - in fact, it‚Äôs adapted from a scenario anticipated by the actions/checkout repo‚Äôs README file:\n\nUse an actions/checkout step to check out your-repo as usual\nUse another actions/checkout step to check out the your-extra-repo submodule, taking advantage of some more optional parameters:\n\nrepository - which repository to check out (default is the repo which the workflow belongs to, but we‚Äôll ask for your-extra-repo instead)\npath - the location to check out to within your-repo (the default is ., but a submodule typically lives within a subdirectory, i.e.¬†you probably don‚Äôt want to check out your-extra-repo right on top of your-repo)\nssh-key - we‚Äôve met this already! We‚Äôll use it here to pass through the private half of the deploy key we set up previously\n\n\njobs:\n  some-job-name:\n    runs-on: ubuntu-latest\n\n    steps:\n\n      - name: Checkout this repo\n        uses: actions/checkout@v4\n        with:\n          submodules: false\n\n      - name: Checkout your-extra-repo submodule\n        uses: actions/checkout@v4\n        with:\n          repository: your-user-or-org/your-extra-repo\n          ssh-key: ${{ secrets.YOUR_EXTRA_REPO_DEPLOY_KEY }}\n          path: ./path/to/submodule\n          \n      # More steps...\nNote: the explicit submodules: false isn‚Äôt required since false is the default, but I think it suggests to the casual reader that there‚Äôs something funky and submodule-related going on‚Ä¶)\n\n\nCase 2: pre-commit hooks\nWe use pre-commit in several of our team‚Äôs repos. For these repos, we also set up a GitHub Actions workflow to run pre-commit whenever a pull request is opened or updated. I won‚Äôt go into too much detail here, as that‚Äôs probably worthy of its own post sometime!\nAs well as using some hooks from public repos, we have a handful of custom ‚Äúteam hooks‚Äù in an internal3 repo within our GitHub organisation.\nThe problem is that under the hood, pre-commit uses git to get hold of the various hook-supplying repos. So once again, we need some way of using a ‚Äúregular‚Äù checkout for public repos, and then a ‚Äúnon-regular‚Äù checkout for our internal repo.\nThis time, the trick is in two parts:\n\nIn your-repo, in the .pre-commit-config.yaml file, use HTTPS-format repo URLs for public repos, and an SSH-format URL for the internal repo:\nrepos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: v4.5.0\n    hooks:\n      - id: trailing-whitespace\n      - id: end-of-file-fixer\n  - repo: https://github.com/lorenzwalthert/precommit\n    rev: v0.4.1\n    hooks:\n      - id: parsable-R\n  - repo: git@github.com:your-user-or-org/your-extra-repo\n    rev: v0.0.1\n    hooks:\n      - id: your-first-hook\n      - id: your-second-hook\nIn your GitHub Actions workflow, copy the private half of the deploy key from the relevant GitHub secret into a keyfile, and then tell pre-commit to use that SSH key for all SSH operations executed by git:\njobs:\n  run:\n    runs-on: ubuntu-latest\n    steps:\n\n    - uses: actions/checkout@v4\n      with:\n        fetch-depth: 0\n\n    - uses: actions/setup-python@v5\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip pre-commit\n\n        # Set up SSH access for some-private-repo\n        mkdir -p ~/.ssh/\n        echo \"${{ secrets.SOME_PRIVATE_REPO_DEPLOY_KEY }}\" &gt; ~/.ssh/deploy-key\n        chmod 600 ~/.ssh/deploy-key\n        ssh-keyscan -H github.com &gt;&gt; ~/.ssh/known_hosts\n\n    - name: Run pre-commit\n      run: |\n        GIT_SSH_COMMAND='ssh -i ~/.ssh/deploy-key -o IdentitiesOnly=yes' \\\n          pre-commit run \\\n          --from-ref ${{ github.event.pull_request.base.sha }} \\\n          --to-ref ${{ github.event.pull_request.head.sha }}"
  },
  {
    "objectID": "posts/2024-04-27-use-deploy-keys-to-access-private-repos-within-github-actions/index.html#footnotes",
    "href": "posts/2024-04-27-use-deploy-keys-to-access-private-repos-within-github-actions/index.html#footnotes",
    "title": "Use deploy keys to access private repos within GitHub Actions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAt the time of writing, this mammoth GitHub issue contains an ongoing discussion: https://github.com/actions/checkout/issues/287‚Ü©Ô∏é\nThis naming pattern helps to make it clear, if you are ever tidying up your deploy keys, which key was used where.‚Ü©Ô∏é\nYou can interchange ‚Äúprivate‚Äù and ‚Äúinternal‚Äù throughout this post - the key thing is that both are ‚Äúnon-public‚Äù.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2016-12-02-30-days-30-visualizations-1-dataset-part-5/index.html",
    "href": "posts/2016-12-02-30-days-30-visualizations-1-dataset-part-5/index.html",
    "title": "30 Days, 30 Visualizations, 1 Dataset: Days 21-25",
    "section": "",
    "text": "Day 21 (02/12/16): Delay to first upload\n\n\nPerhaps the DateUploaded column can give us some help in working out why we have duplicate records.\n\n\nLet‚Äôs do a similar test to the one from a couple of days ago, to see if the upload time is different for records with the same update time.\n\n```rdf3 &lt;- filter(df0, Name != ‚Äútest car park‚Äù) %&gt;% group_by(Name, LastUpdate, DateUploaded) %&gt;% filter(n() &gt; 1)\ndf3rconsole## Source: local data frame [0 x 12] ## Groups: Name, LastUpdate, DateUploaded [0] ## ## # ‚Ä¶ with 12 variables: ID , LastUpdate , Name , ## # Description , Capacity , Status , Occupancy , ## # Percentage , Easting , Northing , DateUploaded , ## # Location &lt;p&gt;Ah-ha! An empty dataframe! So it looks like the duplicate records are caused by the same record being uploaded at multiple different times.&lt;/p&gt; &lt;p&gt;Let‚Äôs take the first upload only, and create a plot similar to yesterday‚Äôs.&lt;/p&gt;rdf4 &lt;- df0 %&gt;% select(Name, LastUpdate, DateUploaded) %&gt;% filter(Name != ‚Äútest car park‚Äù) %&gt;% mutate(LastUpdate = as.POSIXct(LastUpdate, tz = ‚ÄúUTC‚Äù, format = ‚Äú%d/%m/%Y %I:%M:%S %p‚Äù), DateUploaded = as.POSIXct(DateUploaded, tz = ‚ÄúUTC‚Äù, format = ‚Äú%d/%m/%Y %I:%M:%S %p‚Äù)) %&gt;% group_by(Name, LastUpdate) %&gt;% summarize(FirstUpload = min(DateUploaded)) %&gt;% mutate(Delay = as.numeric(FirstUpload - LastUpdate))\np &lt;- ggplot(df4, aes(x = Delay)) + geom_histogram(colour = ‚Äúblack‚Äù) + facet_wrap(~ Name, nrow = 2, scales = ‚Äúfree‚Äù) + ggtitle(‚ÄúDelay between update and first upload‚Äù) + xlab(‚ÄúSeconds‚Äù) + ylab(‚ÄúNumber of records‚Äù) + theme(plot.title = element_text(size = rel(1.5))) + scale_y_log10()\np``` \n\nNote that wherever the histogram displays a bar below the axis, this shows zero records in that bin, since we are using a log scale (and the value shown is therefore log10(0) = -Inf); and by similar logic, wherever the histogram shows 0 there is 1 record in that bin (for example, at the extreme right of the Avon Street CP plot).\n\n\nWe can see that for most of the car parks, there is a single record at any extreme values (e.g.¬†that 1 record from Avon Street - the largest delay by a huge margin). However, there are multiple dodgy records at Podium CP and the SouthGate CPs.\n\n\n\nDay 22 (03/12/16): Minute and second of upload\n\n\nLet‚Äôs have a look at when records are uploaded to the online database.\n\n```rdf3 &lt;- select(df0, Name, DateUploaded) %&gt;% mutate(DateUploaded = as.POSIXct(DateUploaded, tz = ‚ÄúUTC‚Äù, format = ‚Äú%d/%m/%Y %I:%M:%S %p‚Äù)) %&gt;% mutate(Minute = minute(DateUploaded), Second = second(DateUploaded))\np1 &lt;- ggplot(df3, aes(x = Minute)) + geom_histogram(binwidth = 1) p2 &lt;- ggplot(df3, aes(x = Second)) + geom_histogram(binwidth = 1)&lt;p&gt;I thought it was time to try out another new package! (Well, new to me‚Ä¶)&lt;/p&gt;rlibrary(grid)\ngrid.newpage() pushViewport(viewport(layout = grid.layout(2, 2, heights = unit(c(0.5, 5), ‚Äúnull‚Äù))))\ngrid.text(‚ÄúMinute and second of upload‚Äù, vp = viewport(layout.pos.row = 1, layout.pos.col = 1:2), gp = gpar(fontsize = 22, fontface = 2))\nprint(p1, vp = viewport(layout.pos.row = 2, layout.pos.col = 1)) print(p2, vp = viewport(layout.pos.row = 2, layout.pos.col = 2))``` \n\nWe can see that records are uploaded promptly every 5 minutes or so, as claimed by the documentation of the database; and that records tend to be uploaded ‚Äòon the minute‚Äô.\n\n\n\nDay 23 (04/12/16): Upload batch sizes and proportions\n\n\nWe saw yesterday that records are uploaded in batches roughly every 5 minutes. But how many records are usually uploaded in one of these batches? And which car parks, if any, ‚Äúskip‚Äù updates?\n\n```rdf2 &lt;- select(df0, Name, DateUploaded) %&gt;% filter(Name != ‚Äútest car park‚Äù) %&gt;% group_by(DateUploaded) %&gt;% mutate(batch_size = n())\np &lt;- ggplot(df2, aes(x = batch_size))\np1 &lt;- p + geom_bar() + xlab(‚ÄúBatch size‚Äù) + ylab(‚ÄúNumber of batches‚Äù) p2 &lt;- p + geom_bar(aes(fill = Name), position = ‚Äúfill‚Äù) + xlab(‚ÄúBatch size‚Äù) + ylab(‚ÄúProportion of batches where present‚Äù)\nlibrary(grid) grid.newpage() pushViewport(viewport(layout = grid.layout(2, 2, heights = unit(c(0.5, 5),‚Äúnull‚Äù), widths = unit(c(1, 2), ‚Äúnull‚Äù)))) grid.text(‚ÄúUpload batch sizes and proportions‚Äù, vp = viewport(layout.pos.row = 1, layout.pos.col = 1:2), gp = gpar(fontsize = 25, fontface = 2)) print(p1, vp = viewport(layout.pos.row = 2, layout.pos.col = 1)) print(p2, vp = viewport(layout.pos.row = 2, layout.pos.col = 2))``` \n\nSo we can see that most batches are of size 8, as expected (these presumably contain one record per car park for each of the 8 car parks), but there are also many smaller batches - some as small as 4 records.\n\n\nWe can also see which car parks are contributiong to these smaller batches, and therefore work out which ones aren‚Äôt (i.e.¬†the ones skipping updates).\n\n\n\nDay 24 (05/12/16): Name == ‚Äútest car park‚Äù\n\n\nWith a coursework deadline looming, I am struggling both for time and ideas - bear with me, the next few days may be a little rough‚Ä¶\n\n\nI‚Äôve been filtering out the records from ‚Äútest car park‚Äù for most of the last month. I think it‚Äôs high time we had a look at them.\n\n```rdf2 &lt;- filter(df, Name == ‚Äútest car park‚Äù)\nlibrary(scales)\np &lt;- ggplot(df2, aes(x = LastUpdate)) + geom_line(aes(y = Occupancy, colour = ‚ÄúOccupancy‚Äù)) + geom_line(aes(y = Capacity, colour = ‚ÄúCapacity‚Äù)) + ggtitle(‚ÄúTest car park records‚Äù) + xlab(‚ÄúTime of update‚Äù) + ylab(‚ÄúTotal‚Äù) + scale_x_datetime(labels = date_format(‚Äú%d/%m/%y‚Äù))+ scale_colour_manual(name = ‚Äú‚Äú, values = c(‚Äùblack‚Äù, ‚Äúred‚Äù))\np``` \n\nAdmittedly not particularly interesting or informative, but probably the cleanest and most error-free data we‚Äôve seen so far.\n\n\n\nDay 25: Calculation of Percentage\n\n\nThere is one more thing I can check about the data: how is the Percentage column calculated? Given that it contains integer values, there must be some rounding involved - perhaps we can see whether values are rounded up or down, or to the nearest integer.\n\n```rdf4 &lt;- select(df0, LastUpdate, Capacity, Occupancy, Percentage) %&gt;% mutate(newPercentage = (Occupancy / Capacity), Difference = (newPercentage - (Percentage/100)))\np &lt;- ggplot(df4, aes(x = Difference)) + geom_histogram(colour = ‚Äúblack‚Äù, bins = 30) + ggtitle(‚ÄúDifference between (Occupancy/Capacity) and Percentage‚Äù)\np``` \n\nAlthough not entirely clear, the bulk of the observations lie to the right of zero. It seems, then, that the result of the (Occupancy/Capacity) calculation is rounded down to give the Percentage column."
  },
  {
    "objectID": "posts/2017-07-08-a-token-update/index.html",
    "href": "posts/2017-07-08-a-token-update/index.html",
    "title": "A Token Update",
    "section": "",
    "text": "Hi.\nIt‚Äôs been a little while since you‚Äôve heard from me‚Ä¶ well, actually, approaching six months since I last posted, and over four months since this website had any sort of update. Life‚Äôs been busy lately, what with maths and revision and maths and exams and maths and such.\nAnyway, I have some exciting news! A couple of weeks ago, I started a work placement at Mango Solutions, a data science consultancy based in Chippenham (some of you might know of the EARL conferences which they organise every year). It‚Äôs a great place and a great bunch of people, and I am very excited to be joining them for the next twelve months!\nAn added bonus of being out on placement for the year is that I will have a slightly more regular structure to my weeks - no maths problem sheets to be wrestling with all weekend! - so I‚Äôll have more time to do fun data-ML-coding-type stuff. There‚Äôs plenty in the pipeline: stay tuned!\nFinally, it was just over a year ago that I was chatting to a friend about ‚Äúdata science‚Äù and ‚Äúmachine learning‚Äù, and thinking ‚Äúthat sounds cool but I literally have no idea what that means‚Äù. Now it‚Äôs my job. This year has been amazing, and I want to say thank you to everyone who‚Äôs ever come across this website, or liked a tweet, or written a post/given a talk that‚Äôs inspired me, or who I‚Äôve met at BMLM, or who I‚Äôve bored half to death talking about neural networks. Thank YOU, dear reader. Hasta pronto."
  }
]